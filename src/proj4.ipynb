{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C3QCAi6I65y"
      },
      "source": [
        "#Utility functions, parameters, and Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y7B7ac5RJlH"
      },
      "source": [
        "#@title General parameters { run: \"auto\", display-mode: \"form\" }\n",
        "from torchvision import transforms\n",
        "import PIL.Image\n",
        "\n",
        "_nb_epochs = 50 #@param {type:\"integer\"}\n",
        "_log_interval = 1 #@param {type:\"integer\"}\n",
        "_dataset = \"CIFAR-10\" #@param [\"ImageNet-16\", \"CIFAR-10\", \"CIFAR-100\", \"CINIC-10\"]\n",
        "\n",
        "_interpolation_method = \"nearest\" #@param [\"nearest\", \"bilinear\", \"cubic\"]\n",
        "if \"nearest\" == _interpolation_method:\n",
        "    _interpolation_method = PIL.Image.NEAREST\n",
        "elif \"bilinear\" == _interpolation_method:\n",
        "    _interpolation_method = PIL.Image.BILINEAR\n",
        "elif \"cubic\" == _interpolation_method:\n",
        "    _interpolation_method = PIL.Image.CUBIC\n",
        "    \n",
        "_transformations = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(64),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu6PdAGlACRO"
      },
      "source": [
        "#@title AlexNet parameters\n",
        "\n",
        "_alex_enabled = True #@param {type:\"boolean\"}\n",
        "if _alex_enabled:\n",
        "    _alex_learning_rate = 0.01 #@param {type:\"number\"}\n",
        "    _alex_momentum = 0.9 #@param {type:\"number\"}\n",
        "    _alex_weight_decay=0.000 #@param {type:\"number\"}\n",
        "    _alex_scheduler=\"Step\" #@param [\"Step\", \"Adaptive\"]\n",
        "    _alex_step_size=10 #@param {type:\"number\"}\n",
        "    _alex_gamma = 0.1 #@param {type:\"number\"}\n",
        "    _alex_pretrained = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98MeFQMGAbXa"
      },
      "source": [
        "#@title SqueezeNet parameters\n",
        "\n",
        "_squeeze_learning_rate = 0.001 #@param {type:\"number\"}\n",
        "_squeeze_momentum = 0.9 #@param {type:\"number\"}\n",
        "_squeeze_weight_decay=0.0002 #@param {type:\"number\"}\n",
        "_squeeze_scheduler=\"Step\" #@param [\"Step\", \"Adaptive\"]\n",
        "_squeeze_step_size=10 #@param {type:\"number\"}\n",
        "_squeeze_gamma = 0.1 #@param {type:\"number\"}\n",
        "_squeeze_pretrained = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFoc4u5gil6O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d78f1e53-8a68-40ea-d8f2-e17292d7a8e3"
      },
      "source": [
        "#@title Google Drive {run: \"auto\"}\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "data_folder = '/gdrive/My Drive/COMP551_Assignment3_data/'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN1vdP6E7NT7"
      },
      "source": [
        "#@title Utility functions\n",
        "\n",
        "def log_progress(sequence, every=None, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "    \n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "        \n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "                    \n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n",
        "\n",
        "def try_makedirs(d):\n",
        "    import os\n",
        "    try:\n",
        "        os.makedirs(d)\n",
        "    except FileExistsError as e:\n",
        "        pass\n",
        "\n",
        "def load_image(filename):\n",
        "    return \\\n",
        "        PIL.Image.open(filename)\\\n",
        "          .convert('RGB')\n",
        "\n",
        "# def load_image(filename):\n",
        "#     return \\\n",
        "#         PIL.Image.open(filename)\\\n",
        "#           .resize((64,64), _interpolation_method)\\\n",
        "#           .convert('RGB')\n",
        "\n",
        "# def load_image(filename):\n",
        "#     return np.array(\n",
        "#         PIL.Image.open(filename)\\\n",
        "#           .resize((64,64), _interpolation_method)\\\n",
        "#           .convert('RGB'),\n",
        "#         dtype=np.float32\n",
        "#     )/255\n",
        "\n",
        "\n",
        "def plot_results(\n",
        "    results_1, model_1_name, model_1_color,\n",
        "    results_2, model_2_name, model_2_color\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    x = range(1, 1+results_1.shape[0])\n",
        "    one_plt = plt.plot(x, results_1[:,1], model_1_color + '-', x, results_1[:,1], model_1_color + 'o')\n",
        "    two_plt = plt.plot(x, results_2[:,1], model_2_color + '-', x, results_2[:,1], model_2_color + 'o')\n",
        "    \n",
        "    for i in range(1, _nb_epochs):\n",
        "        if results_2[i,2] != results_2[i-1,2] \\\n",
        "          and not np.isnan(results_2[i,2]) \\\n",
        "          and not np.isnan(results_2[i-1, 2]):\n",
        "            plt.plot([i-0.1, i-0.1], [0, 100], model_1_color + '--')\n",
        "        if results_1[i,2] != results_1[i-1,2] \\\n",
        "          and not np.isnan(results_1[i,2]) \\\n",
        "          and not np.isnan(results_1[i-1, 2]):\n",
        "            plt.plot([i+0.1, i+0.1], [0, 100], model_2_color + '--')\n",
        "\n",
        "    plt.legend((one_plt[0], two_plt[0]), (model_1_name, model_2_name))\n",
        "    plt.title('Model accuracy improvement over time')\n",
        "    plt.xlabel('# of epochs')\n",
        "    plt.xticks(np.linspace(1, results_1.shape[0], 11))\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.ylim(0, 100)\n",
        "    plt.yticks(np.linspace(0, 100, 11));\n",
        "    plt.draw()\n",
        "    \n",
        "    \n",
        "def plot_results_to_grid(\n",
        "    grid, where,\n",
        "    results_1, model_1_name, model_1_color,\n",
        "    results_2, model_2_name, model_2_color\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    with grid.output_to(where[0], where[1]):\n",
        "        grid.clear_cell()\n",
        "        plot_results(results_1, model_1_name, model_1_color, results_2, model_2_name, model_2_color)\n",
        "\n",
        "        \n",
        "def elapsed_time(model_name, i):\n",
        "    import time\n",
        "    \n",
        "    if 0 == i:\n",
        "        elapsed_time.start = time.time()\n",
        "    else:\n",
        "        print(\n",
        "            \"{{{}}} The last training epoch took {} seconds.\\n\\n\".format(\n",
        "                model_name, time.time() - elapsed_time.start\n",
        "            )\n",
        "        )\n",
        "        \n",
        "\n",
        "class CatchIO:\n",
        "    def __init__(self):\n",
        "        self._stdout = None\n",
        "        self.buffer = None\n",
        "    \n",
        "    def __enter__(self):\n",
        "        import sys\n",
        "        import io\n",
        "        \n",
        "        self._stdout = sys.stdout\n",
        "        sys.stdout = io.StringIO()\n",
        "    \n",
        "    def __exit__(self, type_, value, traceback):\n",
        "        import sys\n",
        "        \n",
        "        self.buffer = sys.stdout.getvalue()\n",
        "        sys.stdout = self._stdout\n",
        "        if value is not None:\n",
        "            raise value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVa_NYwJH_u"
      },
      "source": [
        "#Models and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxuWtQ5tLCiy"
      },
      "source": [
        "#@title AlexNet{display-mode: \"form\"}\n",
        "from torchvision.models.alexnet import alexnet\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class AlexNet(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.alex = alexnet(pretrained=_alex_pretrained, num_classes=1000);\n",
        "   \n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.alex.forward(x), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sc2mKvqLE36"
      },
      "source": [
        "#@title SqueezeNet {display-mode: \"form\"}\n",
        "import torchvision.models.squeezenet\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.squeeze = torchvision.models.squeezenet.squeezenet1_0(pretrained=_squeeze_pretrained, num_classes=1000);\n",
        "   \n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.squeeze.forward(x), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILsk-md9jUYc",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c9941e3-4f7b-4f02-db04-d0df944d17bf"
      },
      "source": [
        "#@title ImageNet-16 _(warning: very slow)_\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "import pandas\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "path_to_ImageNet16 = \"COMP551_Assignment4/data/\" #@param {type:\"string\"}\n",
        "\n",
        "if \"ImageNet-16\" == _dataset:\n",
        "    IMGSZ = (16,16,3)\n",
        "    NUM_IMAGES = 1275273\n",
        "\n",
        "    train_pickle_folder = 'data/16/train/pickle'\n",
        "    train_image_folder = 'data/16/train/images'\n",
        "    valid_image_folder = 'data/16/validation/images'\n",
        "    test_pickle_folder = 'data/16/test/pickle'\n",
        "    test_image_folder = 'data/16/test/images'\n",
        "\n",
        "    force = False #@param {type:\"boolean\"}\n",
        "    train_valid_split = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "    ct = np.zeros(1000)\n",
        "\n",
        "    if not os.path.exists(train_image_folder) or force:\n",
        "        # Create folders\n",
        "        try_makedirs(train_pickle_folder)\n",
        "        try_makedirs(train_image_folder)\n",
        "        try_makedirs(valid_image_folder)\n",
        "        try_makedirs(test_pickle_folder)\n",
        "        try_makedirs(test_image_folder)\n",
        "        for ix in range(1000):\n",
        "            try_makedirs(os.path.join(train_image_folder, \"class_{}\".format(1+ix)))\n",
        "            try_makedirs(os.path.join(valid_image_folder, \"class_{}\".format(1+ix)))\n",
        "            try_makedirs(os.path.join(test_image_folder, \"class_{}\".format(1+ix)))\n",
        "        # Extract training data zipfile\n",
        "        z = ZipFile(\n",
        "            os.path.join(\n",
        "                '/gdrive/My Drive/', path_to_ImageNet16, 'Imagenet16_train.zip'\n",
        "            )\n",
        "        )\n",
        "        z.extractall(train_pickle_folder)\n",
        "        z.close()\n",
        "        # Extract training images to folders\n",
        "        for ix in log_progress(\n",
        "            range(1, 11), every=1, name=\"Converting pickle files to images\"\n",
        "        ):\n",
        "            # Read pickle\n",
        "            tmp = pandas.read_pickle(\n",
        "                os.path.join(train_pickle_folder, 'train_data_batch_{}'.format(ix))\n",
        "            )\n",
        "            # CIFAR format to array of images\n",
        "            train_images = tmp['data'] \\\n",
        "              .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "              .transpose([0, 2, 3, 1])\n",
        "            # Save images to png (lossless format)\n",
        "            for jx in range(len(train_images)):\n",
        "                label = tmp['labels'][jx]\n",
        "                # Split training set into training and validation sets\n",
        "                if ct[label-1] > np.floor(train_valid_split * NUM_IMAGES / 1000):\n",
        "                    # validation\n",
        "                    out_folder = valid_image_folder\n",
        "                else:\n",
        "                    # training\n",
        "                    out_folder = train_image_folder\n",
        "                    ct[label-1] = 1 + ct[label-1]\n",
        "                # Save\n",
        "                PIL.Image.fromarray(train_images[jx]).save(\n",
        "                    os.path.join(\n",
        "                        out_folder,\n",
        "                        'class_{}'.format(label),\n",
        "                        '{}.png'.format(jx)\n",
        "                    )\n",
        "                )\n",
        "            # Remove pickle file since we're done with it\n",
        "            os.remove(\n",
        "                os.path.join(train_pickle_folder, 'train_data_batch_{}'.format(ix))\n",
        "            )\n",
        "\n",
        "        # Extract test data zipfile\n",
        "        z = ZipFile(\n",
        "            os.path.join(\n",
        "                '/gdrive/My Drive/', path_to_ImageNet16, 'Imagenet16_val.zip'\n",
        "            )\n",
        "        )\n",
        "        z.extractall(test_pickle_folder)\n",
        "        z.close()\n",
        "        # Extract test images to folders\n",
        "        #   Read pickle\n",
        "        tmp = pandas.read_pickle(os.path.join(test_pickle_folder, 'val_data'))\n",
        "        #   CIFAR format to array of images\n",
        "        test_images = tmp['data'] \\\n",
        "          .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "          .transpose([0, 2, 3, 1])\n",
        "        #   Save images to png (lossless format)\n",
        "        for jx in range(len(test_images)):\n",
        "            # Save\n",
        "            PIL.Image.fromarray(test_images[jx]).save(\n",
        "                os.path.join(\n",
        "                    test_image_folder,\n",
        "                    'class_{}'.format(tmp['labels'][jx]),\n",
        "                    '{}.png'.format(jx)\n",
        "                )\n",
        "            )\n",
        "        #   Remove pickle file since we're done with it\n",
        "        os.remove(\n",
        "            os.path.join(test_pickle_folder, 'val_data')\n",
        "        )\n",
        "\n",
        "    trainset = DatasetFolder(\n",
        "        train_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    validset = DatasetFolder(\n",
        "        valid_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    testset = DatasetFolder(\n",
        "        test_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print('Disabled')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disabled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukY8MyoV82C",
        "cellView": "form"
      },
      "source": [
        "#@title CIFAR-10\n",
        "from torchvision.datasets import DatasetFolder\n",
        "import tarfile\n",
        "import os\n",
        "import pandas\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "path_to_CIFAR10 = \"COMP551_Assignment4/data/\" #@param {type:\"string\"}\n",
        "\n",
        "if \"CIFAR-10\" == _dataset:\n",
        "    IMGSZ = (32, 32, 3)\n",
        "    NUM_IMAGES = 50000\n",
        "    NUM_CLASSES = 10\n",
        "\n",
        "    pickle_folder = 'data/CIFAR10/pickle'\n",
        "    train_image_folder = 'data/CIFAR10/train/images'\n",
        "    valid_image_folder = 'data/CIFAR10/validation/images'\n",
        "    test_image_folder = 'data/CIFAR10/test/images'\n",
        "\n",
        "    force = False #@param {type:\"boolean\"}\n",
        "    train_valid_split = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "    ct = np.zeros(NUM_CLASSES)\n",
        "\n",
        "    if not os.path.exists(train_image_folder) or force:\n",
        "        # Create folders\n",
        "        try_makedirs(pickle_folder)\n",
        "        try_makedirs(train_image_folder)\n",
        "        try_makedirs(valid_image_folder)\n",
        "        try_makedirs(test_image_folder)\n",
        "        for ix in range(NUM_CLASSES):\n",
        "            try_makedirs(os.path.join(train_image_folder, \"class_{}\".format(ix)))\n",
        "            try_makedirs(os.path.join(valid_image_folder, \"class_{}\".format(ix)))\n",
        "            try_makedirs(os.path.join(test_image_folder, \"class_{}\".format(ix)))\n",
        "        # Extract training data zipfile\n",
        "        tar = tarfile.open(\n",
        "            os.path.join(\n",
        "                '/gdrive/My Drive/', path_to_CIFAR10, 'cifar-10-python.tar.gz'\n",
        "            )\n",
        "        )\n",
        "        tar.extractall(pickle_folder)\n",
        "        tar.close()\n",
        "        # Extract training images to folders\n",
        "        for ix in log_progress(\n",
        "            range(1, 6), every=1, name=\"Converting pickle files to images\"\n",
        "        ):\n",
        "            # Read pickle\n",
        "            tmp = pandas.read_pickle(\n",
        "                os.path.join(\n",
        "                    pickle_folder, 'cifar-10-batches-py', 'data_batch_{}'.format(ix)\n",
        "                )\n",
        "            )\n",
        "            # CIFAR format to array of images\n",
        "            train_images = tmp['data'] \\\n",
        "              .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "              .transpose([0, 2, 3, 1])\n",
        "            # Save images to png (lossless format)\n",
        "            for jx in range(len(train_images)):\n",
        "                label = tmp['labels'][jx]\n",
        "                # Split training set into training and validation sets\n",
        "                if ct[label-1] >= np.floor(train_valid_split * NUM_IMAGES / NUM_CLASSES):\n",
        "                    # validation\n",
        "                    out_folder = valid_image_folder\n",
        "                else:\n",
        "                    # training\n",
        "                    out_folder = train_image_folder\n",
        "                    ct[label-1] = 1 + ct[label-1]\n",
        "                # Save\n",
        "                PIL.Image.fromarray(train_images[jx]).save(\n",
        "                    os.path.join(\n",
        "                        out_folder,\n",
        "                        'class_{}'.format(label),\n",
        "                        '{}.png'.format(jx)\n",
        "                    )\n",
        "                )\n",
        "            # Remove pickle file since we're done with it\n",
        "            os.remove(\n",
        "                os.path.join(\n",
        "                    pickle_folder,\n",
        "                    'cifar-10-batches-py', \n",
        "                    'data_batch_{}'.format(ix)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Extract test images to folders\n",
        "        #   Read pickle\n",
        "        tmp = pandas.read_pickle(\n",
        "            os.path.join(pickle_folder, 'cifar-10-batches-py', 'test_batch')\n",
        "        )\n",
        "        #   CIFAR format to array of images\n",
        "        test_images = tmp['data']\\\n",
        "          .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "          .transpose([0, 2, 3, 1])\n",
        "        #   Save images to png (lossless format)\n",
        "        for jx in range(len(test_images)):\n",
        "            # Save\n",
        "            PIL.Image.fromarray(test_images[jx]).save(\n",
        "                os.path.join(\n",
        "                    test_image_folder,\n",
        "                    'class_{}'.format(tmp['labels'][jx]),\n",
        "                    '{}.png'.format(jx)\n",
        "                )\n",
        "            )\n",
        "        #   Remove pickle file since we're done with it\n",
        "        os.remove(\n",
        "            os.path.join(pickle_folder, 'cifar-10-batches-py', 'test_batch')\n",
        "        )\n",
        "    trainset = DatasetFolder(\n",
        "        train_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    validset = DatasetFolder(\n",
        "        valid_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    testset = DatasetFolder(\n",
        "        test_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print('Disabled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFa3BL1PjSqH",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3523bd9-d476-4b89-b5cc-3517c9e9dd8e"
      },
      "source": [
        "#@title CIFAR-100\n",
        "from torchvision.datasets import DatasetFolder\n",
        "import tarfile\n",
        "import os\n",
        "import pandas\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "path_to_CIFAR100 = \"COMP551_Assignment4/data/\" #@param {type:\"string\"}\n",
        "\n",
        "if \"CIFAR-100\" == _dataset:\n",
        "    print('Loading CIFAR-100... ', end='')\n",
        "    \n",
        "    IMGSZ = (32, 32, 3)\n",
        "    NUM_IMAGES = 50000\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    pickle_folder = 'data/CIFAR100/pickle'\n",
        "    train_image_folder = 'data/CIFAR100/train/images'\n",
        "    valid_image_folder = 'data/CIFAR100/validation/images'\n",
        "    test_image_folder = 'data/CIFAR100/test/images'\n",
        "\n",
        "    label_type = \"fine (100)\" #@param [\"fine (100)\", \"coarse (20)\"]\n",
        "    if label_type == \"fine (100)\":\n",
        "        label_type = \"fine_labels\"\n",
        "    elif label_type == \"coarse (20)\":\n",
        "        label_type = \"coarse_labels\"\n",
        "    \n",
        "    force = False #@param {type:\"boolean\"}\n",
        "    train_valid_split = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "    ct = np.zeros(NUM_CLASSES)\n",
        "\n",
        "    if not os.path.exists(train_image_folder) or force:\n",
        "        # Create folders\n",
        "        try_makedirs(pickle_folder)\n",
        "        try_makedirs(train_image_folder)\n",
        "        try_makedirs(valid_image_folder)\n",
        "        try_makedirs(test_image_folder)\n",
        "        for ix in range(NUM_CLASSES):\n",
        "            try_makedirs(os.path.join(train_image_folder, \"class_{}\".format(ix)))\n",
        "            try_makedirs(os.path.join(valid_image_folder, \"class_{}\".format(ix)))\n",
        "            try_makedirs(os.path.join(test_image_folder, \"class_{}\".format(ix)))\n",
        "        # Extract training data zipfile\n",
        "        tar = tarfile.open(\n",
        "            os.path.join(\n",
        "                '/gdrive/My Drive/', path_to_CIFAR100, 'cifar-100-python.tar.gz'\n",
        "            )\n",
        "        )\n",
        "        tar.extractall(pickle_folder)\n",
        "        tar.close()\n",
        "        # Extract training images to folders\n",
        "        # Read pickle\n",
        "        tmp = pandas.read_pickle(\n",
        "            os.path.join(pickle_folder, 'cifar-100-python', 'train')\n",
        "        )\n",
        "        tmp.keys()\n",
        "        # CIFAR format to array of images\n",
        "        train_images = tmp['data'] \\\n",
        "          .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "          .transpose([0, 2, 3, 1])\n",
        "        # Save images to png (lossless format)\n",
        "        for jx in range(len(train_images)):\n",
        "            label = tmp[label_type][jx]\n",
        "            # Split training set into training and validation sets\n",
        "            if ct[label-1] > np.floor(train_valid_split * NUM_IMAGES / NUM_CLASSES):\n",
        "                # validation\n",
        "                out_folder = valid_image_folder\n",
        "            else:\n",
        "                # training\n",
        "                out_folder = train_image_folder\n",
        "                ct[label-1] = 1 + ct[label-1]\n",
        "            # Save\n",
        "            PIL.Image.fromarray(train_images[jx]).save(\n",
        "                os.path.join(\n",
        "                    out_folder,\n",
        "                    'class_{}'.format(label),\n",
        "                    '{}.png'.format(jx)\n",
        "                )\n",
        "            )\n",
        "        # Remove pickle file since we're done with it\n",
        "        os.remove(os.path.join(pickle_folder, 'cifar-100-python', 'train'))\n",
        "\n",
        "        # Extract test images to folders\n",
        "        #   Read pickle\n",
        "        tmp = pandas.read_pickle(\n",
        "            os.path.join(pickle_folder, 'cifar-100-python', 'test')\n",
        "        )\n",
        "        #   CIFAR format to array of images\n",
        "        test_images = tmp['data']\\\n",
        "          .reshape(-1, IMGSZ[2], IMGSZ[0], IMGSZ[1]) \\\n",
        "          .transpose([0, 2, 3, 1])\n",
        "        #   Save images to png (lossless format)\n",
        "        for jx in range(len(test_images)):\n",
        "            # Save\n",
        "            PIL.Image.fromarray(test_images[jx]).save(\n",
        "                os.path.join(\n",
        "                    test_image_folder,\n",
        "                    'class_{}'.format(tmp[label_type][jx]),\n",
        "                    '{}.png'.format(jx)\n",
        "                )\n",
        "            )\n",
        "        #   Remove pickle file since we're done with it\n",
        "        os.remove(\n",
        "            os.path.join(pickle_folder, 'cifar-100-python', 'test')\n",
        "        )\n",
        "    \n",
        "    trainset = DatasetFolder(\n",
        "        train_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    validset = DatasetFolder(\n",
        "        valid_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    testset = DatasetFolder(\n",
        "        test_image_folder, load_image, ['png'],\n",
        "        transform=_transformations\n",
        "    )\n",
        "    \n",
        "    print('Done.')\n",
        "    \n",
        "else:\n",
        "    print('Disabled')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disabled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrjWvxexUwdW",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b114398a-fb7c-4a30-9430-cdd0187d86ef"
      },
      "source": [
        "#@title CINIC-10\n",
        "from torchvision.datasets import ImageFolder\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "path_to_CINIC = \"NordVPN216856 \" #@param {type:\"string\"}\n",
        "force = False #@param {type:\"boolean\"}\n",
        "image_folder = 'data/CINIC-10'\n",
        "\n",
        "if \"CINIC-10\" == _dataset:\n",
        "    print('Loading CIFAR-100... ', end='')\n",
        "\n",
        "    if not os.path.exists(image_folder) or force:\n",
        "        tar = tarfile.open(os.path.join('/gdrive/My Drive/', path_to_CINIC))\n",
        "        try_makedirs(image_folder)\n",
        "        tar.extractall(image_folder)\n",
        "        tar.close()\n",
        "\n",
        "    trainset = ImageFolder(\n",
        "        os.path.join(image_folder, 'train'),\n",
        "        loader=load_image,\n",
        "        transform=_transformations\n",
        "    )\n",
        "    validset = ImageFolder(\n",
        "        os.path.join(image_folder, 'valid'),\n",
        "        loader=load_image,\n",
        "        transform=_transformations\n",
        "    )\n",
        "    testset = ImageFolder(\n",
        "        os.path.join(image_folder, 'test'),\n",
        "        loader=load_image,\n",
        "        transform=_transformations\n",
        "    )\n",
        "    \n",
        "    print('done')\n",
        "\n",
        "else:\n",
        "    print('Disabled')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disabled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g54Ru642KcgQ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c36a2b20-3137-4782-f2c5-acc8c2c5a5d2"
      },
      "source": [
        "#@title Dataloader and model initialization\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "import PIL.Image\n",
        "\n",
        "\n",
        "def train(model, model_name, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % _log_interval == 0:\n",
        "            print('{{{}}} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    model_name, epoch, batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def validate(model, model_name, device, validation_loader):\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in validation_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            validation_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    validation_loss /= len(validation_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\n{{{}}} Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "        .format(\n",
        "            model_name, validation_loss, correct,\n",
        "            len(validation_loader.dataset),\n",
        "            100. * correct / len(validation_loader.dataset)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return validation_loss, 100. * correct / len(validation_loader.dataset)\n",
        "\n",
        "\n",
        "def test(model, model_name, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\n{{{}}} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "        .format(\n",
        "            model_name, test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "#torch.manual_seed(1)\n",
        "\n",
        "# Initialize training, validation, and test loaders\n",
        "train_loader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
        "valid_loader = DataLoader(validset, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=512, shuffle=True)\n",
        "\n",
        "# Initialize AlexNet\n",
        "if _alex_enabled:\n",
        "    alex = AlexNet().to(\"cuda\")\n",
        "    alex_opt = optim.SGD(\n",
        "        alex.parameters(), lr=_alex_learning_rate, momentum=_alex_momentum,\n",
        "        weight_decay=_alex_weight_decay\n",
        "    )\n",
        "    if \"Step\" == _alex_scheduler:\n",
        "        alex_sch = optim.lr_scheduler.StepLR(alex_opt, _alex_step_size, _alex_gamma)\n",
        "    elif \"Adaptive\" == _alex_scheduler:\n",
        "        alex_sch = optim.lr_scheduler.ReduceLROnPlateau(alex_opt, factor=0.5, patience=2, verbose=True)\n",
        "    else:\n",
        "        assert False, \"Scheduler type not recognized.\"\n",
        "\n",
        "# Initialize SqueezeNet\n",
        "squeeze = SqueezeNet().to(\"cuda\")\n",
        "squeeze_opt = optim.SGD(\n",
        "    squeeze.parameters(), lr=_squeeze_learning_rate, momentum=_squeeze_momentum,\n",
        "    weight_decay=_squeeze_weight_decay\n",
        ")\n",
        "if \"Step\" == _squeeze_scheduler:\n",
        "    squeeze_sch = optim.lr_scheduler.StepLR(squeeze_opt, _squeeze_step_size, _squeeze_gamma)\n",
        "elif \"Adaptive\" == _squeeze_scheduler:\n",
        "    squeeze_sch = optim.lr_scheduler.ReduceLROnPlateau(alex_opt, factor=0.5, patience=2, verbose=True)\n",
        "else:\n",
        "    assert False, \"Scheduler type not recognized.\"\n",
        "\n",
        "# Done!\n",
        "print('Dataloaders and models initialized.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataloaders and models initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNn2IFHic0Iy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "013a2384-73a6-40e2-fc3d-51e99ea856db"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\n",
        "    len(trainset),\n",
        "    len(validset),\n",
        "    len(trainset) / (len(trainset) + len(validset)),\n",
        "    len(testset)\n",
        ")\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(trainset[0][0].numpy().transpose([1, 2, 0]))\n",
        "plt.title('training set')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(validset[0][0].numpy().transpose([1, 2, 0]))\n",
        "plt.title('validation set')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(testset[0][0].numpy().transpose([1, 2, 0]));\n",
        "plt.title('test set')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37814 5000 0.883215770542346 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'test set')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACRCAYAAADTnUPWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHt5Q3qKKV7_"
      },
      "source": [
        "#Training and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suwU4WnxQ8sj",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145865
        },
        "outputId": "fbc37397-a4bb-4c44-dcc2-4e0711d45026"
      },
      "source": [
        "#@title Training\n",
        "from google.colab import widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "grid = widgets.Grid(2, 1)\n",
        "catch = CatchIO()\n",
        "find_lr = re.compile(\"[0-9.e-]*.$\")\n",
        "\n",
        "if _alex_enabled or 'alex_results' not in locals():\n",
        "    alex_results = np.full((1+_nb_epochs, 3), np.nan)\n",
        "    alex_results[0, 2] = locals().get('_alex_learning_rate', np.nan)\n",
        "\n",
        "squeeze_results = np.full((1+_nb_epochs, 3), np.nan)\n",
        "squeeze_results[0, 2] = _squeeze_learning_rate\n",
        "\n",
        "# Prepare figure\n",
        "plot_results_to_grid(grid, (0, 0), alex_results, \"AlexNet\", \"r\", squeeze_results, \"SqueezeNet\", \"b\")\n",
        "\n",
        "for epoch in range(1, _nb_epochs + 1):\n",
        "    # Display training output\n",
        "    with grid.output_to(1, 0):\n",
        "        if _alex_enabled:\n",
        "            elapsed_time(\"AlexNet\", 0)\n",
        "            if \"Step\" == _alex_scheduler:\n",
        "                alex_sch.step()\n",
        "            \n",
        "            alex_results[epoch-1, 2] = alex_opt.param_groups[0]['lr']\n",
        "            \n",
        "            train(alex, \"AlexNet\", \"cuda\", train_loader, alex_opt, epoch)\n",
        "            alex_results[epoch-1, :2] = validate(alex, \"AlexNet\", \"cuda\", valid_loader)\n",
        "            if \"Adaptive\" == _alex_scheduler:\n",
        "                alex_sch.step(alex_results[epoch-1, 0])\n",
        "            elapsed_time(\"AlexNet\", 1)\n",
        "\n",
        "        elapsed_time(\"SqueezeNet\", 0)\n",
        "        squeeze_sch.step()\n",
        "        if \"Step\" == _squeeze_scheduler:\n",
        "            squeeze_results[epoch-1, 2] = squeeze_opt.param_groups[0]['lr']\n",
        "        train(squeeze, \"SqueezeNet\", \"cuda\", train_loader, squeeze_opt, epoch)\n",
        "        squeeze_results[epoch-1, :2] = validate(squeeze, \"SqueezeNet\", \"cuda\", valid_loader)\n",
        "        if \"Adaptive\" == _squeeze_scheduler:\n",
        "            squeeze_sch.step(squeeze_results[epoch-1, 0])\n",
        "        elapsed_time(\"SqueezeNet\", 1)\n",
        "\n",
        "    plot_results_to_grid(grid, (0, 0), alex_results, \"AlexNet\", \"r\", squeeze_results, \"SqueezeNet\", \"b\")\n",
        "\n",
        "    \n",
        "if _alex_enabled:\n",
        "    alex_results[-1, :2] = test(alex, \"AlexNet\", \"cuda\", test_loader)\n",
        "squeeze_results[-1, :2] = test(squeeze, \"SqueezeNet\", \"cuda\", test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id12, #id12 > tbody > tr > th, #id12 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id12><tr><td id=id12-0-0></td></tr><tr><td id=id12-1-0></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"879484a4-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_ddbddc0fd3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8794c054-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_b5c25c0e78"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8794fea2-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"8794c054-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cd13331a97"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"87b20de4-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"879484a4-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_bd8b314158"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"87b3445c-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_d2f1b46497"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"87b37724-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_356c6b5d8c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"87b3b144-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"87b37724-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6ee0e18f1c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 1 [0/37814 (0%)]\tLoss: 6.904803\n",
            "{AlexNet} Train Epoch: 1 [512/37814 (1%)]\tLoss: 6.902843\n",
            "{AlexNet} Train Epoch: 1 [1024/37814 (3%)]\tLoss: 6.899438\n",
            "{AlexNet} Train Epoch: 1 [1536/37814 (4%)]\tLoss: 6.897533\n",
            "{AlexNet} Train Epoch: 1 [2048/37814 (5%)]\tLoss: 6.891885\n",
            "{AlexNet} Train Epoch: 1 [2560/37814 (7%)]\tLoss: 6.885558\n",
            "{AlexNet} Train Epoch: 1 [3072/37814 (8%)]\tLoss: 6.878338\n",
            "{AlexNet} Train Epoch: 1 [3584/37814 (9%)]\tLoss: 6.869989\n",
            "{AlexNet} Train Epoch: 1 [4096/37814 (11%)]\tLoss: 6.862409\n",
            "{AlexNet} Train Epoch: 1 [4608/37814 (12%)]\tLoss: 6.852913\n",
            "{AlexNet} Train Epoch: 1 [5120/37814 (14%)]\tLoss: 6.843389\n",
            "{AlexNet} Train Epoch: 1 [5632/37814 (15%)]\tLoss: 6.833260\n",
            "{AlexNet} Train Epoch: 1 [6144/37814 (16%)]\tLoss: 6.823516\n",
            "{AlexNet} Train Epoch: 1 [6656/37814 (18%)]\tLoss: 6.811656\n",
            "{AlexNet} Train Epoch: 1 [7168/37814 (19%)]\tLoss: 6.799840\n",
            "{AlexNet} Train Epoch: 1 [7680/37814 (20%)]\tLoss: 6.786971\n",
            "{AlexNet} Train Epoch: 1 [8192/37814 (22%)]\tLoss: 6.774587\n",
            "{AlexNet} Train Epoch: 1 [8704/37814 (23%)]\tLoss: 6.761682\n",
            "{AlexNet} Train Epoch: 1 [9216/37814 (24%)]\tLoss: 6.747237\n",
            "{AlexNet} Train Epoch: 1 [9728/37814 (26%)]\tLoss: 6.733220\n",
            "{AlexNet} Train Epoch: 1 [10240/37814 (27%)]\tLoss: 6.718434\n",
            "{AlexNet} Train Epoch: 1 [10752/37814 (28%)]\tLoss: 6.702164\n",
            "{AlexNet} Train Epoch: 1 [11264/37814 (30%)]\tLoss: 6.685328\n",
            "{AlexNet} Train Epoch: 1 [11776/37814 (31%)]\tLoss: 6.667306\n",
            "{AlexNet} Train Epoch: 1 [12288/37814 (32%)]\tLoss: 6.644947\n",
            "{AlexNet} Train Epoch: 1 [12800/37814 (34%)]\tLoss: 6.625569\n",
            "{AlexNet} Train Epoch: 1 [13312/37814 (35%)]\tLoss: 6.599271\n",
            "{AlexNet} Train Epoch: 1 [13824/37814 (36%)]\tLoss: 6.569792\n",
            "{AlexNet} Train Epoch: 1 [14336/37814 (38%)]\tLoss: 6.533073\n",
            "{AlexNet} Train Epoch: 1 [14848/37814 (39%)]\tLoss: 6.488371\n",
            "{AlexNet} Train Epoch: 1 [15360/37814 (41%)]\tLoss: 6.423119\n",
            "{AlexNet} Train Epoch: 1 [15872/37814 (42%)]\tLoss: 6.326347\n",
            "{AlexNet} Train Epoch: 1 [16384/37814 (43%)]\tLoss: 6.146568\n",
            "{AlexNet} Train Epoch: 1 [16896/37814 (45%)]\tLoss: 5.792313\n",
            "{AlexNet} Train Epoch: 1 [17408/37814 (46%)]\tLoss: 4.816409\n",
            "{AlexNet} Train Epoch: 1 [17920/37814 (47%)]\tLoss: 2.612206\n",
            "{AlexNet} Train Epoch: 1 [18432/37814 (49%)]\tLoss: 2.496566\n",
            "{AlexNet} Train Epoch: 1 [18944/37814 (50%)]\tLoss: 7.303306\n",
            "{AlexNet} Train Epoch: 1 [19456/37814 (51%)]\tLoss: 20.090090\n",
            "{AlexNet} Train Epoch: 1 [19968/37814 (53%)]\tLoss: 6.026269\n",
            "{AlexNet} Train Epoch: 1 [20480/37814 (54%)]\tLoss: 6.340082\n",
            "{AlexNet} Train Epoch: 1 [20992/37814 (55%)]\tLoss: 6.340731\n",
            "{AlexNet} Train Epoch: 1 [21504/37814 (57%)]\tLoss: 6.318479\n",
            "{AlexNet} Train Epoch: 1 [22016/37814 (58%)]\tLoss: 6.322579\n",
            "{AlexNet} Train Epoch: 1 [22528/37814 (59%)]\tLoss: 6.304451\n",
            "{AlexNet} Train Epoch: 1 [23040/37814 (61%)]\tLoss: 6.309680\n",
            "{AlexNet} Train Epoch: 1 [23552/37814 (62%)]\tLoss: 6.209980\n",
            "{AlexNet} Train Epoch: 1 [24064/37814 (64%)]\tLoss: 6.150321\n",
            "{AlexNet} Train Epoch: 1 [24576/37814 (65%)]\tLoss: 6.097732\n",
            "{AlexNet} Train Epoch: 1 [25088/37814 (66%)]\tLoss: 6.218793\n",
            "{AlexNet} Train Epoch: 1 [25600/37814 (68%)]\tLoss: 5.912914\n",
            "{AlexNet} Train Epoch: 1 [26112/37814 (69%)]\tLoss: 5.952456\n",
            "{AlexNet} Train Epoch: 1 [26624/37814 (70%)]\tLoss: 6.075174\n",
            "{AlexNet} Train Epoch: 1 [27136/37814 (72%)]\tLoss: 5.950455\n",
            "{AlexNet} Train Epoch: 1 [27648/37814 (73%)]\tLoss: 5.861289\n",
            "{AlexNet} Train Epoch: 1 [28160/37814 (74%)]\tLoss: 6.144994\n",
            "{AlexNet} Train Epoch: 1 [28672/37814 (76%)]\tLoss: 6.039488\n",
            "{AlexNet} Train Epoch: 1 [29184/37814 (77%)]\tLoss: 5.821278\n",
            "{AlexNet} Train Epoch: 1 [29696/37814 (78%)]\tLoss: 5.666679\n",
            "{AlexNet} Train Epoch: 1 [30208/37814 (80%)]\tLoss: 5.372586\n",
            "{AlexNet} Train Epoch: 1 [30720/37814 (81%)]\tLoss: 5.903474\n",
            "{AlexNet} Train Epoch: 1 [31232/37814 (82%)]\tLoss: 5.530824\n",
            "{AlexNet} Train Epoch: 1 [31744/37814 (84%)]\tLoss: 5.722784\n",
            "{AlexNet} Train Epoch: 1 [32256/37814 (85%)]\tLoss: 5.805940\n",
            "{AlexNet} Train Epoch: 1 [32768/37814 (86%)]\tLoss: 5.700723\n",
            "{AlexNet} Train Epoch: 1 [33280/37814 (88%)]\tLoss: 5.553907\n",
            "{AlexNet} Train Epoch: 1 [33792/37814 (89%)]\tLoss: 5.462158\n",
            "{AlexNet} Train Epoch: 1 [34304/37814 (91%)]\tLoss: 5.655462\n",
            "{AlexNet} Train Epoch: 1 [34816/37814 (92%)]\tLoss: 5.320925\n",
            "{AlexNet} Train Epoch: 1 [35328/37814 (93%)]\tLoss: 5.292950\n",
            "{AlexNet} Train Epoch: 1 [35840/37814 (95%)]\tLoss: 5.279474\n",
            "{AlexNet} Train Epoch: 1 [36352/37814 (96%)]\tLoss: 5.427080\n",
            "{AlexNet} Train Epoch: 1 [36864/37814 (97%)]\tLoss: 5.209697\n",
            "{AlexNet} Train Epoch: 1 [31974/37814 (99%)]\tLoss: 5.187082\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 4.9986, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.777989625930786 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 1 [0/37814 (0%)]\tLoss: 6.884150\n",
            "{SqueezeNet} Train Epoch: 1 [512/37814 (1%)]\tLoss: 6.865549\n",
            "{SqueezeNet} Train Epoch: 1 [1024/37814 (3%)]\tLoss: 6.834106\n",
            "{SqueezeNet} Train Epoch: 1 [1536/37814 (4%)]\tLoss: 6.781275\n",
            "{SqueezeNet} Train Epoch: 1 [2048/37814 (5%)]\tLoss: 6.688046\n",
            "{SqueezeNet} Train Epoch: 1 [2560/37814 (7%)]\tLoss: 6.561697\n",
            "{SqueezeNet} Train Epoch: 1 [3072/37814 (8%)]\tLoss: 6.381719\n",
            "{SqueezeNet} Train Epoch: 1 [3584/37814 (9%)]\tLoss: 6.090206\n",
            "{SqueezeNet} Train Epoch: 1 [4096/37814 (11%)]\tLoss: 5.528506\n",
            "{SqueezeNet} Train Epoch: 1 [4608/37814 (12%)]\tLoss: 4.694244\n",
            "{SqueezeNet} Train Epoch: 1 [5120/37814 (14%)]\tLoss: 4.777637\n",
            "{SqueezeNet} Train Epoch: 1 [5632/37814 (15%)]\tLoss: 4.058091\n",
            "{SqueezeNet} Train Epoch: 1 [6144/37814 (16%)]\tLoss: 3.858936\n",
            "{SqueezeNet} Train Epoch: 1 [6656/37814 (18%)]\tLoss: 3.816465\n",
            "{SqueezeNet} Train Epoch: 1 [7168/37814 (19%)]\tLoss: 3.796726\n",
            "{SqueezeNet} Train Epoch: 1 [7680/37814 (20%)]\tLoss: 3.650097\n",
            "{SqueezeNet} Train Epoch: 1 [8192/37814 (22%)]\tLoss: 3.687437\n",
            "{SqueezeNet} Train Epoch: 1 [8704/37814 (23%)]\tLoss: 3.737221\n",
            "{SqueezeNet} Train Epoch: 1 [9216/37814 (24%)]\tLoss: 3.458769\n",
            "{SqueezeNet} Train Epoch: 1 [9728/37814 (26%)]\tLoss: 3.200821\n",
            "{SqueezeNet} Train Epoch: 1 [10240/37814 (27%)]\tLoss: 2.837496\n",
            "{SqueezeNet} Train Epoch: 1 [10752/37814 (28%)]\tLoss: 2.618750\n",
            "{SqueezeNet} Train Epoch: 1 [11264/37814 (30%)]\tLoss: 2.544311\n",
            "{SqueezeNet} Train Epoch: 1 [11776/37814 (31%)]\tLoss: 2.452304\n",
            "{SqueezeNet} Train Epoch: 1 [12288/37814 (32%)]\tLoss: 2.814474\n",
            "{SqueezeNet} Train Epoch: 1 [12800/37814 (34%)]\tLoss: 2.687649\n",
            "{SqueezeNet} Train Epoch: 1 [13312/37814 (35%)]\tLoss: 2.481727\n",
            "{SqueezeNet} Train Epoch: 1 [13824/37814 (36%)]\tLoss: 2.453789\n",
            "{SqueezeNet} Train Epoch: 1 [14336/37814 (38%)]\tLoss: 2.639715\n",
            "{SqueezeNet} Train Epoch: 1 [14848/37814 (39%)]\tLoss: 2.586169\n",
            "{SqueezeNet} Train Epoch: 1 [15360/37814 (41%)]\tLoss: 2.439551\n",
            "{SqueezeNet} Train Epoch: 1 [15872/37814 (42%)]\tLoss: 2.436007\n",
            "{SqueezeNet} Train Epoch: 1 [16384/37814 (43%)]\tLoss: 2.394559\n",
            "{SqueezeNet} Train Epoch: 1 [16896/37814 (45%)]\tLoss: 2.401366\n",
            "{SqueezeNet} Train Epoch: 1 [17408/37814 (46%)]\tLoss: 2.416071\n",
            "{SqueezeNet} Train Epoch: 1 [17920/37814 (47%)]\tLoss: 2.463426\n",
            "{SqueezeNet} Train Epoch: 1 [18432/37814 (49%)]\tLoss: 2.460631\n",
            "{SqueezeNet} Train Epoch: 1 [18944/37814 (50%)]\tLoss: 2.475739\n",
            "{SqueezeNet} Train Epoch: 1 [19456/37814 (51%)]\tLoss: 2.437588\n",
            "{SqueezeNet} Train Epoch: 1 [19968/37814 (53%)]\tLoss: 2.380057\n",
            "{SqueezeNet} Train Epoch: 1 [20480/37814 (54%)]\tLoss: 2.347893\n",
            "{SqueezeNet} Train Epoch: 1 [20992/37814 (55%)]\tLoss: 2.349763\n",
            "{SqueezeNet} Train Epoch: 1 [21504/37814 (57%)]\tLoss: 2.342256\n",
            "{SqueezeNet} Train Epoch: 1 [22016/37814 (58%)]\tLoss: 2.398494\n",
            "{SqueezeNet} Train Epoch: 1 [22528/37814 (59%)]\tLoss: 2.385955\n",
            "{SqueezeNet} Train Epoch: 1 [23040/37814 (61%)]\tLoss: 2.407536\n",
            "{SqueezeNet} Train Epoch: 1 [23552/37814 (62%)]\tLoss: 2.379464\n",
            "{SqueezeNet} Train Epoch: 1 [24064/37814 (64%)]\tLoss: 2.327706\n",
            "{SqueezeNet} Train Epoch: 1 [24576/37814 (65%)]\tLoss: 2.331151\n",
            "{SqueezeNet} Train Epoch: 1 [25088/37814 (66%)]\tLoss: 2.363320\n",
            "{SqueezeNet} Train Epoch: 1 [25600/37814 (68%)]\tLoss: 2.366422\n",
            "{SqueezeNet} Train Epoch: 1 [26112/37814 (69%)]\tLoss: 2.322854\n",
            "{SqueezeNet} Train Epoch: 1 [26624/37814 (70%)]\tLoss: 2.339331\n",
            "{SqueezeNet} Train Epoch: 1 [27136/37814 (72%)]\tLoss: 2.312666\n",
            "{SqueezeNet} Train Epoch: 1 [27648/37814 (73%)]\tLoss: 2.325703\n",
            "{SqueezeNet} Train Epoch: 1 [28160/37814 (74%)]\tLoss: 2.356991\n",
            "{SqueezeNet} Train Epoch: 1 [28672/37814 (76%)]\tLoss: 2.341895\n",
            "{SqueezeNet} Train Epoch: 1 [29184/37814 (77%)]\tLoss: 2.318672\n",
            "{SqueezeNet} Train Epoch: 1 [29696/37814 (78%)]\tLoss: 2.341642\n",
            "{SqueezeNet} Train Epoch: 1 [30208/37814 (80%)]\tLoss: 2.328865\n",
            "{SqueezeNet} Train Epoch: 1 [30720/37814 (81%)]\tLoss: 2.331921\n",
            "{SqueezeNet} Train Epoch: 1 [31232/37814 (82%)]\tLoss: 2.332610\n",
            "{SqueezeNet} Train Epoch: 1 [31744/37814 (84%)]\tLoss: 2.357997\n",
            "{SqueezeNet} Train Epoch: 1 [32256/37814 (85%)]\tLoss: 2.314977\n",
            "{SqueezeNet} Train Epoch: 1 [32768/37814 (86%)]\tLoss: 2.315897\n",
            "{SqueezeNet} Train Epoch: 1 [33280/37814 (88%)]\tLoss: 2.326724\n",
            "{SqueezeNet} Train Epoch: 1 [33792/37814 (89%)]\tLoss: 2.330273\n",
            "{SqueezeNet} Train Epoch: 1 [34304/37814 (91%)]\tLoss: 2.308355\n",
            "{SqueezeNet} Train Epoch: 1 [34816/37814 (92%)]\tLoss: 2.332984\n",
            "{SqueezeNet} Train Epoch: 1 [35328/37814 (93%)]\tLoss: 2.299103\n",
            "{SqueezeNet} Train Epoch: 1 [35840/37814 (95%)]\tLoss: 2.292705\n",
            "{SqueezeNet} Train Epoch: 1 [36352/37814 (96%)]\tLoss: 2.314591\n",
            "{SqueezeNet} Train Epoch: 1 [36864/37814 (97%)]\tLoss: 2.311912\n",
            "{SqueezeNet} Train Epoch: 1 [31974/37814 (99%)]\tLoss: 2.311197\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.3032, Accuracy: 569/5000 (11%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.41761875152588 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a89a9da0-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"87b3445c-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cfb8dea2d1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a89c4ea2-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_77f54c1e11"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a89ca456-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_48588ce5a8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a89ceda8-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a89ca456-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_a205a6b7de"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8c0730e-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a89c4ea2-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_99e9ea8588"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8c13ef6-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_d0b2d0e379"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8c17c4a-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_5546b379b5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8c20dfe-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a8c17c4a-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0d17712a28"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 2 [0/37814 (0%)]\tLoss: 4.883925\n",
            "{AlexNet} Train Epoch: 2 [512/37814 (1%)]\tLoss: 4.936111\n",
            "{AlexNet} Train Epoch: 2 [1024/37814 (3%)]\tLoss: 5.025792\n",
            "{AlexNet} Train Epoch: 2 [1536/37814 (4%)]\tLoss: 4.836839\n",
            "{AlexNet} Train Epoch: 2 [2048/37814 (5%)]\tLoss: 4.968725\n",
            "{AlexNet} Train Epoch: 2 [2560/37814 (7%)]\tLoss: 4.748898\n",
            "{AlexNet} Train Epoch: 2 [3072/37814 (8%)]\tLoss: 4.414847\n",
            "{AlexNet} Train Epoch: 2 [3584/37814 (9%)]\tLoss: 4.481450\n",
            "{AlexNet} Train Epoch: 2 [4096/37814 (11%)]\tLoss: 4.318094\n",
            "{AlexNet} Train Epoch: 2 [4608/37814 (12%)]\tLoss: 4.195436\n",
            "{AlexNet} Train Epoch: 2 [5120/37814 (14%)]\tLoss: 4.184640\n",
            "{AlexNet} Train Epoch: 2 [5632/37814 (15%)]\tLoss: 4.337021\n",
            "{AlexNet} Train Epoch: 2 [6144/37814 (16%)]\tLoss: 3.949127\n",
            "{AlexNet} Train Epoch: 2 [6656/37814 (18%)]\tLoss: 3.797021\n",
            "{AlexNet} Train Epoch: 2 [7168/37814 (19%)]\tLoss: 3.739149\n",
            "{AlexNet} Train Epoch: 2 [7680/37814 (20%)]\tLoss: 3.745583\n",
            "{AlexNet} Train Epoch: 2 [8192/37814 (22%)]\tLoss: 3.494956\n",
            "{AlexNet} Train Epoch: 2 [8704/37814 (23%)]\tLoss: 3.390465\n",
            "{AlexNet} Train Epoch: 2 [9216/37814 (24%)]\tLoss: 3.539281\n",
            "{AlexNet} Train Epoch: 2 [9728/37814 (26%)]\tLoss: 3.251258\n",
            "{AlexNet} Train Epoch: 2 [10240/37814 (27%)]\tLoss: 3.105235\n",
            "{AlexNet} Train Epoch: 2 [10752/37814 (28%)]\tLoss: 3.001041\n",
            "{AlexNet} Train Epoch: 2 [11264/37814 (30%)]\tLoss: 2.818289\n",
            "{AlexNet} Train Epoch: 2 [11776/37814 (31%)]\tLoss: 2.670292\n",
            "{AlexNet} Train Epoch: 2 [12288/37814 (32%)]\tLoss: 2.606408\n",
            "{AlexNet} Train Epoch: 2 [12800/37814 (34%)]\tLoss: 2.531873\n",
            "{AlexNet} Train Epoch: 2 [13312/37814 (35%)]\tLoss: 2.458778\n",
            "{AlexNet} Train Epoch: 2 [13824/37814 (36%)]\tLoss: 2.434697\n",
            "{AlexNet} Train Epoch: 2 [14336/37814 (38%)]\tLoss: 2.451596\n",
            "{AlexNet} Train Epoch: 2 [14848/37814 (39%)]\tLoss: 2.494145\n",
            "{AlexNet} Train Epoch: 2 [15360/37814 (41%)]\tLoss: 2.511346\n",
            "{AlexNet} Train Epoch: 2 [15872/37814 (42%)]\tLoss: 2.481111\n",
            "{AlexNet} Train Epoch: 2 [16384/37814 (43%)]\tLoss: 2.383971\n",
            "{AlexNet} Train Epoch: 2 [16896/37814 (45%)]\tLoss: 2.408160\n",
            "{AlexNet} Train Epoch: 2 [17408/37814 (46%)]\tLoss: 2.424016\n",
            "{AlexNet} Train Epoch: 2 [17920/37814 (47%)]\tLoss: 2.437443\n",
            "{AlexNet} Train Epoch: 2 [18432/37814 (49%)]\tLoss: 2.406590\n",
            "{AlexNet} Train Epoch: 2 [18944/37814 (50%)]\tLoss: 2.384011\n",
            "{AlexNet} Train Epoch: 2 [19456/37814 (51%)]\tLoss: 2.383114\n",
            "{AlexNet} Train Epoch: 2 [19968/37814 (53%)]\tLoss: 2.344962\n",
            "{AlexNet} Train Epoch: 2 [20480/37814 (54%)]\tLoss: 2.371301\n",
            "{AlexNet} Train Epoch: 2 [20992/37814 (55%)]\tLoss: 2.350842\n",
            "{AlexNet} Train Epoch: 2 [21504/37814 (57%)]\tLoss: 2.350430\n",
            "{AlexNet} Train Epoch: 2 [22016/37814 (58%)]\tLoss: 2.347829\n",
            "{AlexNet} Train Epoch: 2 [22528/37814 (59%)]\tLoss: 2.339700\n",
            "{AlexNet} Train Epoch: 2 [23040/37814 (61%)]\tLoss: 2.345671\n",
            "{AlexNet} Train Epoch: 2 [23552/37814 (62%)]\tLoss: 2.329306\n",
            "{AlexNet} Train Epoch: 2 [24064/37814 (64%)]\tLoss: 2.323277\n",
            "{AlexNet} Train Epoch: 2 [24576/37814 (65%)]\tLoss: 2.324461\n",
            "{AlexNet} Train Epoch: 2 [25088/37814 (66%)]\tLoss: 2.347870\n",
            "{AlexNet} Train Epoch: 2 [25600/37814 (68%)]\tLoss: 2.336396\n",
            "{AlexNet} Train Epoch: 2 [26112/37814 (69%)]\tLoss: 2.345517\n",
            "{AlexNet} Train Epoch: 2 [26624/37814 (70%)]\tLoss: 2.351790\n",
            "{AlexNet} Train Epoch: 2 [27136/37814 (72%)]\tLoss: 2.318149\n",
            "{AlexNet} Train Epoch: 2 [27648/37814 (73%)]\tLoss: 2.318598\n",
            "{AlexNet} Train Epoch: 2 [28160/37814 (74%)]\tLoss: 2.313649\n",
            "{AlexNet} Train Epoch: 2 [28672/37814 (76%)]\tLoss: 2.325500\n",
            "{AlexNet} Train Epoch: 2 [29184/37814 (77%)]\tLoss: 2.323717\n",
            "{AlexNet} Train Epoch: 2 [29696/37814 (78%)]\tLoss: 2.341035\n",
            "{AlexNet} Train Epoch: 2 [30208/37814 (80%)]\tLoss: 2.326885\n",
            "{AlexNet} Train Epoch: 2 [30720/37814 (81%)]\tLoss: 2.352927\n",
            "{AlexNet} Train Epoch: 2 [31232/37814 (82%)]\tLoss: 2.341811\n",
            "{AlexNet} Train Epoch: 2 [31744/37814 (84%)]\tLoss: 2.332542\n",
            "{AlexNet} Train Epoch: 2 [32256/37814 (85%)]\tLoss: 2.319729\n",
            "{AlexNet} Train Epoch: 2 [32768/37814 (86%)]\tLoss: 2.317424\n",
            "{AlexNet} Train Epoch: 2 [33280/37814 (88%)]\tLoss: 2.321058\n",
            "{AlexNet} Train Epoch: 2 [33792/37814 (89%)]\tLoss: 2.343636\n",
            "{AlexNet} Train Epoch: 2 [34304/37814 (91%)]\tLoss: 2.349910\n",
            "{AlexNet} Train Epoch: 2 [34816/37814 (92%)]\tLoss: 2.345430\n",
            "{AlexNet} Train Epoch: 2 [35328/37814 (93%)]\tLoss: 2.325498\n",
            "{AlexNet} Train Epoch: 2 [35840/37814 (95%)]\tLoss: 2.314970\n",
            "{AlexNet} Train Epoch: 2 [36352/37814 (96%)]\tLoss: 2.313470\n",
            "{AlexNet} Train Epoch: 2 [36864/37814 (97%)]\tLoss: 2.326927\n",
            "{AlexNet} Train Epoch: 2 [31974/37814 (99%)]\tLoss: 2.331646\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.3356, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.910265922546387 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 2 [0/37814 (0%)]\tLoss: 2.328618\n",
            "{SqueezeNet} Train Epoch: 2 [512/37814 (1%)]\tLoss: 2.318913\n",
            "{SqueezeNet} Train Epoch: 2 [1024/37814 (3%)]\tLoss: 2.327650\n",
            "{SqueezeNet} Train Epoch: 2 [1536/37814 (4%)]\tLoss: 2.304732\n",
            "{SqueezeNet} Train Epoch: 2 [2048/37814 (5%)]\tLoss: 2.306269\n",
            "{SqueezeNet} Train Epoch: 2 [2560/37814 (7%)]\tLoss: 2.329924\n",
            "{SqueezeNet} Train Epoch: 2 [3072/37814 (8%)]\tLoss: 2.321679\n",
            "{SqueezeNet} Train Epoch: 2 [3584/37814 (9%)]\tLoss: 2.308737\n",
            "{SqueezeNet} Train Epoch: 2 [4096/37814 (11%)]\tLoss: 2.303535\n",
            "{SqueezeNet} Train Epoch: 2 [4608/37814 (12%)]\tLoss: 2.297050\n",
            "{SqueezeNet} Train Epoch: 2 [5120/37814 (14%)]\tLoss: 2.321211\n",
            "{SqueezeNet} Train Epoch: 2 [5632/37814 (15%)]\tLoss: 2.308346\n",
            "{SqueezeNet} Train Epoch: 2 [6144/37814 (16%)]\tLoss: 2.295964\n",
            "{SqueezeNet} Train Epoch: 2 [6656/37814 (18%)]\tLoss: 2.300939\n",
            "{SqueezeNet} Train Epoch: 2 [7168/37814 (19%)]\tLoss: 2.294414\n",
            "{SqueezeNet} Train Epoch: 2 [7680/37814 (20%)]\tLoss: 2.298065\n",
            "{SqueezeNet} Train Epoch: 2 [8192/37814 (22%)]\tLoss: 2.299453\n",
            "{SqueezeNet} Train Epoch: 2 [8704/37814 (23%)]\tLoss: 2.313732\n",
            "{SqueezeNet} Train Epoch: 2 [9216/37814 (24%)]\tLoss: 2.304784\n",
            "{SqueezeNet} Train Epoch: 2 [9728/37814 (26%)]\tLoss: 2.314883\n",
            "{SqueezeNet} Train Epoch: 2 [10240/37814 (27%)]\tLoss: 2.282187\n",
            "{SqueezeNet} Train Epoch: 2 [10752/37814 (28%)]\tLoss: 2.294341\n",
            "{SqueezeNet} Train Epoch: 2 [11264/37814 (30%)]\tLoss: 2.308510\n",
            "{SqueezeNet} Train Epoch: 2 [11776/37814 (31%)]\tLoss: 2.298834\n",
            "{SqueezeNet} Train Epoch: 2 [12288/37814 (32%)]\tLoss: 2.288232\n",
            "{SqueezeNet} Train Epoch: 2 [12800/37814 (34%)]\tLoss: 2.292133\n",
            "{SqueezeNet} Train Epoch: 2 [13312/37814 (35%)]\tLoss: 2.303268\n",
            "{SqueezeNet} Train Epoch: 2 [13824/37814 (36%)]\tLoss: 2.302023\n",
            "{SqueezeNet} Train Epoch: 2 [14336/37814 (38%)]\tLoss: 2.289267\n",
            "{SqueezeNet} Train Epoch: 2 [14848/37814 (39%)]\tLoss: 2.293058\n",
            "{SqueezeNet} Train Epoch: 2 [15360/37814 (41%)]\tLoss: 2.282815\n",
            "{SqueezeNet} Train Epoch: 2 [15872/37814 (42%)]\tLoss: 2.296347\n",
            "{SqueezeNet} Train Epoch: 2 [16384/37814 (43%)]\tLoss: 2.265377\n",
            "{SqueezeNet} Train Epoch: 2 [16896/37814 (45%)]\tLoss: 2.321478\n",
            "{SqueezeNet} Train Epoch: 2 [17408/37814 (46%)]\tLoss: 2.329604\n",
            "{SqueezeNet} Train Epoch: 2 [17920/37814 (47%)]\tLoss: 2.296828\n",
            "{SqueezeNet} Train Epoch: 2 [18432/37814 (49%)]\tLoss: 2.297539\n",
            "{SqueezeNet} Train Epoch: 2 [18944/37814 (50%)]\tLoss: 2.279699\n",
            "{SqueezeNet} Train Epoch: 2 [19456/37814 (51%)]\tLoss: 2.311396\n",
            "{SqueezeNet} Train Epoch: 2 [19968/37814 (53%)]\tLoss: 2.307699\n",
            "{SqueezeNet} Train Epoch: 2 [20480/37814 (54%)]\tLoss: 2.280492\n",
            "{SqueezeNet} Train Epoch: 2 [20992/37814 (55%)]\tLoss: 2.281745\n",
            "{SqueezeNet} Train Epoch: 2 [21504/37814 (57%)]\tLoss: 2.292284\n",
            "{SqueezeNet} Train Epoch: 2 [22016/37814 (58%)]\tLoss: 2.286369\n",
            "{SqueezeNet} Train Epoch: 2 [22528/37814 (59%)]\tLoss: 2.288749\n",
            "{SqueezeNet} Train Epoch: 2 [23040/37814 (61%)]\tLoss: 2.298973\n",
            "{SqueezeNet} Train Epoch: 2 [23552/37814 (62%)]\tLoss: 2.284598\n",
            "{SqueezeNet} Train Epoch: 2 [24064/37814 (64%)]\tLoss: 2.290699\n",
            "{SqueezeNet} Train Epoch: 2 [24576/37814 (65%)]\tLoss: 2.291827\n",
            "{SqueezeNet} Train Epoch: 2 [25088/37814 (66%)]\tLoss: 2.280424\n",
            "{SqueezeNet} Train Epoch: 2 [25600/37814 (68%)]\tLoss: 2.273931\n",
            "{SqueezeNet} Train Epoch: 2 [26112/37814 (69%)]\tLoss: 2.288780\n",
            "{SqueezeNet} Train Epoch: 2 [26624/37814 (70%)]\tLoss: 2.264734\n",
            "{SqueezeNet} Train Epoch: 2 [27136/37814 (72%)]\tLoss: 2.266327\n",
            "{SqueezeNet} Train Epoch: 2 [27648/37814 (73%)]\tLoss: 2.266948\n",
            "{SqueezeNet} Train Epoch: 2 [28160/37814 (74%)]\tLoss: 2.263583\n",
            "{SqueezeNet} Train Epoch: 2 [28672/37814 (76%)]\tLoss: 2.263238\n",
            "{SqueezeNet} Train Epoch: 2 [29184/37814 (77%)]\tLoss: 2.251461\n",
            "{SqueezeNet} Train Epoch: 2 [29696/37814 (78%)]\tLoss: 2.254863\n",
            "{SqueezeNet} Train Epoch: 2 [30208/37814 (80%)]\tLoss: 2.253477\n",
            "{SqueezeNet} Train Epoch: 2 [30720/37814 (81%)]\tLoss: 2.268329\n",
            "{SqueezeNet} Train Epoch: 2 [31232/37814 (82%)]\tLoss: 2.228184\n",
            "{SqueezeNet} Train Epoch: 2 [31744/37814 (84%)]\tLoss: 2.235965\n",
            "{SqueezeNet} Train Epoch: 2 [32256/37814 (85%)]\tLoss: 2.234632\n",
            "{SqueezeNet} Train Epoch: 2 [32768/37814 (86%)]\tLoss: 2.212992\n",
            "{SqueezeNet} Train Epoch: 2 [33280/37814 (88%)]\tLoss: 2.215459\n",
            "{SqueezeNet} Train Epoch: 2 [33792/37814 (89%)]\tLoss: 2.242002\n",
            "{SqueezeNet} Train Epoch: 2 [34304/37814 (91%)]\tLoss: 2.257997\n",
            "{SqueezeNet} Train Epoch: 2 [34816/37814 (92%)]\tLoss: 2.217653\n",
            "{SqueezeNet} Train Epoch: 2 [35328/37814 (93%)]\tLoss: 2.212929\n",
            "{SqueezeNet} Train Epoch: 2 [35840/37814 (95%)]\tLoss: 2.227373\n",
            "{SqueezeNet} Train Epoch: 2 [36352/37814 (96%)]\tLoss: 2.204648\n",
            "{SqueezeNet} Train Epoch: 2 [36864/37814 (97%)]\tLoss: 2.196660\n",
            "{SqueezeNet} Train Epoch: 2 [31974/37814 (99%)]\tLoss: 2.188144\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.1767, Accuracy: 993/5000 (20%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.459594011306763 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca5ca5a0-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a8c13ef6-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_71cf8cb4fd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca5e3870-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_52acac4ef6"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca5e7c90-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_f9318eff83"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca5ec664-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca5e7c90-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_942b3cddb3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca824490-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca5e3870-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_61889904ac"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca83de36-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_b0caa28f74"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca842b3e-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_713954468d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca845eb0-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca842b3e-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_93d9ae51f0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 3 [0/37814 (0%)]\tLoss: 2.338313\n",
            "{AlexNet} Train Epoch: 3 [512/37814 (1%)]\tLoss: 2.313453\n",
            "{AlexNet} Train Epoch: 3 [1024/37814 (3%)]\tLoss: 2.315055\n",
            "{AlexNet} Train Epoch: 3 [1536/37814 (4%)]\tLoss: 2.316378\n",
            "{AlexNet} Train Epoch: 3 [2048/37814 (5%)]\tLoss: 2.308141\n",
            "{AlexNet} Train Epoch: 3 [2560/37814 (7%)]\tLoss: 2.316074\n",
            "{AlexNet} Train Epoch: 3 [3072/37814 (8%)]\tLoss: 2.307977\n",
            "{AlexNet} Train Epoch: 3 [3584/37814 (9%)]\tLoss: 2.309323\n",
            "{AlexNet} Train Epoch: 3 [4096/37814 (11%)]\tLoss: 2.313622\n",
            "{AlexNet} Train Epoch: 3 [4608/37814 (12%)]\tLoss: 2.310448\n",
            "{AlexNet} Train Epoch: 3 [5120/37814 (14%)]\tLoss: 2.311171\n",
            "{AlexNet} Train Epoch: 3 [5632/37814 (15%)]\tLoss: 2.314178\n",
            "{AlexNet} Train Epoch: 3 [6144/37814 (16%)]\tLoss: 2.303072\n",
            "{AlexNet} Train Epoch: 3 [6656/37814 (18%)]\tLoss: 2.306572\n",
            "{AlexNet} Train Epoch: 3 [7168/37814 (19%)]\tLoss: 2.310498\n",
            "{AlexNet} Train Epoch: 3 [7680/37814 (20%)]\tLoss: 2.305656\n",
            "{AlexNet} Train Epoch: 3 [8192/37814 (22%)]\tLoss: 2.314140\n",
            "{AlexNet} Train Epoch: 3 [8704/37814 (23%)]\tLoss: 2.311850\n",
            "{AlexNet} Train Epoch: 3 [9216/37814 (24%)]\tLoss: 2.326224\n",
            "{AlexNet} Train Epoch: 3 [9728/37814 (26%)]\tLoss: 2.317125\n",
            "{AlexNet} Train Epoch: 3 [10240/37814 (27%)]\tLoss: 2.306666\n",
            "{AlexNet} Train Epoch: 3 [10752/37814 (28%)]\tLoss: 2.310986\n",
            "{AlexNet} Train Epoch: 3 [11264/37814 (30%)]\tLoss: 2.317523\n",
            "{AlexNet} Train Epoch: 3 [11776/37814 (31%)]\tLoss: 2.322531\n",
            "{AlexNet} Train Epoch: 3 [12288/37814 (32%)]\tLoss: 2.334707\n",
            "{AlexNet} Train Epoch: 3 [12800/37814 (34%)]\tLoss: 2.312147\n",
            "{AlexNet} Train Epoch: 3 [13312/37814 (35%)]\tLoss: 2.305878\n",
            "{AlexNet} Train Epoch: 3 [13824/37814 (36%)]\tLoss: 2.318550\n",
            "{AlexNet} Train Epoch: 3 [14336/37814 (38%)]\tLoss: 2.313743\n",
            "{AlexNet} Train Epoch: 3 [14848/37814 (39%)]\tLoss: 2.308031\n",
            "{AlexNet} Train Epoch: 3 [15360/37814 (41%)]\tLoss: 2.314805\n",
            "{AlexNet} Train Epoch: 3 [15872/37814 (42%)]\tLoss: 2.313117\n",
            "{AlexNet} Train Epoch: 3 [16384/37814 (43%)]\tLoss: 2.323903\n",
            "{AlexNet} Train Epoch: 3 [16896/37814 (45%)]\tLoss: 2.316945\n",
            "{AlexNet} Train Epoch: 3 [17408/37814 (46%)]\tLoss: 2.314510\n",
            "{AlexNet} Train Epoch: 3 [17920/37814 (47%)]\tLoss: 2.316054\n",
            "{AlexNet} Train Epoch: 3 [18432/37814 (49%)]\tLoss: 2.304775\n",
            "{AlexNet} Train Epoch: 3 [18944/37814 (50%)]\tLoss: 2.312495\n",
            "{AlexNet} Train Epoch: 3 [19456/37814 (51%)]\tLoss: 2.313661\n",
            "{AlexNet} Train Epoch: 3 [19968/37814 (53%)]\tLoss: 2.307029\n",
            "{AlexNet} Train Epoch: 3 [20480/37814 (54%)]\tLoss: 2.312344\n",
            "{AlexNet} Train Epoch: 3 [20992/37814 (55%)]\tLoss: 2.319836\n",
            "{AlexNet} Train Epoch: 3 [21504/37814 (57%)]\tLoss: 2.312144\n",
            "{AlexNet} Train Epoch: 3 [22016/37814 (58%)]\tLoss: 2.309473\n",
            "{AlexNet} Train Epoch: 3 [22528/37814 (59%)]\tLoss: 2.302375\n",
            "{AlexNet} Train Epoch: 3 [23040/37814 (61%)]\tLoss: 2.318584\n",
            "{AlexNet} Train Epoch: 3 [23552/37814 (62%)]\tLoss: 2.310898\n",
            "{AlexNet} Train Epoch: 3 [24064/37814 (64%)]\tLoss: 2.306690\n",
            "{AlexNet} Train Epoch: 3 [24576/37814 (65%)]\tLoss: 2.305464\n",
            "{AlexNet} Train Epoch: 3 [25088/37814 (66%)]\tLoss: 2.308267\n",
            "{AlexNet} Train Epoch: 3 [25600/37814 (68%)]\tLoss: 2.301900\n",
            "{AlexNet} Train Epoch: 3 [26112/37814 (69%)]\tLoss: 2.306096\n",
            "{AlexNet} Train Epoch: 3 [26624/37814 (70%)]\tLoss: 2.309074\n",
            "{AlexNet} Train Epoch: 3 [27136/37814 (72%)]\tLoss: 2.309868\n",
            "{AlexNet} Train Epoch: 3 [27648/37814 (73%)]\tLoss: 2.311498\n",
            "{AlexNet} Train Epoch: 3 [28160/37814 (74%)]\tLoss: 2.300809\n",
            "{AlexNet} Train Epoch: 3 [28672/37814 (76%)]\tLoss: 2.299803\n",
            "{AlexNet} Train Epoch: 3 [29184/37814 (77%)]\tLoss: 2.311536\n",
            "{AlexNet} Train Epoch: 3 [29696/37814 (78%)]\tLoss: 2.319931\n",
            "{AlexNet} Train Epoch: 3 [30208/37814 (80%)]\tLoss: 2.313184\n",
            "{AlexNet} Train Epoch: 3 [30720/37814 (81%)]\tLoss: 2.302930\n",
            "{AlexNet} Train Epoch: 3 [31232/37814 (82%)]\tLoss: 2.311189\n",
            "{AlexNet} Train Epoch: 3 [31744/37814 (84%)]\tLoss: 2.309092\n",
            "{AlexNet} Train Epoch: 3 [32256/37814 (85%)]\tLoss: 2.325495\n",
            "{AlexNet} Train Epoch: 3 [32768/37814 (86%)]\tLoss: 2.325369\n",
            "{AlexNet} Train Epoch: 3 [33280/37814 (88%)]\tLoss: 2.311720\n",
            "{AlexNet} Train Epoch: 3 [33792/37814 (89%)]\tLoss: 2.307286\n",
            "{AlexNet} Train Epoch: 3 [34304/37814 (91%)]\tLoss: 2.306902\n",
            "{AlexNet} Train Epoch: 3 [34816/37814 (92%)]\tLoss: 2.306203\n",
            "{AlexNet} Train Epoch: 3 [35328/37814 (93%)]\tLoss: 2.322383\n",
            "{AlexNet} Train Epoch: 3 [35840/37814 (95%)]\tLoss: 2.303284\n",
            "{AlexNet} Train Epoch: 3 [36352/37814 (96%)]\tLoss: 2.328685\n",
            "{AlexNet} Train Epoch: 3 [36864/37814 (97%)]\tLoss: 2.312248\n",
            "{AlexNet} Train Epoch: 3 [31974/37814 (99%)]\tLoss: 2.313422\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.3109, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 28.146929264068604 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 3 [0/37814 (0%)]\tLoss: 2.162189\n",
            "{SqueezeNet} Train Epoch: 3 [512/37814 (1%)]\tLoss: 2.187493\n",
            "{SqueezeNet} Train Epoch: 3 [1024/37814 (3%)]\tLoss: 2.176949\n",
            "{SqueezeNet} Train Epoch: 3 [1536/37814 (4%)]\tLoss: 2.213290\n",
            "{SqueezeNet} Train Epoch: 3 [2048/37814 (5%)]\tLoss: 2.172039\n",
            "{SqueezeNet} Train Epoch: 3 [2560/37814 (7%)]\tLoss: 2.189482\n",
            "{SqueezeNet} Train Epoch: 3 [3072/37814 (8%)]\tLoss: 2.199529\n",
            "{SqueezeNet} Train Epoch: 3 [3584/37814 (9%)]\tLoss: 2.194268\n",
            "{SqueezeNet} Train Epoch: 3 [4096/37814 (11%)]\tLoss: 2.195171\n",
            "{SqueezeNet} Train Epoch: 3 [4608/37814 (12%)]\tLoss: 2.151003\n",
            "{SqueezeNet} Train Epoch: 3 [5120/37814 (14%)]\tLoss: 2.138710\n",
            "{SqueezeNet} Train Epoch: 3 [5632/37814 (15%)]\tLoss: 2.176982\n",
            "{SqueezeNet} Train Epoch: 3 [6144/37814 (16%)]\tLoss: 2.181961\n",
            "{SqueezeNet} Train Epoch: 3 [6656/37814 (18%)]\tLoss: 2.200706\n",
            "{SqueezeNet} Train Epoch: 3 [7168/37814 (19%)]\tLoss: 2.136870\n",
            "{SqueezeNet} Train Epoch: 3 [7680/37814 (20%)]\tLoss: 2.158201\n",
            "{SqueezeNet} Train Epoch: 3 [8192/37814 (22%)]\tLoss: 2.128858\n",
            "{SqueezeNet} Train Epoch: 3 [8704/37814 (23%)]\tLoss: 2.177169\n",
            "{SqueezeNet} Train Epoch: 3 [9216/37814 (24%)]\tLoss: 2.106868\n",
            "{SqueezeNet} Train Epoch: 3 [9728/37814 (26%)]\tLoss: 2.149971\n",
            "{SqueezeNet} Train Epoch: 3 [10240/37814 (27%)]\tLoss: 2.143596\n",
            "{SqueezeNet} Train Epoch: 3 [10752/37814 (28%)]\tLoss: 2.112386\n",
            "{SqueezeNet} Train Epoch: 3 [11264/37814 (30%)]\tLoss: 2.128852\n",
            "{SqueezeNet} Train Epoch: 3 [11776/37814 (31%)]\tLoss: 2.118821\n",
            "{SqueezeNet} Train Epoch: 3 [12288/37814 (32%)]\tLoss: 2.166559\n",
            "{SqueezeNet} Train Epoch: 3 [12800/37814 (34%)]\tLoss: 2.157849\n",
            "{SqueezeNet} Train Epoch: 3 [13312/37814 (35%)]\tLoss: 2.150193\n",
            "{SqueezeNet} Train Epoch: 3 [13824/37814 (36%)]\tLoss: 2.113160\n",
            "{SqueezeNet} Train Epoch: 3 [14336/37814 (38%)]\tLoss: 2.154826\n",
            "{SqueezeNet} Train Epoch: 3 [14848/37814 (39%)]\tLoss: 2.177425\n",
            "{SqueezeNet} Train Epoch: 3 [15360/37814 (41%)]\tLoss: 2.121214\n",
            "{SqueezeNet} Train Epoch: 3 [15872/37814 (42%)]\tLoss: 2.144539\n",
            "{SqueezeNet} Train Epoch: 3 [16384/37814 (43%)]\tLoss: 2.162597\n",
            "{SqueezeNet} Train Epoch: 3 [16896/37814 (45%)]\tLoss: 2.123285\n",
            "{SqueezeNet} Train Epoch: 3 [17408/37814 (46%)]\tLoss: 2.128700\n",
            "{SqueezeNet} Train Epoch: 3 [17920/37814 (47%)]\tLoss: 2.106882\n",
            "{SqueezeNet} Train Epoch: 3 [18432/37814 (49%)]\tLoss: 2.155334\n",
            "{SqueezeNet} Train Epoch: 3 [18944/37814 (50%)]\tLoss: 2.070196\n",
            "{SqueezeNet} Train Epoch: 3 [19456/37814 (51%)]\tLoss: 2.082443\n",
            "{SqueezeNet} Train Epoch: 3 [19968/37814 (53%)]\tLoss: 2.135744\n",
            "{SqueezeNet} Train Epoch: 3 [20480/37814 (54%)]\tLoss: 2.090091\n",
            "{SqueezeNet} Train Epoch: 3 [20992/37814 (55%)]\tLoss: 2.037165\n",
            "{SqueezeNet} Train Epoch: 3 [21504/37814 (57%)]\tLoss: 2.123769\n",
            "{SqueezeNet} Train Epoch: 3 [22016/37814 (58%)]\tLoss: 2.096663\n",
            "{SqueezeNet} Train Epoch: 3 [22528/37814 (59%)]\tLoss: 2.032943\n",
            "{SqueezeNet} Train Epoch: 3 [23040/37814 (61%)]\tLoss: 2.093860\n",
            "{SqueezeNet} Train Epoch: 3 [23552/37814 (62%)]\tLoss: 2.105843\n",
            "{SqueezeNet} Train Epoch: 3 [24064/37814 (64%)]\tLoss: 2.050225\n",
            "{SqueezeNet} Train Epoch: 3 [24576/37814 (65%)]\tLoss: 2.114104\n",
            "{SqueezeNet} Train Epoch: 3 [25088/37814 (66%)]\tLoss: 2.204223\n",
            "{SqueezeNet} Train Epoch: 3 [25600/37814 (68%)]\tLoss: 2.168983\n",
            "{SqueezeNet} Train Epoch: 3 [26112/37814 (69%)]\tLoss: 2.118051\n",
            "{SqueezeNet} Train Epoch: 3 [26624/37814 (70%)]\tLoss: 2.175346\n",
            "{SqueezeNet} Train Epoch: 3 [27136/37814 (72%)]\tLoss: 2.117625\n",
            "{SqueezeNet} Train Epoch: 3 [27648/37814 (73%)]\tLoss: 2.276978\n",
            "{SqueezeNet} Train Epoch: 3 [28160/37814 (74%)]\tLoss: 2.099160\n",
            "{SqueezeNet} Train Epoch: 3 [28672/37814 (76%)]\tLoss: 2.193388\n",
            "{SqueezeNet} Train Epoch: 3 [29184/37814 (77%)]\tLoss: 2.165318\n",
            "{SqueezeNet} Train Epoch: 3 [29696/37814 (78%)]\tLoss: 2.174362\n",
            "{SqueezeNet} Train Epoch: 3 [30208/37814 (80%)]\tLoss: 2.197374\n",
            "{SqueezeNet} Train Epoch: 3 [30720/37814 (81%)]\tLoss: 2.107959\n",
            "{SqueezeNet} Train Epoch: 3 [31232/37814 (82%)]\tLoss: 2.118642\n",
            "{SqueezeNet} Train Epoch: 3 [31744/37814 (84%)]\tLoss: 2.168617\n",
            "{SqueezeNet} Train Epoch: 3 [32256/37814 (85%)]\tLoss: 2.180166\n",
            "{SqueezeNet} Train Epoch: 3 [32768/37814 (86%)]\tLoss: 2.092869\n",
            "{SqueezeNet} Train Epoch: 3 [33280/37814 (88%)]\tLoss: 2.121948\n",
            "{SqueezeNet} Train Epoch: 3 [33792/37814 (89%)]\tLoss: 2.094953\n",
            "{SqueezeNet} Train Epoch: 3 [34304/37814 (91%)]\tLoss: 2.126771\n",
            "{SqueezeNet} Train Epoch: 3 [34816/37814 (92%)]\tLoss: 2.049423\n",
            "{SqueezeNet} Train Epoch: 3 [35328/37814 (93%)]\tLoss: 2.112920\n",
            "{SqueezeNet} Train Epoch: 3 [35840/37814 (95%)]\tLoss: 2.126133\n",
            "{SqueezeNet} Train Epoch: 3 [36352/37814 (96%)]\tLoss: 2.090293\n",
            "{SqueezeNet} Train Epoch: 3 [36864/37814 (97%)]\tLoss: 2.066787\n",
            "{SqueezeNet} Train Epoch: 3 [31974/37814 (99%)]\tLoss: 2.154000\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.0646, Accuracy: 1035/5000 (21%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.272756099700928 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ecbf4404-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca83de36-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f34e3b5ab3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ecc0df1c-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_c30bafcf05"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ecc1236e-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_8e08c93d8b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ecc166e4-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ecc1236e-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c2fed2bef5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ece520ca-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ecc0df1c-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_353d40c61e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ece80452-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_67762f2ce1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ece8a24a-60e2-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_0dfafc0e2c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ece93c82-60e2-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ece8a24a-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_7b1b4119fc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 4 [0/37814 (0%)]\tLoss: 2.315112\n",
            "{AlexNet} Train Epoch: 4 [512/37814 (1%)]\tLoss: 2.305570\n",
            "{AlexNet} Train Epoch: 4 [1024/37814 (3%)]\tLoss: 2.307846\n",
            "{AlexNet} Train Epoch: 4 [1536/37814 (4%)]\tLoss: 2.307440\n",
            "{AlexNet} Train Epoch: 4 [2048/37814 (5%)]\tLoss: 2.314058\n",
            "{AlexNet} Train Epoch: 4 [2560/37814 (7%)]\tLoss: 2.319713\n",
            "{AlexNet} Train Epoch: 4 [3072/37814 (8%)]\tLoss: 2.306538\n",
            "{AlexNet} Train Epoch: 4 [3584/37814 (9%)]\tLoss: 2.309806\n",
            "{AlexNet} Train Epoch: 4 [4096/37814 (11%)]\tLoss: 2.312600\n",
            "{AlexNet} Train Epoch: 4 [4608/37814 (12%)]\tLoss: 2.317972\n",
            "{AlexNet} Train Epoch: 4 [5120/37814 (14%)]\tLoss: 2.309297\n",
            "{AlexNet} Train Epoch: 4 [5632/37814 (15%)]\tLoss: 2.304579\n",
            "{AlexNet} Train Epoch: 4 [6144/37814 (16%)]\tLoss: 2.309165\n",
            "{AlexNet} Train Epoch: 4 [6656/37814 (18%)]\tLoss: 2.305843\n",
            "{AlexNet} Train Epoch: 4 [7168/37814 (19%)]\tLoss: 2.302214\n",
            "{AlexNet} Train Epoch: 4 [7680/37814 (20%)]\tLoss: 2.338832\n",
            "{AlexNet} Train Epoch: 4 [8192/37814 (22%)]\tLoss: 2.311376\n",
            "{AlexNet} Train Epoch: 4 [8704/37814 (23%)]\tLoss: 2.308885\n",
            "{AlexNet} Train Epoch: 4 [9216/37814 (24%)]\tLoss: 2.307459\n",
            "{AlexNet} Train Epoch: 4 [9728/37814 (26%)]\tLoss: 2.308934\n",
            "{AlexNet} Train Epoch: 4 [10240/37814 (27%)]\tLoss: 2.294062\n",
            "{AlexNet} Train Epoch: 4 [10752/37814 (28%)]\tLoss: 2.312170\n",
            "{AlexNet} Train Epoch: 4 [11264/37814 (30%)]\tLoss: 2.312497\n",
            "{AlexNet} Train Epoch: 4 [11776/37814 (31%)]\tLoss: 2.312713\n",
            "{AlexNet} Train Epoch: 4 [12288/37814 (32%)]\tLoss: 2.321996\n",
            "{AlexNet} Train Epoch: 4 [12800/37814 (34%)]\tLoss: 2.314940\n",
            "{AlexNet} Train Epoch: 4 [13312/37814 (35%)]\tLoss: 2.304233\n",
            "{AlexNet} Train Epoch: 4 [13824/37814 (36%)]\tLoss: 2.308407\n",
            "{AlexNet} Train Epoch: 4 [14336/37814 (38%)]\tLoss: 2.305875\n",
            "{AlexNet} Train Epoch: 4 [14848/37814 (39%)]\tLoss: 2.313556\n",
            "{AlexNet} Train Epoch: 4 [15360/37814 (41%)]\tLoss: 2.303692\n",
            "{AlexNet} Train Epoch: 4 [15872/37814 (42%)]\tLoss: 2.309927\n",
            "{AlexNet} Train Epoch: 4 [16384/37814 (43%)]\tLoss: 2.312142\n",
            "{AlexNet} Train Epoch: 4 [16896/37814 (45%)]\tLoss: 2.309225\n",
            "{AlexNet} Train Epoch: 4 [17408/37814 (46%)]\tLoss: 2.316756\n",
            "{AlexNet} Train Epoch: 4 [17920/37814 (47%)]\tLoss: 2.311558\n",
            "{AlexNet} Train Epoch: 4 [18432/37814 (49%)]\tLoss: 2.306122\n",
            "{AlexNet} Train Epoch: 4 [18944/37814 (50%)]\tLoss: 2.311538\n",
            "{AlexNet} Train Epoch: 4 [19456/37814 (51%)]\tLoss: 2.308615\n",
            "{AlexNet} Train Epoch: 4 [19968/37814 (53%)]\tLoss: 2.331413\n",
            "{AlexNet} Train Epoch: 4 [20480/37814 (54%)]\tLoss: 2.299614\n",
            "{AlexNet} Train Epoch: 4 [20992/37814 (55%)]\tLoss: 2.316490\n",
            "{AlexNet} Train Epoch: 4 [21504/37814 (57%)]\tLoss: 2.301436\n",
            "{AlexNet} Train Epoch: 4 [22016/37814 (58%)]\tLoss: 2.310536\n",
            "{AlexNet} Train Epoch: 4 [22528/37814 (59%)]\tLoss: 2.316744\n",
            "{AlexNet} Train Epoch: 4 [23040/37814 (61%)]\tLoss: 2.308125\n",
            "{AlexNet} Train Epoch: 4 [23552/37814 (62%)]\tLoss: 2.301145\n",
            "{AlexNet} Train Epoch: 4 [24064/37814 (64%)]\tLoss: 2.326714\n",
            "{AlexNet} Train Epoch: 4 [24576/37814 (65%)]\tLoss: 2.317629\n",
            "{AlexNet} Train Epoch: 4 [25088/37814 (66%)]\tLoss: 2.324833\n",
            "{AlexNet} Train Epoch: 4 [25600/37814 (68%)]\tLoss: 2.306164\n",
            "{AlexNet} Train Epoch: 4 [26112/37814 (69%)]\tLoss: 2.310282\n",
            "{AlexNet} Train Epoch: 4 [26624/37814 (70%)]\tLoss: 2.318948\n",
            "{AlexNet} Train Epoch: 4 [27136/37814 (72%)]\tLoss: 2.313550\n",
            "{AlexNet} Train Epoch: 4 [27648/37814 (73%)]\tLoss: 2.317020\n",
            "{AlexNet} Train Epoch: 4 [28160/37814 (74%)]\tLoss: 2.299886\n",
            "{AlexNet} Train Epoch: 4 [28672/37814 (76%)]\tLoss: 2.310587\n",
            "{AlexNet} Train Epoch: 4 [29184/37814 (77%)]\tLoss: 2.317312\n",
            "{AlexNet} Train Epoch: 4 [29696/37814 (78%)]\tLoss: 2.311695\n",
            "{AlexNet} Train Epoch: 4 [30208/37814 (80%)]\tLoss: 2.311998\n",
            "{AlexNet} Train Epoch: 4 [30720/37814 (81%)]\tLoss: 2.310819\n",
            "{AlexNet} Train Epoch: 4 [31232/37814 (82%)]\tLoss: 2.307142\n",
            "{AlexNet} Train Epoch: 4 [31744/37814 (84%)]\tLoss: 2.311304\n",
            "{AlexNet} Train Epoch: 4 [32256/37814 (85%)]\tLoss: 2.293772\n",
            "{AlexNet} Train Epoch: 4 [32768/37814 (86%)]\tLoss: 2.309512\n",
            "{AlexNet} Train Epoch: 4 [33280/37814 (88%)]\tLoss: 2.310522\n",
            "{AlexNet} Train Epoch: 4 [33792/37814 (89%)]\tLoss: 2.321328\n",
            "{AlexNet} Train Epoch: 4 [34304/37814 (91%)]\tLoss: 2.306608\n",
            "{AlexNet} Train Epoch: 4 [34816/37814 (92%)]\tLoss: 2.307716\n",
            "{AlexNet} Train Epoch: 4 [35328/37814 (93%)]\tLoss: 2.309895\n",
            "{AlexNet} Train Epoch: 4 [35840/37814 (95%)]\tLoss: 2.307962\n",
            "{AlexNet} Train Epoch: 4 [36352/37814 (96%)]\tLoss: 2.303642\n",
            "{AlexNet} Train Epoch: 4 [36864/37814 (97%)]\tLoss: 2.323315\n",
            "{AlexNet} Train Epoch: 4 [31974/37814 (99%)]\tLoss: 2.311533\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.3152, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.83769726753235 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 4 [0/37814 (0%)]\tLoss: 2.063386\n",
            "{SqueezeNet} Train Epoch: 4 [512/37814 (1%)]\tLoss: 2.083853\n",
            "{SqueezeNet} Train Epoch: 4 [1024/37814 (3%)]\tLoss: 2.063818\n",
            "{SqueezeNet} Train Epoch: 4 [1536/37814 (4%)]\tLoss: 2.056782\n",
            "{SqueezeNet} Train Epoch: 4 [2048/37814 (5%)]\tLoss: 2.051281\n",
            "{SqueezeNet} Train Epoch: 4 [2560/37814 (7%)]\tLoss: 2.073291\n",
            "{SqueezeNet} Train Epoch: 4 [3072/37814 (8%)]\tLoss: 2.051101\n",
            "{SqueezeNet} Train Epoch: 4 [3584/37814 (9%)]\tLoss: 2.071213\n",
            "{SqueezeNet} Train Epoch: 4 [4096/37814 (11%)]\tLoss: 2.081643\n",
            "{SqueezeNet} Train Epoch: 4 [4608/37814 (12%)]\tLoss: 2.161440\n",
            "{SqueezeNet} Train Epoch: 4 [5120/37814 (14%)]\tLoss: 2.039466\n",
            "{SqueezeNet} Train Epoch: 4 [5632/37814 (15%)]\tLoss: 2.047479\n",
            "{SqueezeNet} Train Epoch: 4 [6144/37814 (16%)]\tLoss: 2.065335\n",
            "{SqueezeNet} Train Epoch: 4 [6656/37814 (18%)]\tLoss: 2.036442\n",
            "{SqueezeNet} Train Epoch: 4 [7168/37814 (19%)]\tLoss: 2.037262\n",
            "{SqueezeNet} Train Epoch: 4 [7680/37814 (20%)]\tLoss: 2.151872\n",
            "{SqueezeNet} Train Epoch: 4 [8192/37814 (22%)]\tLoss: 2.088209\n",
            "{SqueezeNet} Train Epoch: 4 [8704/37814 (23%)]\tLoss: 2.140301\n",
            "{SqueezeNet} Train Epoch: 4 [9216/37814 (24%)]\tLoss: 2.141831\n",
            "{SqueezeNet} Train Epoch: 4 [9728/37814 (26%)]\tLoss: 2.065843\n",
            "{SqueezeNet} Train Epoch: 4 [10240/37814 (27%)]\tLoss: 2.025304\n",
            "{SqueezeNet} Train Epoch: 4 [10752/37814 (28%)]\tLoss: 2.066878\n",
            "{SqueezeNet} Train Epoch: 4 [11264/37814 (30%)]\tLoss: 2.162047\n",
            "{SqueezeNet} Train Epoch: 4 [11776/37814 (31%)]\tLoss: 2.093126\n",
            "{SqueezeNet} Train Epoch: 4 [12288/37814 (32%)]\tLoss: 2.107866\n",
            "{SqueezeNet} Train Epoch: 4 [12800/37814 (34%)]\tLoss: 2.176163\n",
            "{SqueezeNet} Train Epoch: 4 [13312/37814 (35%)]\tLoss: 2.085567\n",
            "{SqueezeNet} Train Epoch: 4 [13824/37814 (36%)]\tLoss: 2.101194\n",
            "{SqueezeNet} Train Epoch: 4 [14336/37814 (38%)]\tLoss: 2.127807\n",
            "{SqueezeNet} Train Epoch: 4 [14848/37814 (39%)]\tLoss: 2.080849\n",
            "{SqueezeNet} Train Epoch: 4 [15360/37814 (41%)]\tLoss: 2.084340\n",
            "{SqueezeNet} Train Epoch: 4 [15872/37814 (42%)]\tLoss: 2.046063\n",
            "{SqueezeNet} Train Epoch: 4 [16384/37814 (43%)]\tLoss: 2.062295\n",
            "{SqueezeNet} Train Epoch: 4 [16896/37814 (45%)]\tLoss: 2.090280\n",
            "{SqueezeNet} Train Epoch: 4 [17408/37814 (46%)]\tLoss: 2.116275\n",
            "{SqueezeNet} Train Epoch: 4 [17920/37814 (47%)]\tLoss: 2.021959\n",
            "{SqueezeNet} Train Epoch: 4 [18432/37814 (49%)]\tLoss: 2.064972\n",
            "{SqueezeNet} Train Epoch: 4 [18944/37814 (50%)]\tLoss: 2.036014\n",
            "{SqueezeNet} Train Epoch: 4 [19456/37814 (51%)]\tLoss: 2.054425\n",
            "{SqueezeNet} Train Epoch: 4 [19968/37814 (53%)]\tLoss: 2.070831\n",
            "{SqueezeNet} Train Epoch: 4 [20480/37814 (54%)]\tLoss: 2.086992\n",
            "{SqueezeNet} Train Epoch: 4 [20992/37814 (55%)]\tLoss: 2.144858\n",
            "{SqueezeNet} Train Epoch: 4 [21504/37814 (57%)]\tLoss: 1.991718\n",
            "{SqueezeNet} Train Epoch: 4 [22016/37814 (58%)]\tLoss: 2.132035\n",
            "{SqueezeNet} Train Epoch: 4 [22528/37814 (59%)]\tLoss: 2.021116\n",
            "{SqueezeNet} Train Epoch: 4 [23040/37814 (61%)]\tLoss: 2.031075\n",
            "{SqueezeNet} Train Epoch: 4 [23552/37814 (62%)]\tLoss: 2.068112\n",
            "{SqueezeNet} Train Epoch: 4 [24064/37814 (64%)]\tLoss: 2.062572\n",
            "{SqueezeNet} Train Epoch: 4 [24576/37814 (65%)]\tLoss: 2.112888\n",
            "{SqueezeNet} Train Epoch: 4 [25088/37814 (66%)]\tLoss: 2.008490\n",
            "{SqueezeNet} Train Epoch: 4 [25600/37814 (68%)]\tLoss: 2.054217\n",
            "{SqueezeNet} Train Epoch: 4 [26112/37814 (69%)]\tLoss: 2.055200\n",
            "{SqueezeNet} Train Epoch: 4 [26624/37814 (70%)]\tLoss: 2.045914\n",
            "{SqueezeNet} Train Epoch: 4 [27136/37814 (72%)]\tLoss: 2.030628\n",
            "{SqueezeNet} Train Epoch: 4 [27648/37814 (73%)]\tLoss: 2.021162\n",
            "{SqueezeNet} Train Epoch: 4 [28160/37814 (74%)]\tLoss: 2.078131\n",
            "{SqueezeNet} Train Epoch: 4 [28672/37814 (76%)]\tLoss: 2.085040\n",
            "{SqueezeNet} Train Epoch: 4 [29184/37814 (77%)]\tLoss: 2.093205\n",
            "{SqueezeNet} Train Epoch: 4 [29696/37814 (78%)]\tLoss: 2.020256\n",
            "{SqueezeNet} Train Epoch: 4 [30208/37814 (80%)]\tLoss: 2.033255\n",
            "{SqueezeNet} Train Epoch: 4 [30720/37814 (81%)]\tLoss: 2.094915\n",
            "{SqueezeNet} Train Epoch: 4 [31232/37814 (82%)]\tLoss: 2.058750\n",
            "{SqueezeNet} Train Epoch: 4 [31744/37814 (84%)]\tLoss: 2.043606\n",
            "{SqueezeNet} Train Epoch: 4 [32256/37814 (85%)]\tLoss: 2.163549\n",
            "{SqueezeNet} Train Epoch: 4 [32768/37814 (86%)]\tLoss: 2.129342\n",
            "{SqueezeNet} Train Epoch: 4 [33280/37814 (88%)]\tLoss: 2.048489\n",
            "{SqueezeNet} Train Epoch: 4 [33792/37814 (89%)]\tLoss: 2.097350\n",
            "{SqueezeNet} Train Epoch: 4 [34304/37814 (91%)]\tLoss: 2.021400\n",
            "{SqueezeNet} Train Epoch: 4 [34816/37814 (92%)]\tLoss: 2.078424\n",
            "{SqueezeNet} Train Epoch: 4 [35328/37814 (93%)]\tLoss: 2.003518\n",
            "{SqueezeNet} Train Epoch: 4 [35840/37814 (95%)]\tLoss: 2.025707\n",
            "{SqueezeNet} Train Epoch: 4 [36352/37814 (96%)]\tLoss: 2.038139\n",
            "{SqueezeNet} Train Epoch: 4 [36864/37814 (97%)]\tLoss: 2.010205\n",
            "{SqueezeNet} Train Epoch: 4 [31974/37814 (99%)]\tLoss: 2.019256\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.0143, Accuracy: 1181/5000 (24%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.053220748901367 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e3ad0bc-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ece80452-60e2-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8c772d3bea"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e3cdd94-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_37ff409296"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e3d360e-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_791f9ca337"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e3d824e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0e3d360e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_adaef0c434"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e68edee-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0e3cdd94-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e31fc7afce"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e6abcf0-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_499c6ba7dd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e6b061a-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_cc64ee1813"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0e6b499a-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0e6b061a-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6ebe9dac1a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 5 [0/37814 (0%)]\tLoss: 2.304966\n",
            "{AlexNet} Train Epoch: 5 [512/37814 (1%)]\tLoss: 2.315364\n",
            "{AlexNet} Train Epoch: 5 [1024/37814 (3%)]\tLoss: 2.304969\n",
            "{AlexNet} Train Epoch: 5 [1536/37814 (4%)]\tLoss: 2.309764\n",
            "{AlexNet} Train Epoch: 5 [2048/37814 (5%)]\tLoss: 2.312544\n",
            "{AlexNet} Train Epoch: 5 [2560/37814 (7%)]\tLoss: 2.311986\n",
            "{AlexNet} Train Epoch: 5 [3072/37814 (8%)]\tLoss: 2.306366\n",
            "{AlexNet} Train Epoch: 5 [3584/37814 (9%)]\tLoss: 2.308116\n",
            "{AlexNet} Train Epoch: 5 [4096/37814 (11%)]\tLoss: 2.318679\n",
            "{AlexNet} Train Epoch: 5 [4608/37814 (12%)]\tLoss: 2.306944\n",
            "{AlexNet} Train Epoch: 5 [5120/37814 (14%)]\tLoss: 2.317146\n",
            "{AlexNet} Train Epoch: 5 [5632/37814 (15%)]\tLoss: 2.305427\n",
            "{AlexNet} Train Epoch: 5 [6144/37814 (16%)]\tLoss: 2.303440\n",
            "{AlexNet} Train Epoch: 5 [6656/37814 (18%)]\tLoss: 2.315146\n",
            "{AlexNet} Train Epoch: 5 [7168/37814 (19%)]\tLoss: 2.318695\n",
            "{AlexNet} Train Epoch: 5 [7680/37814 (20%)]\tLoss: 2.320071\n",
            "{AlexNet} Train Epoch: 5 [8192/37814 (22%)]\tLoss: 2.313326\n",
            "{AlexNet} Train Epoch: 5 [8704/37814 (23%)]\tLoss: 2.324230\n",
            "{AlexNet} Train Epoch: 5 [9216/37814 (24%)]\tLoss: 2.320618\n",
            "{AlexNet} Train Epoch: 5 [9728/37814 (26%)]\tLoss: 2.313473\n",
            "{AlexNet} Train Epoch: 5 [10240/37814 (27%)]\tLoss: 2.300739\n",
            "{AlexNet} Train Epoch: 5 [10752/37814 (28%)]\tLoss: 2.306522\n",
            "{AlexNet} Train Epoch: 5 [11264/37814 (30%)]\tLoss: 2.306014\n",
            "{AlexNet} Train Epoch: 5 [11776/37814 (31%)]\tLoss: 2.307505\n",
            "{AlexNet} Train Epoch: 5 [12288/37814 (32%)]\tLoss: 2.325832\n",
            "{AlexNet} Train Epoch: 5 [12800/37814 (34%)]\tLoss: 2.311450\n",
            "{AlexNet} Train Epoch: 5 [13312/37814 (35%)]\tLoss: 2.316573\n",
            "{AlexNet} Train Epoch: 5 [13824/37814 (36%)]\tLoss: 2.315549\n",
            "{AlexNet} Train Epoch: 5 [14336/37814 (38%)]\tLoss: 2.305743\n",
            "{AlexNet} Train Epoch: 5 [14848/37814 (39%)]\tLoss: 2.305671\n",
            "{AlexNet} Train Epoch: 5 [15360/37814 (41%)]\tLoss: 2.308031\n",
            "{AlexNet} Train Epoch: 5 [15872/37814 (42%)]\tLoss: 2.309488\n",
            "{AlexNet} Train Epoch: 5 [16384/37814 (43%)]\tLoss: 2.311338\n",
            "{AlexNet} Train Epoch: 5 [16896/37814 (45%)]\tLoss: 2.322154\n",
            "{AlexNet} Train Epoch: 5 [17408/37814 (46%)]\tLoss: 2.312837\n",
            "{AlexNet} Train Epoch: 5 [17920/37814 (47%)]\tLoss: 2.310438\n",
            "{AlexNet} Train Epoch: 5 [18432/37814 (49%)]\tLoss: 2.312355\n",
            "{AlexNet} Train Epoch: 5 [18944/37814 (50%)]\tLoss: 2.310902\n",
            "{AlexNet} Train Epoch: 5 [19456/37814 (51%)]\tLoss: 2.307269\n",
            "{AlexNet} Train Epoch: 5 [19968/37814 (53%)]\tLoss: 2.313874\n",
            "{AlexNet} Train Epoch: 5 [20480/37814 (54%)]\tLoss: 2.297256\n",
            "{AlexNet} Train Epoch: 5 [20992/37814 (55%)]\tLoss: 2.314263\n",
            "{AlexNet} Train Epoch: 5 [21504/37814 (57%)]\tLoss: 2.300712\n",
            "{AlexNet} Train Epoch: 5 [22016/37814 (58%)]\tLoss: 2.305065\n",
            "{AlexNet} Train Epoch: 5 [22528/37814 (59%)]\tLoss: 2.313922\n",
            "{AlexNet} Train Epoch: 5 [23040/37814 (61%)]\tLoss: 2.306605\n",
            "{AlexNet} Train Epoch: 5 [23552/37814 (62%)]\tLoss: 2.311879\n",
            "{AlexNet} Train Epoch: 5 [24064/37814 (64%)]\tLoss: 2.305845\n",
            "{AlexNet} Train Epoch: 5 [24576/37814 (65%)]\tLoss: 2.320002\n",
            "{AlexNet} Train Epoch: 5 [25088/37814 (66%)]\tLoss: 2.309661\n",
            "{AlexNet} Train Epoch: 5 [25600/37814 (68%)]\tLoss: 2.314211\n",
            "{AlexNet} Train Epoch: 5 [26112/37814 (69%)]\tLoss: 2.310926\n",
            "{AlexNet} Train Epoch: 5 [26624/37814 (70%)]\tLoss: 2.312838\n",
            "{AlexNet} Train Epoch: 5 [27136/37814 (72%)]\tLoss: 2.304977\n",
            "{AlexNet} Train Epoch: 5 [27648/37814 (73%)]\tLoss: 2.304204\n",
            "{AlexNet} Train Epoch: 5 [28160/37814 (74%)]\tLoss: 2.313627\n",
            "{AlexNet} Train Epoch: 5 [28672/37814 (76%)]\tLoss: 2.306342\n",
            "{AlexNet} Train Epoch: 5 [29184/37814 (77%)]\tLoss: 2.308798\n",
            "{AlexNet} Train Epoch: 5 [29696/37814 (78%)]\tLoss: 2.314314\n",
            "{AlexNet} Train Epoch: 5 [30208/37814 (80%)]\tLoss: 2.309867\n",
            "{AlexNet} Train Epoch: 5 [30720/37814 (81%)]\tLoss: 2.306505\n",
            "{AlexNet} Train Epoch: 5 [31232/37814 (82%)]\tLoss: 2.303716\n",
            "{AlexNet} Train Epoch: 5 [31744/37814 (84%)]\tLoss: 2.305475\n",
            "{AlexNet} Train Epoch: 5 [32256/37814 (85%)]\tLoss: 2.307571\n",
            "{AlexNet} Train Epoch: 5 [32768/37814 (86%)]\tLoss: 2.301347\n",
            "{AlexNet} Train Epoch: 5 [33280/37814 (88%)]\tLoss: 2.307014\n",
            "{AlexNet} Train Epoch: 5 [33792/37814 (89%)]\tLoss: 2.305608\n",
            "{AlexNet} Train Epoch: 5 [34304/37814 (91%)]\tLoss: 2.311347\n",
            "{AlexNet} Train Epoch: 5 [34816/37814 (92%)]\tLoss: 2.311076\n",
            "{AlexNet} Train Epoch: 5 [35328/37814 (93%)]\tLoss: 2.304229\n",
            "{AlexNet} Train Epoch: 5 [35840/37814 (95%)]\tLoss: 2.310758\n",
            "{AlexNet} Train Epoch: 5 [36352/37814 (96%)]\tLoss: 2.319079\n",
            "{AlexNet} Train Epoch: 5 [36864/37814 (97%)]\tLoss: 2.313876\n",
            "{AlexNet} Train Epoch: 5 [31974/37814 (99%)]\tLoss: 2.307468\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.3068, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.08340811729431 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 5 [0/37814 (0%)]\tLoss: 1.978395\n",
            "{SqueezeNet} Train Epoch: 5 [512/37814 (1%)]\tLoss: 2.052923\n",
            "{SqueezeNet} Train Epoch: 5 [1024/37814 (3%)]\tLoss: 2.028655\n",
            "{SqueezeNet} Train Epoch: 5 [1536/37814 (4%)]\tLoss: 2.067769\n",
            "{SqueezeNet} Train Epoch: 5 [2048/37814 (5%)]\tLoss: 1.944342\n",
            "{SqueezeNet} Train Epoch: 5 [2560/37814 (7%)]\tLoss: 1.996906\n",
            "{SqueezeNet} Train Epoch: 5 [3072/37814 (8%)]\tLoss: 1.966865\n",
            "{SqueezeNet} Train Epoch: 5 [3584/37814 (9%)]\tLoss: 1.955002\n",
            "{SqueezeNet} Train Epoch: 5 [4096/37814 (11%)]\tLoss: 1.985804\n",
            "{SqueezeNet} Train Epoch: 5 [4608/37814 (12%)]\tLoss: 2.022269\n",
            "{SqueezeNet} Train Epoch: 5 [5120/37814 (14%)]\tLoss: 2.050585\n",
            "{SqueezeNet} Train Epoch: 5 [5632/37814 (15%)]\tLoss: 2.002411\n",
            "{SqueezeNet} Train Epoch: 5 [6144/37814 (16%)]\tLoss: 2.013269\n",
            "{SqueezeNet} Train Epoch: 5 [6656/37814 (18%)]\tLoss: 2.078835\n",
            "{SqueezeNet} Train Epoch: 5 [7168/37814 (19%)]\tLoss: 1.996770\n",
            "{SqueezeNet} Train Epoch: 5 [7680/37814 (20%)]\tLoss: 2.014719\n",
            "{SqueezeNet} Train Epoch: 5 [8192/37814 (22%)]\tLoss: 2.005869\n",
            "{SqueezeNet} Train Epoch: 5 [8704/37814 (23%)]\tLoss: 1.916903\n",
            "{SqueezeNet} Train Epoch: 5 [9216/37814 (24%)]\tLoss: 2.001033\n",
            "{SqueezeNet} Train Epoch: 5 [9728/37814 (26%)]\tLoss: 1.963721\n",
            "{SqueezeNet} Train Epoch: 5 [10240/37814 (27%)]\tLoss: 1.976271\n",
            "{SqueezeNet} Train Epoch: 5 [10752/37814 (28%)]\tLoss: 2.079029\n",
            "{SqueezeNet} Train Epoch: 5 [11264/37814 (30%)]\tLoss: 2.205761\n",
            "{SqueezeNet} Train Epoch: 5 [11776/37814 (31%)]\tLoss: 2.075727\n",
            "{SqueezeNet} Train Epoch: 5 [12288/37814 (32%)]\tLoss: 2.001977\n",
            "{SqueezeNet} Train Epoch: 5 [12800/37814 (34%)]\tLoss: 2.065667\n",
            "{SqueezeNet} Train Epoch: 5 [13312/37814 (35%)]\tLoss: 2.048481\n",
            "{SqueezeNet} Train Epoch: 5 [13824/37814 (36%)]\tLoss: 1.984327\n",
            "{SqueezeNet} Train Epoch: 5 [14336/37814 (38%)]\tLoss: 2.097397\n",
            "{SqueezeNet} Train Epoch: 5 [14848/37814 (39%)]\tLoss: 2.057016\n",
            "{SqueezeNet} Train Epoch: 5 [15360/37814 (41%)]\tLoss: 2.052686\n",
            "{SqueezeNet} Train Epoch: 5 [15872/37814 (42%)]\tLoss: 2.040496\n",
            "{SqueezeNet} Train Epoch: 5 [16384/37814 (43%)]\tLoss: 2.064634\n",
            "{SqueezeNet} Train Epoch: 5 [16896/37814 (45%)]\tLoss: 2.010574\n",
            "{SqueezeNet} Train Epoch: 5 [17408/37814 (46%)]\tLoss: 2.062433\n",
            "{SqueezeNet} Train Epoch: 5 [17920/37814 (47%)]\tLoss: 2.059616\n",
            "{SqueezeNet} Train Epoch: 5 [18432/37814 (49%)]\tLoss: 2.078557\n",
            "{SqueezeNet} Train Epoch: 5 [18944/37814 (50%)]\tLoss: 2.016253\n",
            "{SqueezeNet} Train Epoch: 5 [19456/37814 (51%)]\tLoss: 2.010951\n",
            "{SqueezeNet} Train Epoch: 5 [19968/37814 (53%)]\tLoss: 1.972391\n",
            "{SqueezeNet} Train Epoch: 5 [20480/37814 (54%)]\tLoss: 2.024379\n",
            "{SqueezeNet} Train Epoch: 5 [20992/37814 (55%)]\tLoss: 2.040272\n",
            "{SqueezeNet} Train Epoch: 5 [21504/37814 (57%)]\tLoss: 2.006865\n",
            "{SqueezeNet} Train Epoch: 5 [22016/37814 (58%)]\tLoss: 2.043596\n",
            "{SqueezeNet} Train Epoch: 5 [22528/37814 (59%)]\tLoss: 2.010329\n",
            "{SqueezeNet} Train Epoch: 5 [23040/37814 (61%)]\tLoss: 1.991190\n",
            "{SqueezeNet} Train Epoch: 5 [23552/37814 (62%)]\tLoss: 2.038416\n",
            "{SqueezeNet} Train Epoch: 5 [24064/37814 (64%)]\tLoss: 1.989793\n",
            "{SqueezeNet} Train Epoch: 5 [24576/37814 (65%)]\tLoss: 1.955727\n",
            "{SqueezeNet} Train Epoch: 5 [25088/37814 (66%)]\tLoss: 2.024466\n",
            "{SqueezeNet} Train Epoch: 5 [25600/37814 (68%)]\tLoss: 1.985463\n",
            "{SqueezeNet} Train Epoch: 5 [26112/37814 (69%)]\tLoss: 1.998917\n",
            "{SqueezeNet} Train Epoch: 5 [26624/37814 (70%)]\tLoss: 2.149428\n",
            "{SqueezeNet} Train Epoch: 5 [27136/37814 (72%)]\tLoss: 2.030954\n",
            "{SqueezeNet} Train Epoch: 5 [27648/37814 (73%)]\tLoss: 1.969882\n",
            "{SqueezeNet} Train Epoch: 5 [28160/37814 (74%)]\tLoss: 2.128278\n",
            "{SqueezeNet} Train Epoch: 5 [28672/37814 (76%)]\tLoss: 1.989319\n",
            "{SqueezeNet} Train Epoch: 5 [29184/37814 (77%)]\tLoss: 2.085158\n",
            "{SqueezeNet} Train Epoch: 5 [29696/37814 (78%)]\tLoss: 1.996986\n",
            "{SqueezeNet} Train Epoch: 5 [30208/37814 (80%)]\tLoss: 2.057202\n",
            "{SqueezeNet} Train Epoch: 5 [30720/37814 (81%)]\tLoss: 2.053792\n",
            "{SqueezeNet} Train Epoch: 5 [31232/37814 (82%)]\tLoss: 1.999708\n",
            "{SqueezeNet} Train Epoch: 5 [31744/37814 (84%)]\tLoss: 2.063519\n",
            "{SqueezeNet} Train Epoch: 5 [32256/37814 (85%)]\tLoss: 2.007673\n",
            "{SqueezeNet} Train Epoch: 5 [32768/37814 (86%)]\tLoss: 2.073373\n",
            "{SqueezeNet} Train Epoch: 5 [33280/37814 (88%)]\tLoss: 1.977864\n",
            "{SqueezeNet} Train Epoch: 5 [33792/37814 (89%)]\tLoss: 2.147645\n",
            "{SqueezeNet} Train Epoch: 5 [34304/37814 (91%)]\tLoss: 2.041894\n",
            "{SqueezeNet} Train Epoch: 5 [34816/37814 (92%)]\tLoss: 2.147723\n",
            "{SqueezeNet} Train Epoch: 5 [35328/37814 (93%)]\tLoss: 2.063150\n",
            "{SqueezeNet} Train Epoch: 5 [35840/37814 (95%)]\tLoss: 2.097451\n",
            "{SqueezeNet} Train Epoch: 5 [36352/37814 (96%)]\tLoss: 2.097013\n",
            "{SqueezeNet} Train Epoch: 5 [36864/37814 (97%)]\tLoss: 2.018944\n",
            "{SqueezeNet} Train Epoch: 5 [31974/37814 (99%)]\tLoss: 2.068219\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.0414, Accuracy: 1223/5000 (24%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.413410186767578 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f80f986-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0e6abcf0-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_bafda9ca44"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f829282-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_a16a52927d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f82d38c-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_34e7c796e2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f8313ba-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f82d38c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_af90142232"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2fa65c44-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f829282-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_017b7fc4b2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2fa7e118-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_32f54ebfc0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2fa82ce0-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_8501270ebf"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2fa85cec-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2fa82ce0-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_52b18a0c0f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 6 [0/37814 (0%)]\tLoss: 2.312593\n",
            "{AlexNet} Train Epoch: 6 [512/37814 (1%)]\tLoss: 2.307788\n",
            "{AlexNet} Train Epoch: 6 [1024/37814 (3%)]\tLoss: 2.319441\n",
            "{AlexNet} Train Epoch: 6 [1536/37814 (4%)]\tLoss: 2.318066\n",
            "{AlexNet} Train Epoch: 6 [2048/37814 (5%)]\tLoss: 2.306141\n",
            "{AlexNet} Train Epoch: 6 [2560/37814 (7%)]\tLoss: 2.304464\n",
            "{AlexNet} Train Epoch: 6 [3072/37814 (8%)]\tLoss: 2.298278\n",
            "{AlexNet} Train Epoch: 6 [3584/37814 (9%)]\tLoss: 2.315637\n",
            "{AlexNet} Train Epoch: 6 [4096/37814 (11%)]\tLoss: 2.314763\n",
            "{AlexNet} Train Epoch: 6 [4608/37814 (12%)]\tLoss: 2.308955\n",
            "{AlexNet} Train Epoch: 6 [5120/37814 (14%)]\tLoss: 2.310207\n",
            "{AlexNet} Train Epoch: 6 [5632/37814 (15%)]\tLoss: 2.303299\n",
            "{AlexNet} Train Epoch: 6 [6144/37814 (16%)]\tLoss: 2.310916\n",
            "{AlexNet} Train Epoch: 6 [6656/37814 (18%)]\tLoss: 2.302151\n",
            "{AlexNet} Train Epoch: 6 [7168/37814 (19%)]\tLoss: 2.307293\n",
            "{AlexNet} Train Epoch: 6 [7680/37814 (20%)]\tLoss: 2.309842\n",
            "{AlexNet} Train Epoch: 6 [8192/37814 (22%)]\tLoss: 2.305710\n",
            "{AlexNet} Train Epoch: 6 [8704/37814 (23%)]\tLoss: 2.307572\n",
            "{AlexNet} Train Epoch: 6 [9216/37814 (24%)]\tLoss: 2.309081\n",
            "{AlexNet} Train Epoch: 6 [9728/37814 (26%)]\tLoss: 2.306381\n",
            "{AlexNet} Train Epoch: 6 [10240/37814 (27%)]\tLoss: 2.304538\n",
            "{AlexNet} Train Epoch: 6 [10752/37814 (28%)]\tLoss: 2.300011\n",
            "{AlexNet} Train Epoch: 6 [11264/37814 (30%)]\tLoss: 2.306659\n",
            "{AlexNet} Train Epoch: 6 [11776/37814 (31%)]\tLoss: 2.313231\n",
            "{AlexNet} Train Epoch: 6 [12288/37814 (32%)]\tLoss: 2.309721\n",
            "{AlexNet} Train Epoch: 6 [12800/37814 (34%)]\tLoss: 2.304883\n",
            "{AlexNet} Train Epoch: 6 [13312/37814 (35%)]\tLoss: 2.311693\n",
            "{AlexNet} Train Epoch: 6 [13824/37814 (36%)]\tLoss: 2.309331\n",
            "{AlexNet} Train Epoch: 6 [14336/37814 (38%)]\tLoss: 2.308541\n",
            "{AlexNet} Train Epoch: 6 [14848/37814 (39%)]\tLoss: 2.307656\n",
            "{AlexNet} Train Epoch: 6 [15360/37814 (41%)]\tLoss: 2.300437\n",
            "{AlexNet} Train Epoch: 6 [15872/37814 (42%)]\tLoss: 2.306690\n",
            "{AlexNet} Train Epoch: 6 [16384/37814 (43%)]\tLoss: 2.306503\n",
            "{AlexNet} Train Epoch: 6 [16896/37814 (45%)]\tLoss: 2.313233\n",
            "{AlexNet} Train Epoch: 6 [17408/37814 (46%)]\tLoss: 2.307811\n",
            "{AlexNet} Train Epoch: 6 [17920/37814 (47%)]\tLoss: 2.301123\n",
            "{AlexNet} Train Epoch: 6 [18432/37814 (49%)]\tLoss: 2.304892\n",
            "{AlexNet} Train Epoch: 6 [18944/37814 (50%)]\tLoss: 2.303137\n",
            "{AlexNet} Train Epoch: 6 [19456/37814 (51%)]\tLoss: 2.306279\n",
            "{AlexNet} Train Epoch: 6 [19968/37814 (53%)]\tLoss: 2.321374\n",
            "{AlexNet} Train Epoch: 6 [20480/37814 (54%)]\tLoss: 2.307703\n",
            "{AlexNet} Train Epoch: 6 [20992/37814 (55%)]\tLoss: 2.309003\n",
            "{AlexNet} Train Epoch: 6 [21504/37814 (57%)]\tLoss: 2.290607\n",
            "{AlexNet} Train Epoch: 6 [22016/37814 (58%)]\tLoss: 2.301812\n",
            "{AlexNet} Train Epoch: 6 [22528/37814 (59%)]\tLoss: 2.303390\n",
            "{AlexNet} Train Epoch: 6 [23040/37814 (61%)]\tLoss: 2.298779\n",
            "{AlexNet} Train Epoch: 6 [23552/37814 (62%)]\tLoss: 2.316375\n",
            "{AlexNet} Train Epoch: 6 [24064/37814 (64%)]\tLoss: 2.300391\n",
            "{AlexNet} Train Epoch: 6 [24576/37814 (65%)]\tLoss: 2.295110\n",
            "{AlexNet} Train Epoch: 6 [25088/37814 (66%)]\tLoss: 2.305496\n",
            "{AlexNet} Train Epoch: 6 [25600/37814 (68%)]\tLoss: 2.302538\n",
            "{AlexNet} Train Epoch: 6 [26112/37814 (69%)]\tLoss: 2.301197\n",
            "{AlexNet} Train Epoch: 6 [26624/37814 (70%)]\tLoss: 2.312538\n",
            "{AlexNet} Train Epoch: 6 [27136/37814 (72%)]\tLoss: 2.312430\n",
            "{AlexNet} Train Epoch: 6 [27648/37814 (73%)]\tLoss: 2.310386\n",
            "{AlexNet} Train Epoch: 6 [28160/37814 (74%)]\tLoss: 2.299280\n",
            "{AlexNet} Train Epoch: 6 [28672/37814 (76%)]\tLoss: 2.304509\n",
            "{AlexNet} Train Epoch: 6 [29184/37814 (77%)]\tLoss: 2.300025\n",
            "{AlexNet} Train Epoch: 6 [29696/37814 (78%)]\tLoss: 2.317075\n",
            "{AlexNet} Train Epoch: 6 [30208/37814 (80%)]\tLoss: 2.305833\n",
            "{AlexNet} Train Epoch: 6 [30720/37814 (81%)]\tLoss: 2.298931\n",
            "{AlexNet} Train Epoch: 6 [31232/37814 (82%)]\tLoss: 2.314905\n",
            "{AlexNet} Train Epoch: 6 [31744/37814 (84%)]\tLoss: 2.304976\n",
            "{AlexNet} Train Epoch: 6 [32256/37814 (85%)]\tLoss: 2.301290\n",
            "{AlexNet} Train Epoch: 6 [32768/37814 (86%)]\tLoss: 2.303670\n",
            "{AlexNet} Train Epoch: 6 [33280/37814 (88%)]\tLoss: 2.310311\n",
            "{AlexNet} Train Epoch: 6 [33792/37814 (89%)]\tLoss: 2.298193\n",
            "{AlexNet} Train Epoch: 6 [34304/37814 (91%)]\tLoss: 2.307492\n",
            "{AlexNet} Train Epoch: 6 [34816/37814 (92%)]\tLoss: 2.304214\n",
            "{AlexNet} Train Epoch: 6 [35328/37814 (93%)]\tLoss: 2.311018\n",
            "{AlexNet} Train Epoch: 6 [35840/37814 (95%)]\tLoss: 2.302341\n",
            "{AlexNet} Train Epoch: 6 [36352/37814 (96%)]\tLoss: 2.307995\n",
            "{AlexNet} Train Epoch: 6 [36864/37814 (97%)]\tLoss: 2.298350\n",
            "{AlexNet} Train Epoch: 6 [31974/37814 (99%)]\tLoss: 2.302835\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.3045, Accuracy: 500/5000 (10%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.74313497543335 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 6 [0/37814 (0%)]\tLoss: 2.000591\n",
            "{SqueezeNet} Train Epoch: 6 [512/37814 (1%)]\tLoss: 2.056444\n",
            "{SqueezeNet} Train Epoch: 6 [1024/37814 (3%)]\tLoss: 2.044245\n",
            "{SqueezeNet} Train Epoch: 6 [1536/37814 (4%)]\tLoss: 2.000456\n",
            "{SqueezeNet} Train Epoch: 6 [2048/37814 (5%)]\tLoss: 2.018048\n",
            "{SqueezeNet} Train Epoch: 6 [2560/37814 (7%)]\tLoss: 2.019457\n",
            "{SqueezeNet} Train Epoch: 6 [3072/37814 (8%)]\tLoss: 1.990856\n",
            "{SqueezeNet} Train Epoch: 6 [3584/37814 (9%)]\tLoss: 2.065383\n",
            "{SqueezeNet} Train Epoch: 6 [4096/37814 (11%)]\tLoss: 1.992841\n",
            "{SqueezeNet} Train Epoch: 6 [4608/37814 (12%)]\tLoss: 2.059931\n",
            "{SqueezeNet} Train Epoch: 6 [5120/37814 (14%)]\tLoss: 2.057311\n",
            "{SqueezeNet} Train Epoch: 6 [5632/37814 (15%)]\tLoss: 2.055441\n",
            "{SqueezeNet} Train Epoch: 6 [6144/37814 (16%)]\tLoss: 2.019486\n",
            "{SqueezeNet} Train Epoch: 6 [6656/37814 (18%)]\tLoss: 2.022012\n",
            "{SqueezeNet} Train Epoch: 6 [7168/37814 (19%)]\tLoss: 1.976349\n",
            "{SqueezeNet} Train Epoch: 6 [7680/37814 (20%)]\tLoss: 1.970875\n",
            "{SqueezeNet} Train Epoch: 6 [8192/37814 (22%)]\tLoss: 2.068455\n",
            "{SqueezeNet} Train Epoch: 6 [8704/37814 (23%)]\tLoss: 1.982489\n",
            "{SqueezeNet} Train Epoch: 6 [9216/37814 (24%)]\tLoss: 2.044524\n",
            "{SqueezeNet} Train Epoch: 6 [9728/37814 (26%)]\tLoss: 2.006833\n",
            "{SqueezeNet} Train Epoch: 6 [10240/37814 (27%)]\tLoss: 2.040325\n",
            "{SqueezeNet} Train Epoch: 6 [10752/37814 (28%)]\tLoss: 2.046924\n",
            "{SqueezeNet} Train Epoch: 6 [11264/37814 (30%)]\tLoss: 2.034479\n",
            "{SqueezeNet} Train Epoch: 6 [11776/37814 (31%)]\tLoss: 1.955522\n",
            "{SqueezeNet} Train Epoch: 6 [12288/37814 (32%)]\tLoss: 1.941479\n",
            "{SqueezeNet} Train Epoch: 6 [12800/37814 (34%)]\tLoss: 1.986434\n",
            "{SqueezeNet} Train Epoch: 6 [13312/37814 (35%)]\tLoss: 1.958536\n",
            "{SqueezeNet} Train Epoch: 6 [13824/37814 (36%)]\tLoss: 2.021962\n",
            "{SqueezeNet} Train Epoch: 6 [14336/37814 (38%)]\tLoss: 2.003603\n",
            "{SqueezeNet} Train Epoch: 6 [14848/37814 (39%)]\tLoss: 2.039145\n",
            "{SqueezeNet} Train Epoch: 6 [15360/37814 (41%)]\tLoss: 1.952630\n",
            "{SqueezeNet} Train Epoch: 6 [15872/37814 (42%)]\tLoss: 2.069445\n",
            "{SqueezeNet} Train Epoch: 6 [16384/37814 (43%)]\tLoss: 1.971551\n",
            "{SqueezeNet} Train Epoch: 6 [16896/37814 (45%)]\tLoss: 1.992648\n",
            "{SqueezeNet} Train Epoch: 6 [17408/37814 (46%)]\tLoss: 1.951242\n",
            "{SqueezeNet} Train Epoch: 6 [17920/37814 (47%)]\tLoss: 2.009017\n",
            "{SqueezeNet} Train Epoch: 6 [18432/37814 (49%)]\tLoss: 1.975630\n",
            "{SqueezeNet} Train Epoch: 6 [18944/37814 (50%)]\tLoss: 1.937076\n",
            "{SqueezeNet} Train Epoch: 6 [19456/37814 (51%)]\tLoss: 1.979076\n",
            "{SqueezeNet} Train Epoch: 6 [19968/37814 (53%)]\tLoss: 2.054868\n",
            "{SqueezeNet} Train Epoch: 6 [20480/37814 (54%)]\tLoss: 1.960217\n",
            "{SqueezeNet} Train Epoch: 6 [20992/37814 (55%)]\tLoss: 1.987057\n",
            "{SqueezeNet} Train Epoch: 6 [21504/37814 (57%)]\tLoss: 1.956077\n",
            "{SqueezeNet} Train Epoch: 6 [22016/37814 (58%)]\tLoss: 2.113510\n",
            "{SqueezeNet} Train Epoch: 6 [22528/37814 (59%)]\tLoss: 1.990749\n",
            "{SqueezeNet} Train Epoch: 6 [23040/37814 (61%)]\tLoss: 1.995089\n",
            "{SqueezeNet} Train Epoch: 6 [23552/37814 (62%)]\tLoss: 1.982952\n",
            "{SqueezeNet} Train Epoch: 6 [24064/37814 (64%)]\tLoss: 2.023487\n",
            "{SqueezeNet} Train Epoch: 6 [24576/37814 (65%)]\tLoss: 1.967270\n",
            "{SqueezeNet} Train Epoch: 6 [25088/37814 (66%)]\tLoss: 1.972598\n",
            "{SqueezeNet} Train Epoch: 6 [25600/37814 (68%)]\tLoss: 1.938850\n",
            "{SqueezeNet} Train Epoch: 6 [26112/37814 (69%)]\tLoss: 1.956331\n",
            "{SqueezeNet} Train Epoch: 6 [26624/37814 (70%)]\tLoss: 1.998437\n",
            "{SqueezeNet} Train Epoch: 6 [27136/37814 (72%)]\tLoss: 1.946673\n",
            "{SqueezeNet} Train Epoch: 6 [27648/37814 (73%)]\tLoss: 1.966555\n",
            "{SqueezeNet} Train Epoch: 6 [28160/37814 (74%)]\tLoss: 1.991637\n",
            "{SqueezeNet} Train Epoch: 6 [28672/37814 (76%)]\tLoss: 1.966189\n",
            "{SqueezeNet} Train Epoch: 6 [29184/37814 (77%)]\tLoss: 1.998126\n",
            "{SqueezeNet} Train Epoch: 6 [29696/37814 (78%)]\tLoss: 1.909750\n",
            "{SqueezeNet} Train Epoch: 6 [30208/37814 (80%)]\tLoss: 1.937625\n",
            "{SqueezeNet} Train Epoch: 6 [30720/37814 (81%)]\tLoss: 2.002822\n",
            "{SqueezeNet} Train Epoch: 6 [31232/37814 (82%)]\tLoss: 1.944615\n",
            "{SqueezeNet} Train Epoch: 6 [31744/37814 (84%)]\tLoss: 1.952266\n",
            "{SqueezeNet} Train Epoch: 6 [32256/37814 (85%)]\tLoss: 2.032724\n",
            "{SqueezeNet} Train Epoch: 6 [32768/37814 (86%)]\tLoss: 1.971919\n",
            "{SqueezeNet} Train Epoch: 6 [33280/37814 (88%)]\tLoss: 1.961414\n",
            "{SqueezeNet} Train Epoch: 6 [33792/37814 (89%)]\tLoss: 1.984898\n",
            "{SqueezeNet} Train Epoch: 6 [34304/37814 (91%)]\tLoss: 1.911391\n",
            "{SqueezeNet} Train Epoch: 6 [34816/37814 (92%)]\tLoss: 2.025163\n",
            "{SqueezeNet} Train Epoch: 6 [35328/37814 (93%)]\tLoss: 1.920796\n",
            "{SqueezeNet} Train Epoch: 6 [35840/37814 (95%)]\tLoss: 2.025460\n",
            "{SqueezeNet} Train Epoch: 6 [36352/37814 (96%)]\tLoss: 1.921833\n",
            "{SqueezeNet} Train Epoch: 6 [36864/37814 (97%)]\tLoss: 1.996229\n",
            "{SqueezeNet} Train Epoch: 6 [31974/37814 (99%)]\tLoss: 1.990485\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 2.0095, Accuracy: 1210/5000 (24%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.7175931930542 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5150c7b2-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2fa7e118-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6d0b2a67c0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51526f86-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_78c1b8e492"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5152bec8-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_a276587c86"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"515308a6-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"5152bec8-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_32159ef644"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5176078e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"51526f86-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_dca341d0f8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51770350-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_97a5d8ce0e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51773a8c-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_ded9cd3300"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5177722c-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"51773a8c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e108608108"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 7 [0/37814 (0%)]\tLoss: 2.314878\n",
            "{AlexNet} Train Epoch: 7 [512/37814 (1%)]\tLoss: 2.313296\n",
            "{AlexNet} Train Epoch: 7 [1024/37814 (3%)]\tLoss: 2.300719\n",
            "{AlexNet} Train Epoch: 7 [1536/37814 (4%)]\tLoss: 2.301732\n",
            "{AlexNet} Train Epoch: 7 [2048/37814 (5%)]\tLoss: 2.303249\n",
            "{AlexNet} Train Epoch: 7 [2560/37814 (7%)]\tLoss: 2.299030\n",
            "{AlexNet} Train Epoch: 7 [3072/37814 (8%)]\tLoss: 2.293176\n",
            "{AlexNet} Train Epoch: 7 [3584/37814 (9%)]\tLoss: 2.300846\n",
            "{AlexNet} Train Epoch: 7 [4096/37814 (11%)]\tLoss: 2.301778\n",
            "{AlexNet} Train Epoch: 7 [4608/37814 (12%)]\tLoss: 2.312639\n",
            "{AlexNet} Train Epoch: 7 [5120/37814 (14%)]\tLoss: 2.311629\n",
            "{AlexNet} Train Epoch: 7 [5632/37814 (15%)]\tLoss: 2.299115\n",
            "{AlexNet} Train Epoch: 7 [6144/37814 (16%)]\tLoss: 2.308362\n",
            "{AlexNet} Train Epoch: 7 [6656/37814 (18%)]\tLoss: 2.287846\n",
            "{AlexNet} Train Epoch: 7 [7168/37814 (19%)]\tLoss: 2.296747\n",
            "{AlexNet} Train Epoch: 7 [7680/37814 (20%)]\tLoss: 2.298727\n",
            "{AlexNet} Train Epoch: 7 [8192/37814 (22%)]\tLoss: 2.303924\n",
            "{AlexNet} Train Epoch: 7 [8704/37814 (23%)]\tLoss: 2.317865\n",
            "{AlexNet} Train Epoch: 7 [9216/37814 (24%)]\tLoss: 2.309705\n",
            "{AlexNet} Train Epoch: 7 [9728/37814 (26%)]\tLoss: 2.290070\n",
            "{AlexNet} Train Epoch: 7 [10240/37814 (27%)]\tLoss: 2.290315\n",
            "{AlexNet} Train Epoch: 7 [10752/37814 (28%)]\tLoss: 2.302449\n",
            "{AlexNet} Train Epoch: 7 [11264/37814 (30%)]\tLoss: 2.300308\n",
            "{AlexNet} Train Epoch: 7 [11776/37814 (31%)]\tLoss: 2.299341\n",
            "{AlexNet} Train Epoch: 7 [12288/37814 (32%)]\tLoss: 2.311680\n",
            "{AlexNet} Train Epoch: 7 [12800/37814 (34%)]\tLoss: 2.295302\n",
            "{AlexNet} Train Epoch: 7 [13312/37814 (35%)]\tLoss: 2.301242\n",
            "{AlexNet} Train Epoch: 7 [13824/37814 (36%)]\tLoss: 2.293248\n",
            "{AlexNet} Train Epoch: 7 [14336/37814 (38%)]\tLoss: 2.294850\n",
            "{AlexNet} Train Epoch: 7 [14848/37814 (39%)]\tLoss: 2.300793\n",
            "{AlexNet} Train Epoch: 7 [15360/37814 (41%)]\tLoss: 2.291574\n",
            "{AlexNet} Train Epoch: 7 [15872/37814 (42%)]\tLoss: 2.289379\n",
            "{AlexNet} Train Epoch: 7 [16384/37814 (43%)]\tLoss: 2.298418\n",
            "{AlexNet} Train Epoch: 7 [16896/37814 (45%)]\tLoss: 2.294441\n",
            "{AlexNet} Train Epoch: 7 [17408/37814 (46%)]\tLoss: 2.303205\n",
            "{AlexNet} Train Epoch: 7 [17920/37814 (47%)]\tLoss: 2.298396\n",
            "{AlexNet} Train Epoch: 7 [18432/37814 (49%)]\tLoss: 2.287826\n",
            "{AlexNet} Train Epoch: 7 [18944/37814 (50%)]\tLoss: 2.293132\n",
            "{AlexNet} Train Epoch: 7 [19456/37814 (51%)]\tLoss: 2.300752\n",
            "{AlexNet} Train Epoch: 7 [19968/37814 (53%)]\tLoss: 2.300743\n",
            "{AlexNet} Train Epoch: 7 [20480/37814 (54%)]\tLoss: 2.290308\n",
            "{AlexNet} Train Epoch: 7 [20992/37814 (55%)]\tLoss: 2.303375\n",
            "{AlexNet} Train Epoch: 7 [21504/37814 (57%)]\tLoss: 2.286629\n",
            "{AlexNet} Train Epoch: 7 [22016/37814 (58%)]\tLoss: 2.286091\n",
            "{AlexNet} Train Epoch: 7 [22528/37814 (59%)]\tLoss: 2.297603\n",
            "{AlexNet} Train Epoch: 7 [23040/37814 (61%)]\tLoss: 2.291847\n",
            "{AlexNet} Train Epoch: 7 [23552/37814 (62%)]\tLoss: 2.289771\n",
            "{AlexNet} Train Epoch: 7 [24064/37814 (64%)]\tLoss: 2.286842\n",
            "{AlexNet} Train Epoch: 7 [24576/37814 (65%)]\tLoss: 2.305525\n",
            "{AlexNet} Train Epoch: 7 [25088/37814 (66%)]\tLoss: 2.286244\n",
            "{AlexNet} Train Epoch: 7 [25600/37814 (68%)]\tLoss: 2.318377\n",
            "{AlexNet} Train Epoch: 7 [26112/37814 (69%)]\tLoss: 2.282250\n",
            "{AlexNet} Train Epoch: 7 [26624/37814 (70%)]\tLoss: 2.288134\n",
            "{AlexNet} Train Epoch: 7 [27136/37814 (72%)]\tLoss: 2.287794\n",
            "{AlexNet} Train Epoch: 7 [27648/37814 (73%)]\tLoss: 2.297182\n",
            "{AlexNet} Train Epoch: 7 [28160/37814 (74%)]\tLoss: 2.287594\n",
            "{AlexNet} Train Epoch: 7 [28672/37814 (76%)]\tLoss: 2.297571\n",
            "{AlexNet} Train Epoch: 7 [29184/37814 (77%)]\tLoss: 2.273305\n",
            "{AlexNet} Train Epoch: 7 [29696/37814 (78%)]\tLoss: 2.276811\n",
            "{AlexNet} Train Epoch: 7 [30208/37814 (80%)]\tLoss: 2.272935\n",
            "{AlexNet} Train Epoch: 7 [30720/37814 (81%)]\tLoss: 2.283965\n",
            "{AlexNet} Train Epoch: 7 [31232/37814 (82%)]\tLoss: 2.283623\n",
            "{AlexNet} Train Epoch: 7 [31744/37814 (84%)]\tLoss: 2.286988\n",
            "{AlexNet} Train Epoch: 7 [32256/37814 (85%)]\tLoss: 2.294715\n",
            "{AlexNet} Train Epoch: 7 [32768/37814 (86%)]\tLoss: 2.279897\n",
            "{AlexNet} Train Epoch: 7 [33280/37814 (88%)]\tLoss: 2.281780\n",
            "{AlexNet} Train Epoch: 7 [33792/37814 (89%)]\tLoss: 2.286396\n",
            "{AlexNet} Train Epoch: 7 [34304/37814 (91%)]\tLoss: 2.268239\n",
            "{AlexNet} Train Epoch: 7 [34816/37814 (92%)]\tLoss: 2.269605\n",
            "{AlexNet} Train Epoch: 7 [35328/37814 (93%)]\tLoss: 2.267325\n",
            "{AlexNet} Train Epoch: 7 [35840/37814 (95%)]\tLoss: 2.281211\n",
            "{AlexNet} Train Epoch: 7 [36352/37814 (96%)]\tLoss: 2.275172\n",
            "{AlexNet} Train Epoch: 7 [36864/37814 (97%)]\tLoss: 2.269935\n",
            "{AlexNet} Train Epoch: 7 [31974/37814 (99%)]\tLoss: 2.276350\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.2757, Accuracy: 811/5000 (16%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.75898027420044 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 7 [0/37814 (0%)]\tLoss: 1.998556\n",
            "{SqueezeNet} Train Epoch: 7 [512/37814 (1%)]\tLoss: 2.019362\n",
            "{SqueezeNet} Train Epoch: 7 [1024/37814 (3%)]\tLoss: 1.963758\n",
            "{SqueezeNet} Train Epoch: 7 [1536/37814 (4%)]\tLoss: 1.989169\n",
            "{SqueezeNet} Train Epoch: 7 [2048/37814 (5%)]\tLoss: 1.914864\n",
            "{SqueezeNet} Train Epoch: 7 [2560/37814 (7%)]\tLoss: 1.976441\n",
            "{SqueezeNet} Train Epoch: 7 [3072/37814 (8%)]\tLoss: 2.011407\n",
            "{SqueezeNet} Train Epoch: 7 [3584/37814 (9%)]\tLoss: 1.961633\n",
            "{SqueezeNet} Train Epoch: 7 [4096/37814 (11%)]\tLoss: 1.958888\n",
            "{SqueezeNet} Train Epoch: 7 [4608/37814 (12%)]\tLoss: 1.954947\n",
            "{SqueezeNet} Train Epoch: 7 [5120/37814 (14%)]\tLoss: 1.918344\n",
            "{SqueezeNet} Train Epoch: 7 [5632/37814 (15%)]\tLoss: 1.928860\n",
            "{SqueezeNet} Train Epoch: 7 [6144/37814 (16%)]\tLoss: 1.940434\n",
            "{SqueezeNet} Train Epoch: 7 [6656/37814 (18%)]\tLoss: 1.954969\n",
            "{SqueezeNet} Train Epoch: 7 [7168/37814 (19%)]\tLoss: 1.908380\n",
            "{SqueezeNet} Train Epoch: 7 [7680/37814 (20%)]\tLoss: 1.931297\n",
            "{SqueezeNet} Train Epoch: 7 [8192/37814 (22%)]\tLoss: 2.010715\n",
            "{SqueezeNet} Train Epoch: 7 [8704/37814 (23%)]\tLoss: 2.001414\n",
            "{SqueezeNet} Train Epoch: 7 [9216/37814 (24%)]\tLoss: 1.907410\n",
            "{SqueezeNet} Train Epoch: 7 [9728/37814 (26%)]\tLoss: 1.978393\n",
            "{SqueezeNet} Train Epoch: 7 [10240/37814 (27%)]\tLoss: 2.012505\n",
            "{SqueezeNet} Train Epoch: 7 [10752/37814 (28%)]\tLoss: 1.894098\n",
            "{SqueezeNet} Train Epoch: 7 [11264/37814 (30%)]\tLoss: 1.957688\n",
            "{SqueezeNet} Train Epoch: 7 [11776/37814 (31%)]\tLoss: 1.938839\n",
            "{SqueezeNet} Train Epoch: 7 [12288/37814 (32%)]\tLoss: 1.988874\n",
            "{SqueezeNet} Train Epoch: 7 [12800/37814 (34%)]\tLoss: 1.948687\n",
            "{SqueezeNet} Train Epoch: 7 [13312/37814 (35%)]\tLoss: 1.936181\n",
            "{SqueezeNet} Train Epoch: 7 [13824/37814 (36%)]\tLoss: 1.980543\n",
            "{SqueezeNet} Train Epoch: 7 [14336/37814 (38%)]\tLoss: 1.947323\n",
            "{SqueezeNet} Train Epoch: 7 [14848/37814 (39%)]\tLoss: 1.918676\n",
            "{SqueezeNet} Train Epoch: 7 [15360/37814 (41%)]\tLoss: 1.932468\n",
            "{SqueezeNet} Train Epoch: 7 [15872/37814 (42%)]\tLoss: 1.900051\n",
            "{SqueezeNet} Train Epoch: 7 [16384/37814 (43%)]\tLoss: 1.920099\n",
            "{SqueezeNet} Train Epoch: 7 [16896/37814 (45%)]\tLoss: 1.926550\n",
            "{SqueezeNet} Train Epoch: 7 [17408/37814 (46%)]\tLoss: 1.954332\n",
            "{SqueezeNet} Train Epoch: 7 [17920/37814 (47%)]\tLoss: 1.883953\n",
            "{SqueezeNet} Train Epoch: 7 [18432/37814 (49%)]\tLoss: 1.960722\n",
            "{SqueezeNet} Train Epoch: 7 [18944/37814 (50%)]\tLoss: 1.968539\n",
            "{SqueezeNet} Train Epoch: 7 [19456/37814 (51%)]\tLoss: 1.932273\n",
            "{SqueezeNet} Train Epoch: 7 [19968/37814 (53%)]\tLoss: 2.006660\n",
            "{SqueezeNet} Train Epoch: 7 [20480/37814 (54%)]\tLoss: 1.972848\n",
            "{SqueezeNet} Train Epoch: 7 [20992/37814 (55%)]\tLoss: 1.986193\n",
            "{SqueezeNet} Train Epoch: 7 [21504/37814 (57%)]\tLoss: 1.959029\n",
            "{SqueezeNet} Train Epoch: 7 [22016/37814 (58%)]\tLoss: 1.934279\n",
            "{SqueezeNet} Train Epoch: 7 [22528/37814 (59%)]\tLoss: 2.171377\n",
            "{SqueezeNet} Train Epoch: 7 [23040/37814 (61%)]\tLoss: 2.001565\n",
            "{SqueezeNet} Train Epoch: 7 [23552/37814 (62%)]\tLoss: 2.005832\n",
            "{SqueezeNet} Train Epoch: 7 [24064/37814 (64%)]\tLoss: 2.021355\n",
            "{SqueezeNet} Train Epoch: 7 [24576/37814 (65%)]\tLoss: 1.952245\n",
            "{SqueezeNet} Train Epoch: 7 [25088/37814 (66%)]\tLoss: 1.948053\n",
            "{SqueezeNet} Train Epoch: 7 [25600/37814 (68%)]\tLoss: 1.977799\n",
            "{SqueezeNet} Train Epoch: 7 [26112/37814 (69%)]\tLoss: 1.921121\n",
            "{SqueezeNet} Train Epoch: 7 [26624/37814 (70%)]\tLoss: 2.000669\n",
            "{SqueezeNet} Train Epoch: 7 [27136/37814 (72%)]\tLoss: 2.004538\n",
            "{SqueezeNet} Train Epoch: 7 [27648/37814 (73%)]\tLoss: 2.001353\n",
            "{SqueezeNet} Train Epoch: 7 [28160/37814 (74%)]\tLoss: 2.022962\n",
            "{SqueezeNet} Train Epoch: 7 [28672/37814 (76%)]\tLoss: 1.962653\n",
            "{SqueezeNet} Train Epoch: 7 [29184/37814 (77%)]\tLoss: 1.992253\n",
            "{SqueezeNet} Train Epoch: 7 [29696/37814 (78%)]\tLoss: 1.957425\n",
            "{SqueezeNet} Train Epoch: 7 [30208/37814 (80%)]\tLoss: 2.006762\n",
            "{SqueezeNet} Train Epoch: 7 [30720/37814 (81%)]\tLoss: 2.026188\n",
            "{SqueezeNet} Train Epoch: 7 [31232/37814 (82%)]\tLoss: 1.909490\n",
            "{SqueezeNet} Train Epoch: 7 [31744/37814 (84%)]\tLoss: 1.982208\n",
            "{SqueezeNet} Train Epoch: 7 [32256/37814 (85%)]\tLoss: 2.009917\n",
            "{SqueezeNet} Train Epoch: 7 [32768/37814 (86%)]\tLoss: 1.961835\n",
            "{SqueezeNet} Train Epoch: 7 [33280/37814 (88%)]\tLoss: 1.994936\n",
            "{SqueezeNet} Train Epoch: 7 [33792/37814 (89%)]\tLoss: 1.959683\n",
            "{SqueezeNet} Train Epoch: 7 [34304/37814 (91%)]\tLoss: 2.025630\n",
            "{SqueezeNet} Train Epoch: 7 [34816/37814 (92%)]\tLoss: 1.967896\n",
            "{SqueezeNet} Train Epoch: 7 [35328/37814 (93%)]\tLoss: 1.997065\n",
            "{SqueezeNet} Train Epoch: 7 [35840/37814 (95%)]\tLoss: 1.950411\n",
            "{SqueezeNet} Train Epoch: 7 [36352/37814 (96%)]\tLoss: 2.038090\n",
            "{SqueezeNet} Train Epoch: 7 [36864/37814 (97%)]\tLoss: 1.957280\n",
            "{SqueezeNet} Train Epoch: 7 [31974/37814 (99%)]\tLoss: 1.898809\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.9373, Accuracy: 1338/5000 (27%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.904481172561646 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"733f05c8-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"51770350-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_ca9d9e36b1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"73416e1c-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_831437d6b2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"7341b03e-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_eecd95b1c3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"7341f56c-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"7341b03e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_bd76119989"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"7364ee64-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"73416e1c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_76b2d9ec02"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"7366a07e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_cc7545fcd7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"7366e2dc-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_bb2bde3126"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"73671ebe-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"7366e2dc-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_ece555873c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 8 [0/37814 (0%)]\tLoss: 2.277581\n",
            "{AlexNet} Train Epoch: 8 [512/37814 (1%)]\tLoss: 2.260645\n",
            "{AlexNet} Train Epoch: 8 [1024/37814 (3%)]\tLoss: 2.272607\n",
            "{AlexNet} Train Epoch: 8 [1536/37814 (4%)]\tLoss: 2.261826\n",
            "{AlexNet} Train Epoch: 8 [2048/37814 (5%)]\tLoss: 2.254200\n",
            "{AlexNet} Train Epoch: 8 [2560/37814 (7%)]\tLoss: 2.262619\n",
            "{AlexNet} Train Epoch: 8 [3072/37814 (8%)]\tLoss: 2.257670\n",
            "{AlexNet} Train Epoch: 8 [3584/37814 (9%)]\tLoss: 2.257531\n",
            "{AlexNet} Train Epoch: 8 [4096/37814 (11%)]\tLoss: 2.273818\n",
            "{AlexNet} Train Epoch: 8 [4608/37814 (12%)]\tLoss: 2.261345\n",
            "{AlexNet} Train Epoch: 8 [5120/37814 (14%)]\tLoss: 2.265386\n",
            "{AlexNet} Train Epoch: 8 [5632/37814 (15%)]\tLoss: 2.262366\n",
            "{AlexNet} Train Epoch: 8 [6144/37814 (16%)]\tLoss: 2.244102\n",
            "{AlexNet} Train Epoch: 8 [6656/37814 (18%)]\tLoss: 2.257318\n",
            "{AlexNet} Train Epoch: 8 [7168/37814 (19%)]\tLoss: 2.267620\n",
            "{AlexNet} Train Epoch: 8 [7680/37814 (20%)]\tLoss: 2.257941\n",
            "{AlexNet} Train Epoch: 8 [8192/37814 (22%)]\tLoss: 2.258812\n",
            "{AlexNet} Train Epoch: 8 [8704/37814 (23%)]\tLoss: 2.246205\n",
            "{AlexNet} Train Epoch: 8 [9216/37814 (24%)]\tLoss: 2.247305\n",
            "{AlexNet} Train Epoch: 8 [9728/37814 (26%)]\tLoss: 2.236861\n",
            "{AlexNet} Train Epoch: 8 [10240/37814 (27%)]\tLoss: 2.227557\n",
            "{AlexNet} Train Epoch: 8 [10752/37814 (28%)]\tLoss: 2.249746\n",
            "{AlexNet} Train Epoch: 8 [11264/37814 (30%)]\tLoss: 2.249072\n",
            "{AlexNet} Train Epoch: 8 [11776/37814 (31%)]\tLoss: 2.238856\n",
            "{AlexNet} Train Epoch: 8 [12288/37814 (32%)]\tLoss: 2.238733\n",
            "{AlexNet} Train Epoch: 8 [12800/37814 (34%)]\tLoss: 2.249659\n",
            "{AlexNet} Train Epoch: 8 [13312/37814 (35%)]\tLoss: 2.242332\n",
            "{AlexNet} Train Epoch: 8 [13824/37814 (36%)]\tLoss: 2.239867\n",
            "{AlexNet} Train Epoch: 8 [14336/37814 (38%)]\tLoss: 2.214712\n",
            "{AlexNet} Train Epoch: 8 [14848/37814 (39%)]\tLoss: 2.242189\n",
            "{AlexNet} Train Epoch: 8 [15360/37814 (41%)]\tLoss: 2.211931\n",
            "{AlexNet} Train Epoch: 8 [15872/37814 (42%)]\tLoss: 2.251219\n",
            "{AlexNet} Train Epoch: 8 [16384/37814 (43%)]\tLoss: 2.234637\n",
            "{AlexNet} Train Epoch: 8 [16896/37814 (45%)]\tLoss: 2.258122\n",
            "{AlexNet} Train Epoch: 8 [17408/37814 (46%)]\tLoss: 2.217752\n",
            "{AlexNet} Train Epoch: 8 [17920/37814 (47%)]\tLoss: 2.257905\n",
            "{AlexNet} Train Epoch: 8 [18432/37814 (49%)]\tLoss: 2.220518\n",
            "{AlexNet} Train Epoch: 8 [18944/37814 (50%)]\tLoss: 2.226989\n",
            "{AlexNet} Train Epoch: 8 [19456/37814 (51%)]\tLoss: 2.223387\n",
            "{AlexNet} Train Epoch: 8 [19968/37814 (53%)]\tLoss: 2.206470\n",
            "{AlexNet} Train Epoch: 8 [20480/37814 (54%)]\tLoss: 2.224475\n",
            "{AlexNet} Train Epoch: 8 [20992/37814 (55%)]\tLoss: 2.239203\n",
            "{AlexNet} Train Epoch: 8 [21504/37814 (57%)]\tLoss: 2.229243\n",
            "{AlexNet} Train Epoch: 8 [22016/37814 (58%)]\tLoss: 2.277176\n",
            "{AlexNet} Train Epoch: 8 [22528/37814 (59%)]\tLoss: 2.217729\n",
            "{AlexNet} Train Epoch: 8 [23040/37814 (61%)]\tLoss: 2.215318\n",
            "{AlexNet} Train Epoch: 8 [23552/37814 (62%)]\tLoss: 2.226544\n",
            "{AlexNet} Train Epoch: 8 [24064/37814 (64%)]\tLoss: 2.250286\n",
            "{AlexNet} Train Epoch: 8 [24576/37814 (65%)]\tLoss: 2.206242\n",
            "{AlexNet} Train Epoch: 8 [25088/37814 (66%)]\tLoss: 2.230508\n",
            "{AlexNet} Train Epoch: 8 [25600/37814 (68%)]\tLoss: 2.235663\n",
            "{AlexNet} Train Epoch: 8 [26112/37814 (69%)]\tLoss: 2.182449\n",
            "{AlexNet} Train Epoch: 8 [26624/37814 (70%)]\tLoss: 2.243652\n",
            "{AlexNet} Train Epoch: 8 [27136/37814 (72%)]\tLoss: 2.223626\n",
            "{AlexNet} Train Epoch: 8 [27648/37814 (73%)]\tLoss: 2.219389\n",
            "{AlexNet} Train Epoch: 8 [28160/37814 (74%)]\tLoss: 2.202591\n",
            "{AlexNet} Train Epoch: 8 [28672/37814 (76%)]\tLoss: 2.201003\n",
            "{AlexNet} Train Epoch: 8 [29184/37814 (77%)]\tLoss: 2.247881\n",
            "{AlexNet} Train Epoch: 8 [29696/37814 (78%)]\tLoss: 2.191780\n",
            "{AlexNet} Train Epoch: 8 [30208/37814 (80%)]\tLoss: 2.304113\n",
            "{AlexNet} Train Epoch: 8 [30720/37814 (81%)]\tLoss: 2.225963\n",
            "{AlexNet} Train Epoch: 8 [31232/37814 (82%)]\tLoss: 2.234143\n",
            "{AlexNet} Train Epoch: 8 [31744/37814 (84%)]\tLoss: 2.224670\n",
            "{AlexNet} Train Epoch: 8 [32256/37814 (85%)]\tLoss: 2.233864\n",
            "{AlexNet} Train Epoch: 8 [32768/37814 (86%)]\tLoss: 2.203130\n",
            "{AlexNet} Train Epoch: 8 [33280/37814 (88%)]\tLoss: 2.208899\n",
            "{AlexNet} Train Epoch: 8 [33792/37814 (89%)]\tLoss: 2.192213\n",
            "{AlexNet} Train Epoch: 8 [34304/37814 (91%)]\tLoss: 2.204943\n",
            "{AlexNet} Train Epoch: 8 [34816/37814 (92%)]\tLoss: 2.193949\n",
            "{AlexNet} Train Epoch: 8 [35328/37814 (93%)]\tLoss: 2.196721\n",
            "{AlexNet} Train Epoch: 8 [35840/37814 (95%)]\tLoss: 2.219468\n",
            "{AlexNet} Train Epoch: 8 [36352/37814 (96%)]\tLoss: 2.187912\n",
            "{AlexNet} Train Epoch: 8 [36864/37814 (97%)]\tLoss: 2.167173\n",
            "{AlexNet} Train Epoch: 8 [31974/37814 (99%)]\tLoss: 2.154472\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.1859, Accuracy: 908/5000 (18%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.809598922729492 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 8 [0/37814 (0%)]\tLoss: 1.950803\n",
            "{SqueezeNet} Train Epoch: 8 [512/37814 (1%)]\tLoss: 1.927053\n",
            "{SqueezeNet} Train Epoch: 8 [1024/37814 (3%)]\tLoss: 1.931307\n",
            "{SqueezeNet} Train Epoch: 8 [1536/37814 (4%)]\tLoss: 1.989991\n",
            "{SqueezeNet} Train Epoch: 8 [2048/37814 (5%)]\tLoss: 1.957049\n",
            "{SqueezeNet} Train Epoch: 8 [2560/37814 (7%)]\tLoss: 1.894887\n",
            "{SqueezeNet} Train Epoch: 8 [3072/37814 (8%)]\tLoss: 1.988533\n",
            "{SqueezeNet} Train Epoch: 8 [3584/37814 (9%)]\tLoss: 1.978317\n",
            "{SqueezeNet} Train Epoch: 8 [4096/37814 (11%)]\tLoss: 1.953239\n",
            "{SqueezeNet} Train Epoch: 8 [4608/37814 (12%)]\tLoss: 1.952933\n",
            "{SqueezeNet} Train Epoch: 8 [5120/37814 (14%)]\tLoss: 1.980685\n",
            "{SqueezeNet} Train Epoch: 8 [5632/37814 (15%)]\tLoss: 2.004859\n",
            "{SqueezeNet} Train Epoch: 8 [6144/37814 (16%)]\tLoss: 1.920600\n",
            "{SqueezeNet} Train Epoch: 8 [6656/37814 (18%)]\tLoss: 1.982738\n",
            "{SqueezeNet} Train Epoch: 8 [7168/37814 (19%)]\tLoss: 1.928012\n",
            "{SqueezeNet} Train Epoch: 8 [7680/37814 (20%)]\tLoss: 1.922383\n",
            "{SqueezeNet} Train Epoch: 8 [8192/37814 (22%)]\tLoss: 1.980789\n",
            "{SqueezeNet} Train Epoch: 8 [8704/37814 (23%)]\tLoss: 1.916282\n",
            "{SqueezeNet} Train Epoch: 8 [9216/37814 (24%)]\tLoss: 1.938234\n",
            "{SqueezeNet} Train Epoch: 8 [9728/37814 (26%)]\tLoss: 1.910142\n",
            "{SqueezeNet} Train Epoch: 8 [10240/37814 (27%)]\tLoss: 1.912536\n",
            "{SqueezeNet} Train Epoch: 8 [10752/37814 (28%)]\tLoss: 1.970951\n",
            "{SqueezeNet} Train Epoch: 8 [11264/37814 (30%)]\tLoss: 1.911374\n",
            "{SqueezeNet} Train Epoch: 8 [11776/37814 (31%)]\tLoss: 1.925133\n",
            "{SqueezeNet} Train Epoch: 8 [12288/37814 (32%)]\tLoss: 1.840351\n",
            "{SqueezeNet} Train Epoch: 8 [12800/37814 (34%)]\tLoss: 1.936375\n",
            "{SqueezeNet} Train Epoch: 8 [13312/37814 (35%)]\tLoss: 1.942156\n",
            "{SqueezeNet} Train Epoch: 8 [13824/37814 (36%)]\tLoss: 1.915085\n",
            "{SqueezeNet} Train Epoch: 8 [14336/37814 (38%)]\tLoss: 2.050404\n",
            "{SqueezeNet} Train Epoch: 8 [14848/37814 (39%)]\tLoss: 1.927505\n",
            "{SqueezeNet} Train Epoch: 8 [15360/37814 (41%)]\tLoss: 1.877084\n",
            "{SqueezeNet} Train Epoch: 8 [15872/37814 (42%)]\tLoss: 1.942080\n",
            "{SqueezeNet} Train Epoch: 8 [16384/37814 (43%)]\tLoss: 1.917471\n",
            "{SqueezeNet} Train Epoch: 8 [16896/37814 (45%)]\tLoss: 1.921443\n",
            "{SqueezeNet} Train Epoch: 8 [17408/37814 (46%)]\tLoss: 1.960212\n",
            "{SqueezeNet} Train Epoch: 8 [17920/37814 (47%)]\tLoss: 1.987399\n",
            "{SqueezeNet} Train Epoch: 8 [18432/37814 (49%)]\tLoss: 1.969757\n",
            "{SqueezeNet} Train Epoch: 8 [18944/37814 (50%)]\tLoss: 1.947161\n",
            "{SqueezeNet} Train Epoch: 8 [19456/37814 (51%)]\tLoss: 1.962585\n",
            "{SqueezeNet} Train Epoch: 8 [19968/37814 (53%)]\tLoss: 1.948561\n",
            "{SqueezeNet} Train Epoch: 8 [20480/37814 (54%)]\tLoss: 2.005337\n",
            "{SqueezeNet} Train Epoch: 8 [20992/37814 (55%)]\tLoss: 1.976315\n",
            "{SqueezeNet} Train Epoch: 8 [21504/37814 (57%)]\tLoss: 1.929485\n",
            "{SqueezeNet} Train Epoch: 8 [22016/37814 (58%)]\tLoss: 1.971277\n",
            "{SqueezeNet} Train Epoch: 8 [22528/37814 (59%)]\tLoss: 1.892339\n",
            "{SqueezeNet} Train Epoch: 8 [23040/37814 (61%)]\tLoss: 1.966154\n",
            "{SqueezeNet} Train Epoch: 8 [23552/37814 (62%)]\tLoss: 1.925235\n",
            "{SqueezeNet} Train Epoch: 8 [24064/37814 (64%)]\tLoss: 1.912145\n",
            "{SqueezeNet} Train Epoch: 8 [24576/37814 (65%)]\tLoss: 1.949376\n",
            "{SqueezeNet} Train Epoch: 8 [25088/37814 (66%)]\tLoss: 1.946220\n",
            "{SqueezeNet} Train Epoch: 8 [25600/37814 (68%)]\tLoss: 1.889215\n",
            "{SqueezeNet} Train Epoch: 8 [26112/37814 (69%)]\tLoss: 1.956254\n",
            "{SqueezeNet} Train Epoch: 8 [26624/37814 (70%)]\tLoss: 1.880437\n",
            "{SqueezeNet} Train Epoch: 8 [27136/37814 (72%)]\tLoss: 1.959821\n",
            "{SqueezeNet} Train Epoch: 8 [27648/37814 (73%)]\tLoss: 1.928977\n",
            "{SqueezeNet} Train Epoch: 8 [28160/37814 (74%)]\tLoss: 1.889722\n",
            "{SqueezeNet} Train Epoch: 8 [28672/37814 (76%)]\tLoss: 1.953989\n",
            "{SqueezeNet} Train Epoch: 8 [29184/37814 (77%)]\tLoss: 1.926342\n",
            "{SqueezeNet} Train Epoch: 8 [29696/37814 (78%)]\tLoss: 1.927345\n",
            "{SqueezeNet} Train Epoch: 8 [30208/37814 (80%)]\tLoss: 1.862349\n",
            "{SqueezeNet} Train Epoch: 8 [30720/37814 (81%)]\tLoss: 1.939977\n",
            "{SqueezeNet} Train Epoch: 8 [31232/37814 (82%)]\tLoss: 1.931363\n",
            "{SqueezeNet} Train Epoch: 8 [31744/37814 (84%)]\tLoss: 1.899935\n",
            "{SqueezeNet} Train Epoch: 8 [32256/37814 (85%)]\tLoss: 1.927480\n",
            "{SqueezeNet} Train Epoch: 8 [32768/37814 (86%)]\tLoss: 1.871285\n",
            "{SqueezeNet} Train Epoch: 8 [33280/37814 (88%)]\tLoss: 1.957017\n",
            "{SqueezeNet} Train Epoch: 8 [33792/37814 (89%)]\tLoss: 1.895280\n",
            "{SqueezeNet} Train Epoch: 8 [34304/37814 (91%)]\tLoss: 1.959537\n",
            "{SqueezeNet} Train Epoch: 8 [34816/37814 (92%)]\tLoss: 1.925979\n",
            "{SqueezeNet} Train Epoch: 8 [35328/37814 (93%)]\tLoss: 1.918294\n",
            "{SqueezeNet} Train Epoch: 8 [35840/37814 (95%)]\tLoss: 1.948339\n",
            "{SqueezeNet} Train Epoch: 8 [36352/37814 (96%)]\tLoss: 1.962388\n",
            "{SqueezeNet} Train Epoch: 8 [36864/37814 (97%)]\tLoss: 2.019886\n",
            "{SqueezeNet} Train Epoch: 8 [31974/37814 (99%)]\tLoss: 1.943314\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.9424, Accuracy: 1362/5000 (27%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.67282271385193 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9512cfe0-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"7366a07e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6776b46b83"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9514621a-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_10983612aa"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9514a48c-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_32d4663bda"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9514ee7e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9514a48c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b9f0dd739c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9538d05a-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9514621a-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c359abaaea"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"953a543e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9d536bf2e7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"953ab6f4-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_3c42e56125"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"953b16e4-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"953ab6f4-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cbacebd7a7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 9 [0/37814 (0%)]\tLoss: 2.202503\n",
            "{AlexNet} Train Epoch: 9 [512/37814 (1%)]\tLoss: 2.192637\n",
            "{AlexNet} Train Epoch: 9 [1024/37814 (3%)]\tLoss: 2.178871\n",
            "{AlexNet} Train Epoch: 9 [1536/37814 (4%)]\tLoss: 2.167657\n",
            "{AlexNet} Train Epoch: 9 [2048/37814 (5%)]\tLoss: 2.154726\n",
            "{AlexNet} Train Epoch: 9 [2560/37814 (7%)]\tLoss: 2.201844\n",
            "{AlexNet} Train Epoch: 9 [3072/37814 (8%)]\tLoss: 2.165842\n",
            "{AlexNet} Train Epoch: 9 [3584/37814 (9%)]\tLoss: 2.178820\n",
            "{AlexNet} Train Epoch: 9 [4096/37814 (11%)]\tLoss: 2.204853\n",
            "{AlexNet} Train Epoch: 9 [4608/37814 (12%)]\tLoss: 2.201622\n",
            "{AlexNet} Train Epoch: 9 [5120/37814 (14%)]\tLoss: 2.167164\n",
            "{AlexNet} Train Epoch: 9 [5632/37814 (15%)]\tLoss: 2.177896\n",
            "{AlexNet} Train Epoch: 9 [6144/37814 (16%)]\tLoss: 2.178546\n",
            "{AlexNet} Train Epoch: 9 [6656/37814 (18%)]\tLoss: 2.176558\n",
            "{AlexNet} Train Epoch: 9 [7168/37814 (19%)]\tLoss: 2.197289\n",
            "{AlexNet} Train Epoch: 9 [7680/37814 (20%)]\tLoss: 2.156894\n",
            "{AlexNet} Train Epoch: 9 [8192/37814 (22%)]\tLoss: 2.214505\n",
            "{AlexNet} Train Epoch: 9 [8704/37814 (23%)]\tLoss: 2.167390\n",
            "{AlexNet} Train Epoch: 9 [9216/37814 (24%)]\tLoss: 2.162848\n",
            "{AlexNet} Train Epoch: 9 [9728/37814 (26%)]\tLoss: 2.253322\n",
            "{AlexNet} Train Epoch: 9 [10240/37814 (27%)]\tLoss: 2.137896\n",
            "{AlexNet} Train Epoch: 9 [10752/37814 (28%)]\tLoss: 2.163067\n",
            "{AlexNet} Train Epoch: 9 [11264/37814 (30%)]\tLoss: 2.235406\n",
            "{AlexNet} Train Epoch: 9 [11776/37814 (31%)]\tLoss: 2.153846\n",
            "{AlexNet} Train Epoch: 9 [12288/37814 (32%)]\tLoss: 2.158183\n",
            "{AlexNet} Train Epoch: 9 [12800/37814 (34%)]\tLoss: 2.153876\n",
            "{AlexNet} Train Epoch: 9 [13312/37814 (35%)]\tLoss: 2.124332\n",
            "{AlexNet} Train Epoch: 9 [13824/37814 (36%)]\tLoss: 2.189434\n",
            "{AlexNet} Train Epoch: 9 [14336/37814 (38%)]\tLoss: 2.158109\n",
            "{AlexNet} Train Epoch: 9 [14848/37814 (39%)]\tLoss: 2.143510\n",
            "{AlexNet} Train Epoch: 9 [15360/37814 (41%)]\tLoss: 2.160342\n",
            "{AlexNet} Train Epoch: 9 [15872/37814 (42%)]\tLoss: 2.161253\n",
            "{AlexNet} Train Epoch: 9 [16384/37814 (43%)]\tLoss: 2.240972\n",
            "{AlexNet} Train Epoch: 9 [16896/37814 (45%)]\tLoss: 2.235195\n",
            "{AlexNet} Train Epoch: 9 [17408/37814 (46%)]\tLoss: 2.241624\n",
            "{AlexNet} Train Epoch: 9 [17920/37814 (47%)]\tLoss: 2.180717\n",
            "{AlexNet} Train Epoch: 9 [18432/37814 (49%)]\tLoss: 2.160759\n",
            "{AlexNet} Train Epoch: 9 [18944/37814 (50%)]\tLoss: 2.163381\n",
            "{AlexNet} Train Epoch: 9 [19456/37814 (51%)]\tLoss: 2.245619\n",
            "{AlexNet} Train Epoch: 9 [19968/37814 (53%)]\tLoss: 2.163604\n",
            "{AlexNet} Train Epoch: 9 [20480/37814 (54%)]\tLoss: 2.272187\n",
            "{AlexNet} Train Epoch: 9 [20992/37814 (55%)]\tLoss: 2.152445\n",
            "{AlexNet} Train Epoch: 9 [21504/37814 (57%)]\tLoss: 2.206511\n",
            "{AlexNet} Train Epoch: 9 [22016/37814 (58%)]\tLoss: 2.242776\n",
            "{AlexNet} Train Epoch: 9 [22528/37814 (59%)]\tLoss: 2.210759\n",
            "{AlexNet} Train Epoch: 9 [23040/37814 (61%)]\tLoss: 2.218578\n",
            "{AlexNet} Train Epoch: 9 [23552/37814 (62%)]\tLoss: 2.147032\n",
            "{AlexNet} Train Epoch: 9 [24064/37814 (64%)]\tLoss: 2.175268\n",
            "{AlexNet} Train Epoch: 9 [24576/37814 (65%)]\tLoss: 2.147571\n",
            "{AlexNet} Train Epoch: 9 [25088/37814 (66%)]\tLoss: 2.151360\n",
            "{AlexNet} Train Epoch: 9 [25600/37814 (68%)]\tLoss: 2.129671\n",
            "{AlexNet} Train Epoch: 9 [26112/37814 (69%)]\tLoss: 2.179364\n",
            "{AlexNet} Train Epoch: 9 [26624/37814 (70%)]\tLoss: 2.111948\n",
            "{AlexNet} Train Epoch: 9 [27136/37814 (72%)]\tLoss: 2.130986\n",
            "{AlexNet} Train Epoch: 9 [27648/37814 (73%)]\tLoss: 2.089102\n",
            "{AlexNet} Train Epoch: 9 [28160/37814 (74%)]\tLoss: 2.154273\n",
            "{AlexNet} Train Epoch: 9 [28672/37814 (76%)]\tLoss: 2.126749\n",
            "{AlexNet} Train Epoch: 9 [29184/37814 (77%)]\tLoss: 2.144793\n",
            "{AlexNet} Train Epoch: 9 [29696/37814 (78%)]\tLoss: 2.136503\n",
            "{AlexNet} Train Epoch: 9 [30208/37814 (80%)]\tLoss: 2.166246\n",
            "{AlexNet} Train Epoch: 9 [30720/37814 (81%)]\tLoss: 2.084504\n",
            "{AlexNet} Train Epoch: 9 [31232/37814 (82%)]\tLoss: 2.156751\n",
            "{AlexNet} Train Epoch: 9 [31744/37814 (84%)]\tLoss: 2.126237\n",
            "{AlexNet} Train Epoch: 9 [32256/37814 (85%)]\tLoss: 2.094861\n",
            "{AlexNet} Train Epoch: 9 [32768/37814 (86%)]\tLoss: 2.130566\n",
            "{AlexNet} Train Epoch: 9 [33280/37814 (88%)]\tLoss: 2.041351\n",
            "{AlexNet} Train Epoch: 9 [33792/37814 (89%)]\tLoss: 2.108814\n",
            "{AlexNet} Train Epoch: 9 [34304/37814 (91%)]\tLoss: 2.108828\n",
            "{AlexNet} Train Epoch: 9 [34816/37814 (92%)]\tLoss: 2.157670\n",
            "{AlexNet} Train Epoch: 9 [35328/37814 (93%)]\tLoss: 2.116861\n",
            "{AlexNet} Train Epoch: 9 [35840/37814 (95%)]\tLoss: 2.166202\n",
            "{AlexNet} Train Epoch: 9 [36352/37814 (96%)]\tLoss: 2.166439\n",
            "{AlexNet} Train Epoch: 9 [36864/37814 (97%)]\tLoss: 2.163841\n",
            "{AlexNet} Train Epoch: 9 [31974/37814 (99%)]\tLoss: 2.215466\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.1395, Accuracy: 987/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.817581176757812 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 9 [0/37814 (0%)]\tLoss: 1.940113\n",
            "{SqueezeNet} Train Epoch: 9 [512/37814 (1%)]\tLoss: 1.915181\n",
            "{SqueezeNet} Train Epoch: 9 [1024/37814 (3%)]\tLoss: 1.905636\n",
            "{SqueezeNet} Train Epoch: 9 [1536/37814 (4%)]\tLoss: 1.938555\n",
            "{SqueezeNet} Train Epoch: 9 [2048/37814 (5%)]\tLoss: 1.943449\n",
            "{SqueezeNet} Train Epoch: 9 [2560/37814 (7%)]\tLoss: 1.982885\n",
            "{SqueezeNet} Train Epoch: 9 [3072/37814 (8%)]\tLoss: 1.973691\n",
            "{SqueezeNet} Train Epoch: 9 [3584/37814 (9%)]\tLoss: 1.944935\n",
            "{SqueezeNet} Train Epoch: 9 [4096/37814 (11%)]\tLoss: 1.913802\n",
            "{SqueezeNet} Train Epoch: 9 [4608/37814 (12%)]\tLoss: 1.963997\n",
            "{SqueezeNet} Train Epoch: 9 [5120/37814 (14%)]\tLoss: 1.888751\n",
            "{SqueezeNet} Train Epoch: 9 [5632/37814 (15%)]\tLoss: 2.060604\n",
            "{SqueezeNet} Train Epoch: 9 [6144/37814 (16%)]\tLoss: 1.978115\n",
            "{SqueezeNet} Train Epoch: 9 [6656/37814 (18%)]\tLoss: 1.938758\n",
            "{SqueezeNet} Train Epoch: 9 [7168/37814 (19%)]\tLoss: 1.904784\n",
            "{SqueezeNet} Train Epoch: 9 [7680/37814 (20%)]\tLoss: 1.922220\n",
            "{SqueezeNet} Train Epoch: 9 [8192/37814 (22%)]\tLoss: 1.893822\n",
            "{SqueezeNet} Train Epoch: 9 [8704/37814 (23%)]\tLoss: 1.963970\n",
            "{SqueezeNet} Train Epoch: 9 [9216/37814 (24%)]\tLoss: 1.928016\n",
            "{SqueezeNet} Train Epoch: 9 [9728/37814 (26%)]\tLoss: 1.899058\n",
            "{SqueezeNet} Train Epoch: 9 [10240/37814 (27%)]\tLoss: 1.882967\n",
            "{SqueezeNet} Train Epoch: 9 [10752/37814 (28%)]\tLoss: 1.931592\n",
            "{SqueezeNet} Train Epoch: 9 [11264/37814 (30%)]\tLoss: 1.819320\n",
            "{SqueezeNet} Train Epoch: 9 [11776/37814 (31%)]\tLoss: 1.901909\n",
            "{SqueezeNet} Train Epoch: 9 [12288/37814 (32%)]\tLoss: 1.957037\n",
            "{SqueezeNet} Train Epoch: 9 [12800/37814 (34%)]\tLoss: 2.007673\n",
            "{SqueezeNet} Train Epoch: 9 [13312/37814 (35%)]\tLoss: 1.927686\n",
            "{SqueezeNet} Train Epoch: 9 [13824/37814 (36%)]\tLoss: 1.924936\n",
            "{SqueezeNet} Train Epoch: 9 [14336/37814 (38%)]\tLoss: 1.982889\n",
            "{SqueezeNet} Train Epoch: 9 [14848/37814 (39%)]\tLoss: 2.017268\n",
            "{SqueezeNet} Train Epoch: 9 [15360/37814 (41%)]\tLoss: 1.928383\n",
            "{SqueezeNet} Train Epoch: 9 [15872/37814 (42%)]\tLoss: 2.014953\n",
            "{SqueezeNet} Train Epoch: 9 [16384/37814 (43%)]\tLoss: 1.887103\n",
            "{SqueezeNet} Train Epoch: 9 [16896/37814 (45%)]\tLoss: 1.994552\n",
            "{SqueezeNet} Train Epoch: 9 [17408/37814 (46%)]\tLoss: 1.913172\n",
            "{SqueezeNet} Train Epoch: 9 [17920/37814 (47%)]\tLoss: 1.977211\n",
            "{SqueezeNet} Train Epoch: 9 [18432/37814 (49%)]\tLoss: 1.906593\n",
            "{SqueezeNet} Train Epoch: 9 [18944/37814 (50%)]\tLoss: 1.960692\n",
            "{SqueezeNet} Train Epoch: 9 [19456/37814 (51%)]\tLoss: 1.927567\n",
            "{SqueezeNet} Train Epoch: 9 [19968/37814 (53%)]\tLoss: 1.895218\n",
            "{SqueezeNet} Train Epoch: 9 [20480/37814 (54%)]\tLoss: 1.954336\n",
            "{SqueezeNet} Train Epoch: 9 [20992/37814 (55%)]\tLoss: 1.923960\n",
            "{SqueezeNet} Train Epoch: 9 [21504/37814 (57%)]\tLoss: 1.957704\n",
            "{SqueezeNet} Train Epoch: 9 [22016/37814 (58%)]\tLoss: 1.894949\n",
            "{SqueezeNet} Train Epoch: 9 [22528/37814 (59%)]\tLoss: 1.935238\n",
            "{SqueezeNet} Train Epoch: 9 [23040/37814 (61%)]\tLoss: 1.859127\n",
            "{SqueezeNet} Train Epoch: 9 [23552/37814 (62%)]\tLoss: 1.917792\n",
            "{SqueezeNet} Train Epoch: 9 [24064/37814 (64%)]\tLoss: 1.920356\n",
            "{SqueezeNet} Train Epoch: 9 [24576/37814 (65%)]\tLoss: 1.975776\n",
            "{SqueezeNet} Train Epoch: 9 [25088/37814 (66%)]\tLoss: 1.877782\n",
            "{SqueezeNet} Train Epoch: 9 [25600/37814 (68%)]\tLoss: 1.907822\n",
            "{SqueezeNet} Train Epoch: 9 [26112/37814 (69%)]\tLoss: 1.926540\n",
            "{SqueezeNet} Train Epoch: 9 [26624/37814 (70%)]\tLoss: 1.891322\n",
            "{SqueezeNet} Train Epoch: 9 [27136/37814 (72%)]\tLoss: 1.941427\n",
            "{SqueezeNet} Train Epoch: 9 [27648/37814 (73%)]\tLoss: 1.928685\n",
            "{SqueezeNet} Train Epoch: 9 [28160/37814 (74%)]\tLoss: 1.892239\n",
            "{SqueezeNet} Train Epoch: 9 [28672/37814 (76%)]\tLoss: 1.923432\n",
            "{SqueezeNet} Train Epoch: 9 [29184/37814 (77%)]\tLoss: 1.895877\n",
            "{SqueezeNet} Train Epoch: 9 [29696/37814 (78%)]\tLoss: 1.978375\n",
            "{SqueezeNet} Train Epoch: 9 [30208/37814 (80%)]\tLoss: 1.862298\n",
            "{SqueezeNet} Train Epoch: 9 [30720/37814 (81%)]\tLoss: 1.923647\n",
            "{SqueezeNet} Train Epoch: 9 [31232/37814 (82%)]\tLoss: 2.041344\n",
            "{SqueezeNet} Train Epoch: 9 [31744/37814 (84%)]\tLoss: 1.873528\n",
            "{SqueezeNet} Train Epoch: 9 [32256/37814 (85%)]\tLoss: 1.985560\n",
            "{SqueezeNet} Train Epoch: 9 [32768/37814 (86%)]\tLoss: 1.957804\n",
            "{SqueezeNet} Train Epoch: 9 [33280/37814 (88%)]\tLoss: 1.974092\n",
            "{SqueezeNet} Train Epoch: 9 [33792/37814 (89%)]\tLoss: 1.966626\n",
            "{SqueezeNet} Train Epoch: 9 [34304/37814 (91%)]\tLoss: 1.911994\n",
            "{SqueezeNet} Train Epoch: 9 [34816/37814 (92%)]\tLoss: 1.882182\n",
            "{SqueezeNet} Train Epoch: 9 [35328/37814 (93%)]\tLoss: 1.931268\n",
            "{SqueezeNet} Train Epoch: 9 [35840/37814 (95%)]\tLoss: 1.854892\n",
            "{SqueezeNet} Train Epoch: 9 [36352/37814 (96%)]\tLoss: 1.946270\n",
            "{SqueezeNet} Train Epoch: 9 [36864/37814 (97%)]\tLoss: 1.920103\n",
            "{SqueezeNet} Train Epoch: 9 [31974/37814 (99%)]\tLoss: 1.925105\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8983, Accuracy: 1370/5000 (27%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.351464986801147 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6b7a328-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"953a543e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9e81d62e5b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6b9494e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9028ecc019"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6b9978c-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_693db6a44e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6b9e052-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b6b9978c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_5d6e7d376d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6dce4e4-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b6b9494e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4c6e2978d5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6de86fa-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_f4a4131448"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6ded07e-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_d0f8261547"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6df012a-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b6ded07e-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_38cc8ff504"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 10 [0/37814 (0%)]\tLoss: 2.127446\n",
            "{AlexNet} Train Epoch: 10 [512/37814 (1%)]\tLoss: 2.139353\n",
            "{AlexNet} Train Epoch: 10 [1024/37814 (3%)]\tLoss: 2.141275\n",
            "{AlexNet} Train Epoch: 10 [1536/37814 (4%)]\tLoss: 2.132545\n",
            "{AlexNet} Train Epoch: 10 [2048/37814 (5%)]\tLoss: 2.129830\n",
            "{AlexNet} Train Epoch: 10 [2560/37814 (7%)]\tLoss: 2.063516\n",
            "{AlexNet} Train Epoch: 10 [3072/37814 (8%)]\tLoss: 2.098363\n",
            "{AlexNet} Train Epoch: 10 [3584/37814 (9%)]\tLoss: 2.140225\n",
            "{AlexNet} Train Epoch: 10 [4096/37814 (11%)]\tLoss: 2.084375\n",
            "{AlexNet} Train Epoch: 10 [4608/37814 (12%)]\tLoss: 2.113719\n",
            "{AlexNet} Train Epoch: 10 [5120/37814 (14%)]\tLoss: 2.159553\n",
            "{AlexNet} Train Epoch: 10 [5632/37814 (15%)]\tLoss: 2.100150\n",
            "{AlexNet} Train Epoch: 10 [6144/37814 (16%)]\tLoss: 2.135581\n",
            "{AlexNet} Train Epoch: 10 [6656/37814 (18%)]\tLoss: 2.134318\n",
            "{AlexNet} Train Epoch: 10 [7168/37814 (19%)]\tLoss: 2.151272\n",
            "{AlexNet} Train Epoch: 10 [7680/37814 (20%)]\tLoss: 2.075571\n",
            "{AlexNet} Train Epoch: 10 [8192/37814 (22%)]\tLoss: 2.138919\n",
            "{AlexNet} Train Epoch: 10 [8704/37814 (23%)]\tLoss: 2.064641\n",
            "{AlexNet} Train Epoch: 10 [9216/37814 (24%)]\tLoss: 2.091544\n",
            "{AlexNet} Train Epoch: 10 [9728/37814 (26%)]\tLoss: 2.151087\n",
            "{AlexNet} Train Epoch: 10 [10240/37814 (27%)]\tLoss: 2.111251\n",
            "{AlexNet} Train Epoch: 10 [10752/37814 (28%)]\tLoss: 2.086863\n",
            "{AlexNet} Train Epoch: 10 [11264/37814 (30%)]\tLoss: 2.072118\n",
            "{AlexNet} Train Epoch: 10 [11776/37814 (31%)]\tLoss: 2.068129\n",
            "{AlexNet} Train Epoch: 10 [12288/37814 (32%)]\tLoss: 2.105935\n",
            "{AlexNet} Train Epoch: 10 [12800/37814 (34%)]\tLoss: 2.047019\n",
            "{AlexNet} Train Epoch: 10 [13312/37814 (35%)]\tLoss: 2.041240\n",
            "{AlexNet} Train Epoch: 10 [13824/37814 (36%)]\tLoss: 2.083453\n",
            "{AlexNet} Train Epoch: 10 [14336/37814 (38%)]\tLoss: 2.061708\n",
            "{AlexNet} Train Epoch: 10 [14848/37814 (39%)]\tLoss: 2.058598\n",
            "{AlexNet} Train Epoch: 10 [15360/37814 (41%)]\tLoss: 2.075629\n",
            "{AlexNet} Train Epoch: 10 [15872/37814 (42%)]\tLoss: 2.018803\n",
            "{AlexNet} Train Epoch: 10 [16384/37814 (43%)]\tLoss: 2.043594\n",
            "{AlexNet} Train Epoch: 10 [16896/37814 (45%)]\tLoss: 2.110430\n",
            "{AlexNet} Train Epoch: 10 [17408/37814 (46%)]\tLoss: 2.145168\n",
            "{AlexNet} Train Epoch: 10 [17920/37814 (47%)]\tLoss: 2.049181\n",
            "{AlexNet} Train Epoch: 10 [18432/37814 (49%)]\tLoss: 2.023075\n",
            "{AlexNet} Train Epoch: 10 [18944/37814 (50%)]\tLoss: 2.100030\n",
            "{AlexNet} Train Epoch: 10 [19456/37814 (51%)]\tLoss: 2.107055\n",
            "{AlexNet} Train Epoch: 10 [19968/37814 (53%)]\tLoss: 2.068187\n",
            "{AlexNet} Train Epoch: 10 [20480/37814 (54%)]\tLoss: 2.042948\n",
            "{AlexNet} Train Epoch: 10 [20992/37814 (55%)]\tLoss: 2.068601\n",
            "{AlexNet} Train Epoch: 10 [21504/37814 (57%)]\tLoss: 2.083574\n",
            "{AlexNet} Train Epoch: 10 [22016/37814 (58%)]\tLoss: 2.022429\n",
            "{AlexNet} Train Epoch: 10 [22528/37814 (59%)]\tLoss: 2.053089\n",
            "{AlexNet} Train Epoch: 10 [23040/37814 (61%)]\tLoss: 2.045980\n",
            "{AlexNet} Train Epoch: 10 [23552/37814 (62%)]\tLoss: 2.066966\n",
            "{AlexNet} Train Epoch: 10 [24064/37814 (64%)]\tLoss: 1.985111\n",
            "{AlexNet} Train Epoch: 10 [24576/37814 (65%)]\tLoss: 2.077071\n",
            "{AlexNet} Train Epoch: 10 [25088/37814 (66%)]\tLoss: 2.093216\n",
            "{AlexNet} Train Epoch: 10 [25600/37814 (68%)]\tLoss: 2.042962\n",
            "{AlexNet} Train Epoch: 10 [26112/37814 (69%)]\tLoss: 2.274814\n",
            "{AlexNet} Train Epoch: 10 [26624/37814 (70%)]\tLoss: 2.235995\n",
            "{AlexNet} Train Epoch: 10 [27136/37814 (72%)]\tLoss: 2.079596\n",
            "{AlexNet} Train Epoch: 10 [27648/37814 (73%)]\tLoss: 2.215170\n",
            "{AlexNet} Train Epoch: 10 [28160/37814 (74%)]\tLoss: 2.150833\n",
            "{AlexNet} Train Epoch: 10 [28672/37814 (76%)]\tLoss: 2.139810\n",
            "{AlexNet} Train Epoch: 10 [29184/37814 (77%)]\tLoss: 2.242557\n",
            "{AlexNet} Train Epoch: 10 [29696/37814 (78%)]\tLoss: 2.086607\n",
            "{AlexNet} Train Epoch: 10 [30208/37814 (80%)]\tLoss: 2.191872\n",
            "{AlexNet} Train Epoch: 10 [30720/37814 (81%)]\tLoss: 2.169750\n",
            "{AlexNet} Train Epoch: 10 [31232/37814 (82%)]\tLoss: 2.092133\n",
            "{AlexNet} Train Epoch: 10 [31744/37814 (84%)]\tLoss: 2.157271\n",
            "{AlexNet} Train Epoch: 10 [32256/37814 (85%)]\tLoss: 2.115664\n",
            "{AlexNet} Train Epoch: 10 [32768/37814 (86%)]\tLoss: 2.080968\n",
            "{AlexNet} Train Epoch: 10 [33280/37814 (88%)]\tLoss: 2.108411\n",
            "{AlexNet} Train Epoch: 10 [33792/37814 (89%)]\tLoss: 2.077347\n",
            "{AlexNet} Train Epoch: 10 [34304/37814 (91%)]\tLoss: 2.114053\n",
            "{AlexNet} Train Epoch: 10 [34816/37814 (92%)]\tLoss: 2.080416\n",
            "{AlexNet} Train Epoch: 10 [35328/37814 (93%)]\tLoss: 2.094341\n",
            "{AlexNet} Train Epoch: 10 [35840/37814 (95%)]\tLoss: 2.103419\n",
            "{AlexNet} Train Epoch: 10 [36352/37814 (96%)]\tLoss: 2.079841\n",
            "{AlexNet} Train Epoch: 10 [36864/37814 (97%)]\tLoss: 2.117714\n",
            "{AlexNet} Train Epoch: 10 [31974/37814 (99%)]\tLoss: 2.050295\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.0875, Accuracy: 965/5000 (19%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.775487184524536 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 10 [0/37814 (0%)]\tLoss: 1.930767\n",
            "{SqueezeNet} Train Epoch: 10 [512/37814 (1%)]\tLoss: 1.901931\n",
            "{SqueezeNet} Train Epoch: 10 [1024/37814 (3%)]\tLoss: 1.917469\n",
            "{SqueezeNet} Train Epoch: 10 [1536/37814 (4%)]\tLoss: 1.936119\n",
            "{SqueezeNet} Train Epoch: 10 [2048/37814 (5%)]\tLoss: 1.866678\n",
            "{SqueezeNet} Train Epoch: 10 [2560/37814 (7%)]\tLoss: 1.942940\n",
            "{SqueezeNet} Train Epoch: 10 [3072/37814 (8%)]\tLoss: 1.930312\n",
            "{SqueezeNet} Train Epoch: 10 [3584/37814 (9%)]\tLoss: 1.908938\n",
            "{SqueezeNet} Train Epoch: 10 [4096/37814 (11%)]\tLoss: 1.926631\n",
            "{SqueezeNet} Train Epoch: 10 [4608/37814 (12%)]\tLoss: 1.889354\n",
            "{SqueezeNet} Train Epoch: 10 [5120/37814 (14%)]\tLoss: 1.976629\n",
            "{SqueezeNet} Train Epoch: 10 [5632/37814 (15%)]\tLoss: 1.973330\n",
            "{SqueezeNet} Train Epoch: 10 [6144/37814 (16%)]\tLoss: 1.864532\n",
            "{SqueezeNet} Train Epoch: 10 [6656/37814 (18%)]\tLoss: 1.955077\n",
            "{SqueezeNet} Train Epoch: 10 [7168/37814 (19%)]\tLoss: 1.918218\n",
            "{SqueezeNet} Train Epoch: 10 [7680/37814 (20%)]\tLoss: 1.903725\n",
            "{SqueezeNet} Train Epoch: 10 [8192/37814 (22%)]\tLoss: 1.952579\n",
            "{SqueezeNet} Train Epoch: 10 [8704/37814 (23%)]\tLoss: 1.963630\n",
            "{SqueezeNet} Train Epoch: 10 [9216/37814 (24%)]\tLoss: 1.907646\n",
            "{SqueezeNet} Train Epoch: 10 [9728/37814 (26%)]\tLoss: 1.878588\n",
            "{SqueezeNet} Train Epoch: 10 [10240/37814 (27%)]\tLoss: 1.857043\n",
            "{SqueezeNet} Train Epoch: 10 [10752/37814 (28%)]\tLoss: 1.846905\n",
            "{SqueezeNet} Train Epoch: 10 [11264/37814 (30%)]\tLoss: 1.831051\n",
            "{SqueezeNet} Train Epoch: 10 [11776/37814 (31%)]\tLoss: 1.954296\n",
            "{SqueezeNet} Train Epoch: 10 [12288/37814 (32%)]\tLoss: 1.885224\n",
            "{SqueezeNet} Train Epoch: 10 [12800/37814 (34%)]\tLoss: 1.875742\n",
            "{SqueezeNet} Train Epoch: 10 [13312/37814 (35%)]\tLoss: 1.893831\n",
            "{SqueezeNet} Train Epoch: 10 [13824/37814 (36%)]\tLoss: 1.861413\n",
            "{SqueezeNet} Train Epoch: 10 [14336/37814 (38%)]\tLoss: 1.906631\n",
            "{SqueezeNet} Train Epoch: 10 [14848/37814 (39%)]\tLoss: 1.891970\n",
            "{SqueezeNet} Train Epoch: 10 [15360/37814 (41%)]\tLoss: 1.916841\n",
            "{SqueezeNet} Train Epoch: 10 [15872/37814 (42%)]\tLoss: 1.864854\n",
            "{SqueezeNet} Train Epoch: 10 [16384/37814 (43%)]\tLoss: 1.822863\n",
            "{SqueezeNet} Train Epoch: 10 [16896/37814 (45%)]\tLoss: 1.830339\n",
            "{SqueezeNet} Train Epoch: 10 [17408/37814 (46%)]\tLoss: 1.868162\n",
            "{SqueezeNet} Train Epoch: 10 [17920/37814 (47%)]\tLoss: 1.910272\n",
            "{SqueezeNet} Train Epoch: 10 [18432/37814 (49%)]\tLoss: 1.833130\n",
            "{SqueezeNet} Train Epoch: 10 [18944/37814 (50%)]\tLoss: 1.875317\n",
            "{SqueezeNet} Train Epoch: 10 [19456/37814 (51%)]\tLoss: 1.890532\n",
            "{SqueezeNet} Train Epoch: 10 [19968/37814 (53%)]\tLoss: 1.848153\n",
            "{SqueezeNet} Train Epoch: 10 [20480/37814 (54%)]\tLoss: 1.838232\n",
            "{SqueezeNet} Train Epoch: 10 [20992/37814 (55%)]\tLoss: 1.891225\n",
            "{SqueezeNet} Train Epoch: 10 [21504/37814 (57%)]\tLoss: 1.931339\n",
            "{SqueezeNet} Train Epoch: 10 [22016/37814 (58%)]\tLoss: 1.808553\n",
            "{SqueezeNet} Train Epoch: 10 [22528/37814 (59%)]\tLoss: 1.831237\n",
            "{SqueezeNet} Train Epoch: 10 [23040/37814 (61%)]\tLoss: 1.861478\n",
            "{SqueezeNet} Train Epoch: 10 [23552/37814 (62%)]\tLoss: 1.929031\n",
            "{SqueezeNet} Train Epoch: 10 [24064/37814 (64%)]\tLoss: 1.877332\n",
            "{SqueezeNet} Train Epoch: 10 [24576/37814 (65%)]\tLoss: 1.877102\n",
            "{SqueezeNet} Train Epoch: 10 [25088/37814 (66%)]\tLoss: 1.934149\n",
            "{SqueezeNet} Train Epoch: 10 [25600/37814 (68%)]\tLoss: 1.928876\n",
            "{SqueezeNet} Train Epoch: 10 [26112/37814 (69%)]\tLoss: 1.885460\n",
            "{SqueezeNet} Train Epoch: 10 [26624/37814 (70%)]\tLoss: 1.908076\n",
            "{SqueezeNet} Train Epoch: 10 [27136/37814 (72%)]\tLoss: 1.962268\n",
            "{SqueezeNet} Train Epoch: 10 [27648/37814 (73%)]\tLoss: 1.914096\n",
            "{SqueezeNet} Train Epoch: 10 [28160/37814 (74%)]\tLoss: 1.986185\n",
            "{SqueezeNet} Train Epoch: 10 [28672/37814 (76%)]\tLoss: 1.833582\n",
            "{SqueezeNet} Train Epoch: 10 [29184/37814 (77%)]\tLoss: 1.877750\n",
            "{SqueezeNet} Train Epoch: 10 [29696/37814 (78%)]\tLoss: 1.884781\n",
            "{SqueezeNet} Train Epoch: 10 [30208/37814 (80%)]\tLoss: 1.877684\n",
            "{SqueezeNet} Train Epoch: 10 [30720/37814 (81%)]\tLoss: 1.946107\n",
            "{SqueezeNet} Train Epoch: 10 [31232/37814 (82%)]\tLoss: 1.943531\n",
            "{SqueezeNet} Train Epoch: 10 [31744/37814 (84%)]\tLoss: 1.948259\n",
            "{SqueezeNet} Train Epoch: 10 [32256/37814 (85%)]\tLoss: 1.915512\n",
            "{SqueezeNet} Train Epoch: 10 [32768/37814 (86%)]\tLoss: 1.909427\n",
            "{SqueezeNet} Train Epoch: 10 [33280/37814 (88%)]\tLoss: 1.897663\n",
            "{SqueezeNet} Train Epoch: 10 [33792/37814 (89%)]\tLoss: 1.930782\n",
            "{SqueezeNet} Train Epoch: 10 [34304/37814 (91%)]\tLoss: 1.909321\n",
            "{SqueezeNet} Train Epoch: 10 [34816/37814 (92%)]\tLoss: 1.902819\n",
            "{SqueezeNet} Train Epoch: 10 [35328/37814 (93%)]\tLoss: 1.901751\n",
            "{SqueezeNet} Train Epoch: 10 [35840/37814 (95%)]\tLoss: 1.876292\n",
            "{SqueezeNet} Train Epoch: 10 [36352/37814 (96%)]\tLoss: 1.965252\n",
            "{SqueezeNet} Train Epoch: 10 [36864/37814 (97%)]\tLoss: 1.864961\n",
            "{SqueezeNet} Train Epoch: 10 [31974/37814 (99%)]\tLoss: 1.815777\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8684, Accuracy: 1472/5000 (29%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.258583784103394 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d846ac82-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b6de86fa-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_26b2e1aaaf"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d8484858-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_6dbcc496a1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d848ac6c-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_fc1bf97173"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d848f3a2-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d848ac6c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4bbaf8e282"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d86e8248-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d8484858-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4aa2dcf5f9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d870ba7c-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_2a81d98f6c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d8710bb2-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_4015d40555"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d871703e-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d8710bb2-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_fbf6bd267a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 11 [0/37814 (0%)]\tLoss: 2.090088\n",
            "{AlexNet} Train Epoch: 11 [512/37814 (1%)]\tLoss: 2.090844\n",
            "{AlexNet} Train Epoch: 11 [1024/37814 (3%)]\tLoss: 2.070280\n",
            "{AlexNet} Train Epoch: 11 [1536/37814 (4%)]\tLoss: 2.067808\n",
            "{AlexNet} Train Epoch: 11 [2048/37814 (5%)]\tLoss: 2.034042\n",
            "{AlexNet} Train Epoch: 11 [2560/37814 (7%)]\tLoss: 2.026262\n",
            "{AlexNet} Train Epoch: 11 [3072/37814 (8%)]\tLoss: 2.057698\n",
            "{AlexNet} Train Epoch: 11 [3584/37814 (9%)]\tLoss: 2.060660\n",
            "{AlexNet} Train Epoch: 11 [4096/37814 (11%)]\tLoss: 2.061273\n",
            "{AlexNet} Train Epoch: 11 [4608/37814 (12%)]\tLoss: 2.037335\n",
            "{AlexNet} Train Epoch: 11 [5120/37814 (14%)]\tLoss: 2.038893\n",
            "{AlexNet} Train Epoch: 11 [5632/37814 (15%)]\tLoss: 2.059415\n",
            "{AlexNet} Train Epoch: 11 [6144/37814 (16%)]\tLoss: 1.994953\n",
            "{AlexNet} Train Epoch: 11 [6656/37814 (18%)]\tLoss: 2.056221\n",
            "{AlexNet} Train Epoch: 11 [7168/37814 (19%)]\tLoss: 2.036585\n",
            "{AlexNet} Train Epoch: 11 [7680/37814 (20%)]\tLoss: 2.013898\n",
            "{AlexNet} Train Epoch: 11 [8192/37814 (22%)]\tLoss: 2.018969\n",
            "{AlexNet} Train Epoch: 11 [8704/37814 (23%)]\tLoss: 2.029907\n",
            "{AlexNet} Train Epoch: 11 [9216/37814 (24%)]\tLoss: 2.028281\n",
            "{AlexNet} Train Epoch: 11 [9728/37814 (26%)]\tLoss: 2.022656\n",
            "{AlexNet} Train Epoch: 11 [10240/37814 (27%)]\tLoss: 2.071305\n",
            "{AlexNet} Train Epoch: 11 [10752/37814 (28%)]\tLoss: 2.042411\n",
            "{AlexNet} Train Epoch: 11 [11264/37814 (30%)]\tLoss: 2.005429\n",
            "{AlexNet} Train Epoch: 11 [11776/37814 (31%)]\tLoss: 2.041168\n",
            "{AlexNet} Train Epoch: 11 [12288/37814 (32%)]\tLoss: 2.030031\n",
            "{AlexNet} Train Epoch: 11 [12800/37814 (34%)]\tLoss: 2.019659\n",
            "{AlexNet} Train Epoch: 11 [13312/37814 (35%)]\tLoss: 2.026900\n",
            "{AlexNet} Train Epoch: 11 [13824/37814 (36%)]\tLoss: 2.042467\n",
            "{AlexNet} Train Epoch: 11 [14336/37814 (38%)]\tLoss: 2.067663\n",
            "{AlexNet} Train Epoch: 11 [14848/37814 (39%)]\tLoss: 2.008950\n",
            "{AlexNet} Train Epoch: 11 [15360/37814 (41%)]\tLoss: 2.023221\n",
            "{AlexNet} Train Epoch: 11 [15872/37814 (42%)]\tLoss: 2.023674\n",
            "{AlexNet} Train Epoch: 11 [16384/37814 (43%)]\tLoss: 2.030549\n",
            "{AlexNet} Train Epoch: 11 [16896/37814 (45%)]\tLoss: 2.040670\n",
            "{AlexNet} Train Epoch: 11 [17408/37814 (46%)]\tLoss: 2.002994\n",
            "{AlexNet} Train Epoch: 11 [17920/37814 (47%)]\tLoss: 2.009611\n",
            "{AlexNet} Train Epoch: 11 [18432/37814 (49%)]\tLoss: 2.067195\n",
            "{AlexNet} Train Epoch: 11 [18944/37814 (50%)]\tLoss: 1.987406\n",
            "{AlexNet} Train Epoch: 11 [19456/37814 (51%)]\tLoss: 1.981488\n",
            "{AlexNet} Train Epoch: 11 [19968/37814 (53%)]\tLoss: 1.990709\n",
            "{AlexNet} Train Epoch: 11 [20480/37814 (54%)]\tLoss: 2.034676\n",
            "{AlexNet} Train Epoch: 11 [20992/37814 (55%)]\tLoss: 1.995554\n",
            "{AlexNet} Train Epoch: 11 [21504/37814 (57%)]\tLoss: 2.004596\n",
            "{AlexNet} Train Epoch: 11 [22016/37814 (58%)]\tLoss: 2.012794\n",
            "{AlexNet} Train Epoch: 11 [22528/37814 (59%)]\tLoss: 2.054102\n",
            "{AlexNet} Train Epoch: 11 [23040/37814 (61%)]\tLoss: 1.965926\n",
            "{AlexNet} Train Epoch: 11 [23552/37814 (62%)]\tLoss: 2.017609\n",
            "{AlexNet} Train Epoch: 11 [24064/37814 (64%)]\tLoss: 2.032570\n",
            "{AlexNet} Train Epoch: 11 [24576/37814 (65%)]\tLoss: 2.029583\n",
            "{AlexNet} Train Epoch: 11 [25088/37814 (66%)]\tLoss: 2.022956\n",
            "{AlexNet} Train Epoch: 11 [25600/37814 (68%)]\tLoss: 1.985861\n",
            "{AlexNet} Train Epoch: 11 [26112/37814 (69%)]\tLoss: 2.051683\n",
            "{AlexNet} Train Epoch: 11 [26624/37814 (70%)]\tLoss: 2.006146\n",
            "{AlexNet} Train Epoch: 11 [27136/37814 (72%)]\tLoss: 2.008603\n",
            "{AlexNet} Train Epoch: 11 [27648/37814 (73%)]\tLoss: 2.036945\n",
            "{AlexNet} Train Epoch: 11 [28160/37814 (74%)]\tLoss: 2.025339\n",
            "{AlexNet} Train Epoch: 11 [28672/37814 (76%)]\tLoss: 1.973455\n",
            "{AlexNet} Train Epoch: 11 [29184/37814 (77%)]\tLoss: 2.029328\n",
            "{AlexNet} Train Epoch: 11 [29696/37814 (78%)]\tLoss: 2.033871\n",
            "{AlexNet} Train Epoch: 11 [30208/37814 (80%)]\tLoss: 2.025796\n",
            "{AlexNet} Train Epoch: 11 [30720/37814 (81%)]\tLoss: 1.991224\n",
            "{AlexNet} Train Epoch: 11 [31232/37814 (82%)]\tLoss: 2.029175\n",
            "{AlexNet} Train Epoch: 11 [31744/37814 (84%)]\tLoss: 2.015179\n",
            "{AlexNet} Train Epoch: 11 [32256/37814 (85%)]\tLoss: 1.972243\n",
            "{AlexNet} Train Epoch: 11 [32768/37814 (86%)]\tLoss: 1.971092\n",
            "{AlexNet} Train Epoch: 11 [33280/37814 (88%)]\tLoss: 2.012989\n",
            "{AlexNet} Train Epoch: 11 [33792/37814 (89%)]\tLoss: 2.054259\n",
            "{AlexNet} Train Epoch: 11 [34304/37814 (91%)]\tLoss: 2.060634\n",
            "{AlexNet} Train Epoch: 11 [34816/37814 (92%)]\tLoss: 2.016717\n",
            "{AlexNet} Train Epoch: 11 [35328/37814 (93%)]\tLoss: 2.012558\n",
            "{AlexNet} Train Epoch: 11 [35840/37814 (95%)]\tLoss: 2.014938\n",
            "{AlexNet} Train Epoch: 11 [36352/37814 (96%)]\tLoss: 2.009912\n",
            "{AlexNet} Train Epoch: 11 [36864/37814 (97%)]\tLoss: 2.041810\n",
            "{AlexNet} Train Epoch: 11 [31974/37814 (99%)]\tLoss: 2.038884\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 2.0087, Accuracy: 1038/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.78160071372986 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 11 [0/37814 (0%)]\tLoss: 1.822044\n",
            "{SqueezeNet} Train Epoch: 11 [512/37814 (1%)]\tLoss: 1.822457\n",
            "{SqueezeNet} Train Epoch: 11 [1024/37814 (3%)]\tLoss: 1.883188\n",
            "{SqueezeNet} Train Epoch: 11 [1536/37814 (4%)]\tLoss: 1.890352\n",
            "{SqueezeNet} Train Epoch: 11 [2048/37814 (5%)]\tLoss: 1.903798\n",
            "{SqueezeNet} Train Epoch: 11 [2560/37814 (7%)]\tLoss: 1.879580\n",
            "{SqueezeNet} Train Epoch: 11 [3072/37814 (8%)]\tLoss: 1.881354\n",
            "{SqueezeNet} Train Epoch: 11 [3584/37814 (9%)]\tLoss: 1.839598\n",
            "{SqueezeNet} Train Epoch: 11 [4096/37814 (11%)]\tLoss: 1.881342\n",
            "{SqueezeNet} Train Epoch: 11 [4608/37814 (12%)]\tLoss: 1.909092\n",
            "{SqueezeNet} Train Epoch: 11 [5120/37814 (14%)]\tLoss: 1.833144\n",
            "{SqueezeNet} Train Epoch: 11 [5632/37814 (15%)]\tLoss: 1.847325\n",
            "{SqueezeNet} Train Epoch: 11 [6144/37814 (16%)]\tLoss: 1.835896\n",
            "{SqueezeNet} Train Epoch: 11 [6656/37814 (18%)]\tLoss: 1.871354\n",
            "{SqueezeNet} Train Epoch: 11 [7168/37814 (19%)]\tLoss: 1.898104\n",
            "{SqueezeNet} Train Epoch: 11 [7680/37814 (20%)]\tLoss: 1.883078\n",
            "{SqueezeNet} Train Epoch: 11 [8192/37814 (22%)]\tLoss: 1.918222\n",
            "{SqueezeNet} Train Epoch: 11 [8704/37814 (23%)]\tLoss: 1.846914\n",
            "{SqueezeNet} Train Epoch: 11 [9216/37814 (24%)]\tLoss: 1.839409\n",
            "{SqueezeNet} Train Epoch: 11 [9728/37814 (26%)]\tLoss: 1.961579\n",
            "{SqueezeNet} Train Epoch: 11 [10240/37814 (27%)]\tLoss: 1.868334\n",
            "{SqueezeNet} Train Epoch: 11 [10752/37814 (28%)]\tLoss: 1.868526\n",
            "{SqueezeNet} Train Epoch: 11 [11264/37814 (30%)]\tLoss: 1.895680\n",
            "{SqueezeNet} Train Epoch: 11 [11776/37814 (31%)]\tLoss: 1.854308\n",
            "{SqueezeNet} Train Epoch: 11 [12288/37814 (32%)]\tLoss: 1.892594\n",
            "{SqueezeNet} Train Epoch: 11 [12800/37814 (34%)]\tLoss: 1.871008\n",
            "{SqueezeNet} Train Epoch: 11 [13312/37814 (35%)]\tLoss: 1.844124\n",
            "{SqueezeNet} Train Epoch: 11 [13824/37814 (36%)]\tLoss: 1.884364\n",
            "{SqueezeNet} Train Epoch: 11 [14336/37814 (38%)]\tLoss: 1.851145\n",
            "{SqueezeNet} Train Epoch: 11 [14848/37814 (39%)]\tLoss: 1.897847\n",
            "{SqueezeNet} Train Epoch: 11 [15360/37814 (41%)]\tLoss: 1.902539\n",
            "{SqueezeNet} Train Epoch: 11 [15872/37814 (42%)]\tLoss: 1.874865\n",
            "{SqueezeNet} Train Epoch: 11 [16384/37814 (43%)]\tLoss: 1.889361\n",
            "{SqueezeNet} Train Epoch: 11 [16896/37814 (45%)]\tLoss: 1.862602\n",
            "{SqueezeNet} Train Epoch: 11 [17408/37814 (46%)]\tLoss: 1.853231\n",
            "{SqueezeNet} Train Epoch: 11 [17920/37814 (47%)]\tLoss: 1.834292\n",
            "{SqueezeNet} Train Epoch: 11 [18432/37814 (49%)]\tLoss: 1.887664\n",
            "{SqueezeNet} Train Epoch: 11 [18944/37814 (50%)]\tLoss: 1.845462\n",
            "{SqueezeNet} Train Epoch: 11 [19456/37814 (51%)]\tLoss: 1.874297\n",
            "{SqueezeNet} Train Epoch: 11 [19968/37814 (53%)]\tLoss: 1.877293\n",
            "{SqueezeNet} Train Epoch: 11 [20480/37814 (54%)]\tLoss: 1.838018\n",
            "{SqueezeNet} Train Epoch: 11 [20992/37814 (55%)]\tLoss: 1.791177\n",
            "{SqueezeNet} Train Epoch: 11 [21504/37814 (57%)]\tLoss: 1.868330\n",
            "{SqueezeNet} Train Epoch: 11 [22016/37814 (58%)]\tLoss: 1.880596\n",
            "{SqueezeNet} Train Epoch: 11 [22528/37814 (59%)]\tLoss: 1.837548\n",
            "{SqueezeNet} Train Epoch: 11 [23040/37814 (61%)]\tLoss: 1.828563\n",
            "{SqueezeNet} Train Epoch: 11 [23552/37814 (62%)]\tLoss: 1.838271\n",
            "{SqueezeNet} Train Epoch: 11 [24064/37814 (64%)]\tLoss: 1.861060\n",
            "{SqueezeNet} Train Epoch: 11 [24576/37814 (65%)]\tLoss: 1.821763\n",
            "{SqueezeNet} Train Epoch: 11 [25088/37814 (66%)]\tLoss: 1.853984\n",
            "{SqueezeNet} Train Epoch: 11 [25600/37814 (68%)]\tLoss: 1.816027\n",
            "{SqueezeNet} Train Epoch: 11 [26112/37814 (69%)]\tLoss: 1.899515\n",
            "{SqueezeNet} Train Epoch: 11 [26624/37814 (70%)]\tLoss: 1.843985\n",
            "{SqueezeNet} Train Epoch: 11 [27136/37814 (72%)]\tLoss: 1.866802\n",
            "{SqueezeNet} Train Epoch: 11 [27648/37814 (73%)]\tLoss: 1.836128\n",
            "{SqueezeNet} Train Epoch: 11 [28160/37814 (74%)]\tLoss: 1.842619\n",
            "{SqueezeNet} Train Epoch: 11 [28672/37814 (76%)]\tLoss: 1.821871\n",
            "{SqueezeNet} Train Epoch: 11 [29184/37814 (77%)]\tLoss: 1.905555\n",
            "{SqueezeNet} Train Epoch: 11 [29696/37814 (78%)]\tLoss: 1.820810\n",
            "{SqueezeNet} Train Epoch: 11 [30208/37814 (80%)]\tLoss: 1.848283\n",
            "{SqueezeNet} Train Epoch: 11 [30720/37814 (81%)]\tLoss: 1.877857\n",
            "{SqueezeNet} Train Epoch: 11 [31232/37814 (82%)]\tLoss: 1.804998\n",
            "{SqueezeNet} Train Epoch: 11 [31744/37814 (84%)]\tLoss: 1.815397\n",
            "{SqueezeNet} Train Epoch: 11 [32256/37814 (85%)]\tLoss: 1.901104\n",
            "{SqueezeNet} Train Epoch: 11 [32768/37814 (86%)]\tLoss: 1.829697\n",
            "{SqueezeNet} Train Epoch: 11 [33280/37814 (88%)]\tLoss: 1.834159\n",
            "{SqueezeNet} Train Epoch: 11 [33792/37814 (89%)]\tLoss: 1.903085\n",
            "{SqueezeNet} Train Epoch: 11 [34304/37814 (91%)]\tLoss: 1.787703\n",
            "{SqueezeNet} Train Epoch: 11 [34816/37814 (92%)]\tLoss: 1.812166\n",
            "{SqueezeNet} Train Epoch: 11 [35328/37814 (93%)]\tLoss: 1.855009\n",
            "{SqueezeNet} Train Epoch: 11 [35840/37814 (95%)]\tLoss: 1.883079\n",
            "{SqueezeNet} Train Epoch: 11 [36352/37814 (96%)]\tLoss: 1.861996\n",
            "{SqueezeNet} Train Epoch: 11 [36864/37814 (97%)]\tLoss: 1.790209\n",
            "{SqueezeNet} Train Epoch: 11 [31974/37814 (99%)]\tLoss: 1.807947\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8336, Accuracy: 1561/5000 (31%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.467092275619507 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f96159da-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d870ba7c-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_471bccc122"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f9636306-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_8c7444ba49"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f963ad34-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_2af9295918"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f963f406-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f963ad34-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c1ab67faf1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f98a37ec-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f9636306-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_23610adb10"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f98bc8fa-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_df9a075a39"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f98c06d0-60e3-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_5425d747b3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f98c4640-60e3-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f98c06d0-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_667107e38e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 12 [0/37814 (0%)]\tLoss: 2.006081\n",
            "{AlexNet} Train Epoch: 12 [512/37814 (1%)]\tLoss: 1.966783\n",
            "{AlexNet} Train Epoch: 12 [1024/37814 (3%)]\tLoss: 2.015688\n",
            "{AlexNet} Train Epoch: 12 [1536/37814 (4%)]\tLoss: 1.975673\n",
            "{AlexNet} Train Epoch: 12 [2048/37814 (5%)]\tLoss: 1.977102\n",
            "{AlexNet} Train Epoch: 12 [2560/37814 (7%)]\tLoss: 2.014322\n",
            "{AlexNet} Train Epoch: 12 [3072/37814 (8%)]\tLoss: 2.007202\n",
            "{AlexNet} Train Epoch: 12 [3584/37814 (9%)]\tLoss: 1.998213\n",
            "{AlexNet} Train Epoch: 12 [4096/37814 (11%)]\tLoss: 1.993670\n",
            "{AlexNet} Train Epoch: 12 [4608/37814 (12%)]\tLoss: 2.033361\n",
            "{AlexNet} Train Epoch: 12 [5120/37814 (14%)]\tLoss: 2.011629\n",
            "{AlexNet} Train Epoch: 12 [5632/37814 (15%)]\tLoss: 2.029284\n",
            "{AlexNet} Train Epoch: 12 [6144/37814 (16%)]\tLoss: 1.964829\n",
            "{AlexNet} Train Epoch: 12 [6656/37814 (18%)]\tLoss: 2.025878\n",
            "{AlexNet} Train Epoch: 12 [7168/37814 (19%)]\tLoss: 2.012480\n",
            "{AlexNet} Train Epoch: 12 [7680/37814 (20%)]\tLoss: 2.007321\n",
            "{AlexNet} Train Epoch: 12 [8192/37814 (22%)]\tLoss: 1.992054\n",
            "{AlexNet} Train Epoch: 12 [8704/37814 (23%)]\tLoss: 1.959387\n",
            "{AlexNet} Train Epoch: 12 [9216/37814 (24%)]\tLoss: 1.973966\n",
            "{AlexNet} Train Epoch: 12 [9728/37814 (26%)]\tLoss: 2.010037\n",
            "{AlexNet} Train Epoch: 12 [10240/37814 (27%)]\tLoss: 1.994978\n",
            "{AlexNet} Train Epoch: 12 [10752/37814 (28%)]\tLoss: 2.028400\n",
            "{AlexNet} Train Epoch: 12 [11264/37814 (30%)]\tLoss: 2.003850\n",
            "{AlexNet} Train Epoch: 12 [11776/37814 (31%)]\tLoss: 2.022279\n",
            "{AlexNet} Train Epoch: 12 [12288/37814 (32%)]\tLoss: 1.995520\n",
            "{AlexNet} Train Epoch: 12 [12800/37814 (34%)]\tLoss: 1.987424\n",
            "{AlexNet} Train Epoch: 12 [13312/37814 (35%)]\tLoss: 2.013344\n",
            "{AlexNet} Train Epoch: 12 [13824/37814 (36%)]\tLoss: 1.982035\n",
            "{AlexNet} Train Epoch: 12 [14336/37814 (38%)]\tLoss: 2.044675\n",
            "{AlexNet} Train Epoch: 12 [14848/37814 (39%)]\tLoss: 2.043449\n",
            "{AlexNet} Train Epoch: 12 [15360/37814 (41%)]\tLoss: 1.988522\n",
            "{AlexNet} Train Epoch: 12 [15872/37814 (42%)]\tLoss: 2.005752\n",
            "{AlexNet} Train Epoch: 12 [16384/37814 (43%)]\tLoss: 1.949419\n",
            "{AlexNet} Train Epoch: 12 [16896/37814 (45%)]\tLoss: 2.021210\n",
            "{AlexNet} Train Epoch: 12 [17408/37814 (46%)]\tLoss: 1.994896\n",
            "{AlexNet} Train Epoch: 12 [17920/37814 (47%)]\tLoss: 2.010008\n",
            "{AlexNet} Train Epoch: 12 [18432/37814 (49%)]\tLoss: 1.991656\n",
            "{AlexNet} Train Epoch: 12 [18944/37814 (50%)]\tLoss: 1.993805\n",
            "{AlexNet} Train Epoch: 12 [19456/37814 (51%)]\tLoss: 2.021825\n",
            "{AlexNet} Train Epoch: 12 [19968/37814 (53%)]\tLoss: 2.024005\n",
            "{AlexNet} Train Epoch: 12 [20480/37814 (54%)]\tLoss: 2.040669\n",
            "{AlexNet} Train Epoch: 12 [20992/37814 (55%)]\tLoss: 2.009102\n",
            "{AlexNet} Train Epoch: 12 [21504/37814 (57%)]\tLoss: 2.036990\n",
            "{AlexNet} Train Epoch: 12 [22016/37814 (58%)]\tLoss: 2.001528\n",
            "{AlexNet} Train Epoch: 12 [22528/37814 (59%)]\tLoss: 1.987337\n",
            "{AlexNet} Train Epoch: 12 [23040/37814 (61%)]\tLoss: 2.014889\n",
            "{AlexNet} Train Epoch: 12 [23552/37814 (62%)]\tLoss: 2.014683\n",
            "{AlexNet} Train Epoch: 12 [24064/37814 (64%)]\tLoss: 2.011395\n",
            "{AlexNet} Train Epoch: 12 [24576/37814 (65%)]\tLoss: 1.975705\n",
            "{AlexNet} Train Epoch: 12 [25088/37814 (66%)]\tLoss: 2.069302\n",
            "{AlexNet} Train Epoch: 12 [25600/37814 (68%)]\tLoss: 2.039222\n",
            "{AlexNet} Train Epoch: 12 [26112/37814 (69%)]\tLoss: 2.021293\n",
            "{AlexNet} Train Epoch: 12 [26624/37814 (70%)]\tLoss: 2.048405\n",
            "{AlexNet} Train Epoch: 12 [27136/37814 (72%)]\tLoss: 1.977910\n",
            "{AlexNet} Train Epoch: 12 [27648/37814 (73%)]\tLoss: 1.979110\n",
            "{AlexNet} Train Epoch: 12 [28160/37814 (74%)]\tLoss: 2.002748\n",
            "{AlexNet} Train Epoch: 12 [28672/37814 (76%)]\tLoss: 1.993237\n",
            "{AlexNet} Train Epoch: 12 [29184/37814 (77%)]\tLoss: 1.958853\n",
            "{AlexNet} Train Epoch: 12 [29696/37814 (78%)]\tLoss: 2.006807\n",
            "{AlexNet} Train Epoch: 12 [30208/37814 (80%)]\tLoss: 1.997350\n",
            "{AlexNet} Train Epoch: 12 [30720/37814 (81%)]\tLoss: 2.003511\n",
            "{AlexNet} Train Epoch: 12 [31232/37814 (82%)]\tLoss: 1.967957\n",
            "{AlexNet} Train Epoch: 12 [31744/37814 (84%)]\tLoss: 2.001508\n",
            "{AlexNet} Train Epoch: 12 [32256/37814 (85%)]\tLoss: 1.960909\n",
            "{AlexNet} Train Epoch: 12 [32768/37814 (86%)]\tLoss: 2.011543\n",
            "{AlexNet} Train Epoch: 12 [33280/37814 (88%)]\tLoss: 2.029256\n",
            "{AlexNet} Train Epoch: 12 [33792/37814 (89%)]\tLoss: 2.025783\n",
            "{AlexNet} Train Epoch: 12 [34304/37814 (91%)]\tLoss: 2.052256\n",
            "{AlexNet} Train Epoch: 12 [34816/37814 (92%)]\tLoss: 2.000808\n",
            "{AlexNet} Train Epoch: 12 [35328/37814 (93%)]\tLoss: 2.009774\n",
            "{AlexNet} Train Epoch: 12 [35840/37814 (95%)]\tLoss: 1.962303\n",
            "{AlexNet} Train Epoch: 12 [36352/37814 (96%)]\tLoss: 2.023065\n",
            "{AlexNet} Train Epoch: 12 [36864/37814 (97%)]\tLoss: 1.983247\n",
            "{AlexNet} Train Epoch: 12 [31974/37814 (99%)]\tLoss: 2.061269\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9974, Accuracy: 1013/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.711469888687134 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 12 [0/37814 (0%)]\tLoss: 1.856853\n",
            "{SqueezeNet} Train Epoch: 12 [512/37814 (1%)]\tLoss: 1.839856\n",
            "{SqueezeNet} Train Epoch: 12 [1024/37814 (3%)]\tLoss: 1.850537\n",
            "{SqueezeNet} Train Epoch: 12 [1536/37814 (4%)]\tLoss: 1.787815\n",
            "{SqueezeNet} Train Epoch: 12 [2048/37814 (5%)]\tLoss: 1.864818\n",
            "{SqueezeNet} Train Epoch: 12 [2560/37814 (7%)]\tLoss: 1.806209\n",
            "{SqueezeNet} Train Epoch: 12 [3072/37814 (8%)]\tLoss: 1.870702\n",
            "{SqueezeNet} Train Epoch: 12 [3584/37814 (9%)]\tLoss: 1.880431\n",
            "{SqueezeNet} Train Epoch: 12 [4096/37814 (11%)]\tLoss: 1.871750\n",
            "{SqueezeNet} Train Epoch: 12 [4608/37814 (12%)]\tLoss: 1.847747\n",
            "{SqueezeNet} Train Epoch: 12 [5120/37814 (14%)]\tLoss: 1.853737\n",
            "{SqueezeNet} Train Epoch: 12 [5632/37814 (15%)]\tLoss: 1.872461\n",
            "{SqueezeNet} Train Epoch: 12 [6144/37814 (16%)]\tLoss: 1.837062\n",
            "{SqueezeNet} Train Epoch: 12 [6656/37814 (18%)]\tLoss: 1.948203\n",
            "{SqueezeNet} Train Epoch: 12 [7168/37814 (19%)]\tLoss: 1.878530\n",
            "{SqueezeNet} Train Epoch: 12 [7680/37814 (20%)]\tLoss: 1.817583\n",
            "{SqueezeNet} Train Epoch: 12 [8192/37814 (22%)]\tLoss: 1.832647\n",
            "{SqueezeNet} Train Epoch: 12 [8704/37814 (23%)]\tLoss: 1.874579\n",
            "{SqueezeNet} Train Epoch: 12 [9216/37814 (24%)]\tLoss: 1.866256\n",
            "{SqueezeNet} Train Epoch: 12 [9728/37814 (26%)]\tLoss: 1.846413\n",
            "{SqueezeNet} Train Epoch: 12 [10240/37814 (27%)]\tLoss: 1.933085\n",
            "{SqueezeNet} Train Epoch: 12 [10752/37814 (28%)]\tLoss: 1.839993\n",
            "{SqueezeNet} Train Epoch: 12 [11264/37814 (30%)]\tLoss: 1.833284\n",
            "{SqueezeNet} Train Epoch: 12 [11776/37814 (31%)]\tLoss: 1.865691\n",
            "{SqueezeNet} Train Epoch: 12 [12288/37814 (32%)]\tLoss: 1.841262\n",
            "{SqueezeNet} Train Epoch: 12 [12800/37814 (34%)]\tLoss: 1.856382\n",
            "{SqueezeNet} Train Epoch: 12 [13312/37814 (35%)]\tLoss: 1.829269\n",
            "{SqueezeNet} Train Epoch: 12 [13824/37814 (36%)]\tLoss: 1.881683\n",
            "{SqueezeNet} Train Epoch: 12 [14336/37814 (38%)]\tLoss: 1.822826\n",
            "{SqueezeNet} Train Epoch: 12 [14848/37814 (39%)]\tLoss: 1.903281\n",
            "{SqueezeNet} Train Epoch: 12 [15360/37814 (41%)]\tLoss: 1.847298\n",
            "{SqueezeNet} Train Epoch: 12 [15872/37814 (42%)]\tLoss: 1.869955\n",
            "{SqueezeNet} Train Epoch: 12 [16384/37814 (43%)]\tLoss: 1.820126\n",
            "{SqueezeNet} Train Epoch: 12 [16896/37814 (45%)]\tLoss: 1.850391\n",
            "{SqueezeNet} Train Epoch: 12 [17408/37814 (46%)]\tLoss: 1.861979\n",
            "{SqueezeNet} Train Epoch: 12 [17920/37814 (47%)]\tLoss: 1.800605\n",
            "{SqueezeNet} Train Epoch: 12 [18432/37814 (49%)]\tLoss: 1.852254\n",
            "{SqueezeNet} Train Epoch: 12 [18944/37814 (50%)]\tLoss: 1.821184\n",
            "{SqueezeNet} Train Epoch: 12 [19456/37814 (51%)]\tLoss: 1.839338\n",
            "{SqueezeNet} Train Epoch: 12 [19968/37814 (53%)]\tLoss: 1.862416\n",
            "{SqueezeNet} Train Epoch: 12 [20480/37814 (54%)]\tLoss: 1.813554\n",
            "{SqueezeNet} Train Epoch: 12 [20992/37814 (55%)]\tLoss: 1.827905\n",
            "{SqueezeNet} Train Epoch: 12 [21504/37814 (57%)]\tLoss: 1.829065\n",
            "{SqueezeNet} Train Epoch: 12 [22016/37814 (58%)]\tLoss: 1.858155\n",
            "{SqueezeNet} Train Epoch: 12 [22528/37814 (59%)]\tLoss: 1.872599\n",
            "{SqueezeNet} Train Epoch: 12 [23040/37814 (61%)]\tLoss: 1.850973\n",
            "{SqueezeNet} Train Epoch: 12 [23552/37814 (62%)]\tLoss: 1.811449\n",
            "{SqueezeNet} Train Epoch: 12 [24064/37814 (64%)]\tLoss: 1.887363\n",
            "{SqueezeNet} Train Epoch: 12 [24576/37814 (65%)]\tLoss: 1.905407\n",
            "{SqueezeNet} Train Epoch: 12 [25088/37814 (66%)]\tLoss: 1.876858\n",
            "{SqueezeNet} Train Epoch: 12 [25600/37814 (68%)]\tLoss: 1.897685\n",
            "{SqueezeNet} Train Epoch: 12 [26112/37814 (69%)]\tLoss: 1.852655\n",
            "{SqueezeNet} Train Epoch: 12 [26624/37814 (70%)]\tLoss: 1.895151\n",
            "{SqueezeNet} Train Epoch: 12 [27136/37814 (72%)]\tLoss: 1.888640\n",
            "{SqueezeNet} Train Epoch: 12 [27648/37814 (73%)]\tLoss: 1.918255\n",
            "{SqueezeNet} Train Epoch: 12 [28160/37814 (74%)]\tLoss: 1.809165\n",
            "{SqueezeNet} Train Epoch: 12 [28672/37814 (76%)]\tLoss: 1.884319\n",
            "{SqueezeNet} Train Epoch: 12 [29184/37814 (77%)]\tLoss: 1.869421\n",
            "{SqueezeNet} Train Epoch: 12 [29696/37814 (78%)]\tLoss: 1.807971\n",
            "{SqueezeNet} Train Epoch: 12 [30208/37814 (80%)]\tLoss: 1.829885\n",
            "{SqueezeNet} Train Epoch: 12 [30720/37814 (81%)]\tLoss: 1.842127\n",
            "{SqueezeNet} Train Epoch: 12 [31232/37814 (82%)]\tLoss: 1.800965\n",
            "{SqueezeNet} Train Epoch: 12 [31744/37814 (84%)]\tLoss: 1.893207\n",
            "{SqueezeNet} Train Epoch: 12 [32256/37814 (85%)]\tLoss: 1.833033\n",
            "{SqueezeNet} Train Epoch: 12 [32768/37814 (86%)]\tLoss: 1.892712\n",
            "{SqueezeNet} Train Epoch: 12 [33280/37814 (88%)]\tLoss: 1.858886\n",
            "{SqueezeNet} Train Epoch: 12 [33792/37814 (89%)]\tLoss: 1.774730\n",
            "{SqueezeNet} Train Epoch: 12 [34304/37814 (91%)]\tLoss: 1.807783\n",
            "{SqueezeNet} Train Epoch: 12 [34816/37814 (92%)]\tLoss: 1.832778\n",
            "{SqueezeNet} Train Epoch: 12 [35328/37814 (93%)]\tLoss: 1.881620\n",
            "{SqueezeNet} Train Epoch: 12 [35840/37814 (95%)]\tLoss: 1.849982\n",
            "{SqueezeNet} Train Epoch: 12 [36352/37814 (96%)]\tLoss: 1.863342\n",
            "{SqueezeNet} Train Epoch: 12 [36864/37814 (97%)]\tLoss: 1.861125\n",
            "{SqueezeNet} Train Epoch: 12 [31974/37814 (99%)]\tLoss: 1.818205\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8419, Accuracy: 1518/5000 (30%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.021462440490723 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b5dedd2-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f98bc8fa-60e3-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2fb6bf1a60"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b5fbfc2-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_24e0548212"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b60050e-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_2ba2805dff"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b604bea-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b60050e-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_09058f7e24"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b851e02-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b5fbfc2-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0313a9b93a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b8742a4-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_f723b1c0c2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b87c7ba-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_486428d081"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b8841c2-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b87c7ba-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e30ae5ef14"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 13 [0/37814 (0%)]\tLoss: 1.949648\n",
            "{AlexNet} Train Epoch: 13 [512/37814 (1%)]\tLoss: 1.967195\n",
            "{AlexNet} Train Epoch: 13 [1024/37814 (3%)]\tLoss: 1.991705\n",
            "{AlexNet} Train Epoch: 13 [1536/37814 (4%)]\tLoss: 1.965861\n",
            "{AlexNet} Train Epoch: 13 [2048/37814 (5%)]\tLoss: 2.009665\n",
            "{AlexNet} Train Epoch: 13 [2560/37814 (7%)]\tLoss: 2.011855\n",
            "{AlexNet} Train Epoch: 13 [3072/37814 (8%)]\tLoss: 2.060896\n",
            "{AlexNet} Train Epoch: 13 [3584/37814 (9%)]\tLoss: 2.009626\n",
            "{AlexNet} Train Epoch: 13 [4096/37814 (11%)]\tLoss: 1.970174\n",
            "{AlexNet} Train Epoch: 13 [4608/37814 (12%)]\tLoss: 2.027703\n",
            "{AlexNet} Train Epoch: 13 [5120/37814 (14%)]\tLoss: 2.014698\n",
            "{AlexNet} Train Epoch: 13 [5632/37814 (15%)]\tLoss: 1.961395\n",
            "{AlexNet} Train Epoch: 13 [6144/37814 (16%)]\tLoss: 2.027871\n",
            "{AlexNet} Train Epoch: 13 [6656/37814 (18%)]\tLoss: 2.005824\n",
            "{AlexNet} Train Epoch: 13 [7168/37814 (19%)]\tLoss: 1.994866\n",
            "{AlexNet} Train Epoch: 13 [7680/37814 (20%)]\tLoss: 2.049289\n",
            "{AlexNet} Train Epoch: 13 [8192/37814 (22%)]\tLoss: 2.024082\n",
            "{AlexNet} Train Epoch: 13 [8704/37814 (23%)]\tLoss: 1.955672\n",
            "{AlexNet} Train Epoch: 13 [9216/37814 (24%)]\tLoss: 1.997648\n",
            "{AlexNet} Train Epoch: 13 [9728/37814 (26%)]\tLoss: 2.013377\n",
            "{AlexNet} Train Epoch: 13 [10240/37814 (27%)]\tLoss: 1.979429\n",
            "{AlexNet} Train Epoch: 13 [10752/37814 (28%)]\tLoss: 2.036698\n",
            "{AlexNet} Train Epoch: 13 [11264/37814 (30%)]\tLoss: 2.011381\n",
            "{AlexNet} Train Epoch: 13 [11776/37814 (31%)]\tLoss: 2.001434\n",
            "{AlexNet} Train Epoch: 13 [12288/37814 (32%)]\tLoss: 1.977743\n",
            "{AlexNet} Train Epoch: 13 [12800/37814 (34%)]\tLoss: 2.023879\n",
            "{AlexNet} Train Epoch: 13 [13312/37814 (35%)]\tLoss: 1.983814\n",
            "{AlexNet} Train Epoch: 13 [13824/37814 (36%)]\tLoss: 1.989590\n",
            "{AlexNet} Train Epoch: 13 [14336/37814 (38%)]\tLoss: 1.963887\n",
            "{AlexNet} Train Epoch: 13 [14848/37814 (39%)]\tLoss: 1.954889\n",
            "{AlexNet} Train Epoch: 13 [15360/37814 (41%)]\tLoss: 2.041611\n",
            "{AlexNet} Train Epoch: 13 [15872/37814 (42%)]\tLoss: 1.949257\n",
            "{AlexNet} Train Epoch: 13 [16384/37814 (43%)]\tLoss: 1.975537\n",
            "{AlexNet} Train Epoch: 13 [16896/37814 (45%)]\tLoss: 1.991523\n",
            "{AlexNet} Train Epoch: 13 [17408/37814 (46%)]\tLoss: 1.968599\n",
            "{AlexNet} Train Epoch: 13 [17920/37814 (47%)]\tLoss: 2.028326\n",
            "{AlexNet} Train Epoch: 13 [18432/37814 (49%)]\tLoss: 2.019424\n",
            "{AlexNet} Train Epoch: 13 [18944/37814 (50%)]\tLoss: 2.042162\n",
            "{AlexNet} Train Epoch: 13 [19456/37814 (51%)]\tLoss: 1.966102\n",
            "{AlexNet} Train Epoch: 13 [19968/37814 (53%)]\tLoss: 2.004941\n",
            "{AlexNet} Train Epoch: 13 [20480/37814 (54%)]\tLoss: 1.968948\n",
            "{AlexNet} Train Epoch: 13 [20992/37814 (55%)]\tLoss: 1.985313\n",
            "{AlexNet} Train Epoch: 13 [21504/37814 (57%)]\tLoss: 1.964782\n",
            "{AlexNet} Train Epoch: 13 [22016/37814 (58%)]\tLoss: 2.021461\n",
            "{AlexNet} Train Epoch: 13 [22528/37814 (59%)]\tLoss: 2.012175\n",
            "{AlexNet} Train Epoch: 13 [23040/37814 (61%)]\tLoss: 1.957652\n",
            "{AlexNet} Train Epoch: 13 [23552/37814 (62%)]\tLoss: 2.013666\n",
            "{AlexNet} Train Epoch: 13 [24064/37814 (64%)]\tLoss: 1.968251\n",
            "{AlexNet} Train Epoch: 13 [24576/37814 (65%)]\tLoss: 2.026327\n",
            "{AlexNet} Train Epoch: 13 [25088/37814 (66%)]\tLoss: 2.001678\n",
            "{AlexNet} Train Epoch: 13 [25600/37814 (68%)]\tLoss: 2.016524\n",
            "{AlexNet} Train Epoch: 13 [26112/37814 (69%)]\tLoss: 2.001119\n",
            "{AlexNet} Train Epoch: 13 [26624/37814 (70%)]\tLoss: 1.964319\n",
            "{AlexNet} Train Epoch: 13 [27136/37814 (72%)]\tLoss: 2.013053\n",
            "{AlexNet} Train Epoch: 13 [27648/37814 (73%)]\tLoss: 1.953247\n",
            "{AlexNet} Train Epoch: 13 [28160/37814 (74%)]\tLoss: 2.012707\n",
            "{AlexNet} Train Epoch: 13 [28672/37814 (76%)]\tLoss: 2.033505\n",
            "{AlexNet} Train Epoch: 13 [29184/37814 (77%)]\tLoss: 2.044651\n",
            "{AlexNet} Train Epoch: 13 [29696/37814 (78%)]\tLoss: 1.982754\n",
            "{AlexNet} Train Epoch: 13 [30208/37814 (80%)]\tLoss: 2.023834\n",
            "{AlexNet} Train Epoch: 13 [30720/37814 (81%)]\tLoss: 1.974328\n",
            "{AlexNet} Train Epoch: 13 [31232/37814 (82%)]\tLoss: 1.964227\n",
            "{AlexNet} Train Epoch: 13 [31744/37814 (84%)]\tLoss: 2.009247\n",
            "{AlexNet} Train Epoch: 13 [32256/37814 (85%)]\tLoss: 1.998131\n",
            "{AlexNet} Train Epoch: 13 [32768/37814 (86%)]\tLoss: 2.015230\n",
            "{AlexNet} Train Epoch: 13 [33280/37814 (88%)]\tLoss: 2.021512\n",
            "{AlexNet} Train Epoch: 13 [33792/37814 (89%)]\tLoss: 1.974773\n",
            "{AlexNet} Train Epoch: 13 [34304/37814 (91%)]\tLoss: 2.010288\n",
            "{AlexNet} Train Epoch: 13 [34816/37814 (92%)]\tLoss: 2.025002\n",
            "{AlexNet} Train Epoch: 13 [35328/37814 (93%)]\tLoss: 1.992339\n",
            "{AlexNet} Train Epoch: 13 [35840/37814 (95%)]\tLoss: 1.962773\n",
            "{AlexNet} Train Epoch: 13 [36352/37814 (96%)]\tLoss: 2.055332\n",
            "{AlexNet} Train Epoch: 13 [36864/37814 (97%)]\tLoss: 2.020830\n",
            "{AlexNet} Train Epoch: 13 [31974/37814 (99%)]\tLoss: 2.001009\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9941, Accuracy: 1025/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.98669195175171 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 13 [0/37814 (0%)]\tLoss: 1.851636\n",
            "{SqueezeNet} Train Epoch: 13 [512/37814 (1%)]\tLoss: 1.877159\n",
            "{SqueezeNet} Train Epoch: 13 [1024/37814 (3%)]\tLoss: 1.825104\n",
            "{SqueezeNet} Train Epoch: 13 [1536/37814 (4%)]\tLoss: 1.854247\n",
            "{SqueezeNet} Train Epoch: 13 [2048/37814 (5%)]\tLoss: 1.840252\n",
            "{SqueezeNet} Train Epoch: 13 [2560/37814 (7%)]\tLoss: 1.851091\n",
            "{SqueezeNet} Train Epoch: 13 [3072/37814 (8%)]\tLoss: 1.847652\n",
            "{SqueezeNet} Train Epoch: 13 [3584/37814 (9%)]\tLoss: 1.886266\n",
            "{SqueezeNet} Train Epoch: 13 [4096/37814 (11%)]\tLoss: 1.818755\n",
            "{SqueezeNet} Train Epoch: 13 [4608/37814 (12%)]\tLoss: 1.851765\n",
            "{SqueezeNet} Train Epoch: 13 [5120/37814 (14%)]\tLoss: 1.839128\n",
            "{SqueezeNet} Train Epoch: 13 [5632/37814 (15%)]\tLoss: 1.825915\n",
            "{SqueezeNet} Train Epoch: 13 [6144/37814 (16%)]\tLoss: 1.873284\n",
            "{SqueezeNet} Train Epoch: 13 [6656/37814 (18%)]\tLoss: 1.857231\n",
            "{SqueezeNet} Train Epoch: 13 [7168/37814 (19%)]\tLoss: 1.796345\n",
            "{SqueezeNet} Train Epoch: 13 [7680/37814 (20%)]\tLoss: 1.843641\n",
            "{SqueezeNet} Train Epoch: 13 [8192/37814 (22%)]\tLoss: 1.892431\n",
            "{SqueezeNet} Train Epoch: 13 [8704/37814 (23%)]\tLoss: 1.914749\n",
            "{SqueezeNet} Train Epoch: 13 [9216/37814 (24%)]\tLoss: 1.834623\n",
            "{SqueezeNet} Train Epoch: 13 [9728/37814 (26%)]\tLoss: 1.876915\n",
            "{SqueezeNet} Train Epoch: 13 [10240/37814 (27%)]\tLoss: 1.828845\n",
            "{SqueezeNet} Train Epoch: 13 [10752/37814 (28%)]\tLoss: 1.862494\n",
            "{SqueezeNet} Train Epoch: 13 [11264/37814 (30%)]\tLoss: 1.782247\n",
            "{SqueezeNet} Train Epoch: 13 [11776/37814 (31%)]\tLoss: 1.813788\n",
            "{SqueezeNet} Train Epoch: 13 [12288/37814 (32%)]\tLoss: 1.888089\n",
            "{SqueezeNet} Train Epoch: 13 [12800/37814 (34%)]\tLoss: 1.813109\n",
            "{SqueezeNet} Train Epoch: 13 [13312/37814 (35%)]\tLoss: 1.966980\n",
            "{SqueezeNet} Train Epoch: 13 [13824/37814 (36%)]\tLoss: 1.829535\n",
            "{SqueezeNet} Train Epoch: 13 [14336/37814 (38%)]\tLoss: 1.858684\n",
            "{SqueezeNet} Train Epoch: 13 [14848/37814 (39%)]\tLoss: 1.843244\n",
            "{SqueezeNet} Train Epoch: 13 [15360/37814 (41%)]\tLoss: 1.844606\n",
            "{SqueezeNet} Train Epoch: 13 [15872/37814 (42%)]\tLoss: 1.872344\n",
            "{SqueezeNet} Train Epoch: 13 [16384/37814 (43%)]\tLoss: 1.824242\n",
            "{SqueezeNet} Train Epoch: 13 [16896/37814 (45%)]\tLoss: 1.829594\n",
            "{SqueezeNet} Train Epoch: 13 [17408/37814 (46%)]\tLoss: 1.898849\n",
            "{SqueezeNet} Train Epoch: 13 [17920/37814 (47%)]\tLoss: 1.791072\n",
            "{SqueezeNet} Train Epoch: 13 [18432/37814 (49%)]\tLoss: 1.839363\n",
            "{SqueezeNet} Train Epoch: 13 [18944/37814 (50%)]\tLoss: 1.841911\n",
            "{SqueezeNet} Train Epoch: 13 [19456/37814 (51%)]\tLoss: 1.824209\n",
            "{SqueezeNet} Train Epoch: 13 [19968/37814 (53%)]\tLoss: 1.899662\n",
            "{SqueezeNet} Train Epoch: 13 [20480/37814 (54%)]\tLoss: 1.841032\n",
            "{SqueezeNet} Train Epoch: 13 [20992/37814 (55%)]\tLoss: 1.817342\n",
            "{SqueezeNet} Train Epoch: 13 [21504/37814 (57%)]\tLoss: 1.826947\n",
            "{SqueezeNet} Train Epoch: 13 [22016/37814 (58%)]\tLoss: 1.803880\n",
            "{SqueezeNet} Train Epoch: 13 [22528/37814 (59%)]\tLoss: 1.884408\n",
            "{SqueezeNet} Train Epoch: 13 [23040/37814 (61%)]\tLoss: 1.888396\n",
            "{SqueezeNet} Train Epoch: 13 [23552/37814 (62%)]\tLoss: 1.822805\n",
            "{SqueezeNet} Train Epoch: 13 [24064/37814 (64%)]\tLoss: 1.826283\n",
            "{SqueezeNet} Train Epoch: 13 [24576/37814 (65%)]\tLoss: 1.879230\n",
            "{SqueezeNet} Train Epoch: 13 [25088/37814 (66%)]\tLoss: 1.807769\n",
            "{SqueezeNet} Train Epoch: 13 [25600/37814 (68%)]\tLoss: 1.871050\n",
            "{SqueezeNet} Train Epoch: 13 [26112/37814 (69%)]\tLoss: 1.794325\n",
            "{SqueezeNet} Train Epoch: 13 [26624/37814 (70%)]\tLoss: 1.780207\n",
            "{SqueezeNet} Train Epoch: 13 [27136/37814 (72%)]\tLoss: 1.862430\n",
            "{SqueezeNet} Train Epoch: 13 [27648/37814 (73%)]\tLoss: 1.880721\n",
            "{SqueezeNet} Train Epoch: 13 [28160/37814 (74%)]\tLoss: 1.834521\n",
            "{SqueezeNet} Train Epoch: 13 [28672/37814 (76%)]\tLoss: 1.846277\n",
            "{SqueezeNet} Train Epoch: 13 [29184/37814 (77%)]\tLoss: 1.881833\n",
            "{SqueezeNet} Train Epoch: 13 [29696/37814 (78%)]\tLoss: 1.840445\n",
            "{SqueezeNet} Train Epoch: 13 [30208/37814 (80%)]\tLoss: 1.828262\n",
            "{SqueezeNet} Train Epoch: 13 [30720/37814 (81%)]\tLoss: 1.857400\n",
            "{SqueezeNet} Train Epoch: 13 [31232/37814 (82%)]\tLoss: 1.832503\n",
            "{SqueezeNet} Train Epoch: 13 [31744/37814 (84%)]\tLoss: 1.875903\n",
            "{SqueezeNet} Train Epoch: 13 [32256/37814 (85%)]\tLoss: 1.861327\n",
            "{SqueezeNet} Train Epoch: 13 [32768/37814 (86%)]\tLoss: 1.833925\n",
            "{SqueezeNet} Train Epoch: 13 [33280/37814 (88%)]\tLoss: 1.862519\n",
            "{SqueezeNet} Train Epoch: 13 [33792/37814 (89%)]\tLoss: 1.872175\n",
            "{SqueezeNet} Train Epoch: 13 [34304/37814 (91%)]\tLoss: 1.810895\n",
            "{SqueezeNet} Train Epoch: 13 [34816/37814 (92%)]\tLoss: 1.862338\n",
            "{SqueezeNet} Train Epoch: 13 [35328/37814 (93%)]\tLoss: 1.845675\n",
            "{SqueezeNet} Train Epoch: 13 [35840/37814 (95%)]\tLoss: 1.844717\n",
            "{SqueezeNet} Train Epoch: 13 [36352/37814 (96%)]\tLoss: 1.851812\n",
            "{SqueezeNet} Train Epoch: 13 [36864/37814 (97%)]\tLoss: 1.893048\n",
            "{SqueezeNet} Train Epoch: 13 [31974/37814 (99%)]\tLoss: 1.847337\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8399, Accuracy: 1547/5000 (31%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.06278896331787 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3cf2a276-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b8742a4-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_acb50b2c53"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3cf4ea36-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_91ef84f073"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3cf55a3e-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_c7d54a8ca5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3cf5de50-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"3cf55a3e-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_22d0595a2c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3d19db34-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"3cf4ea36-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2199d3a346"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3d1ac882-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_4e8ff18759"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3d1af730-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_73919efc42"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3d1b27d2-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"3d1af730-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e6d6896f85"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 14 [0/37814 (0%)]\tLoss: 1.967244\n",
            "{AlexNet} Train Epoch: 14 [512/37814 (1%)]\tLoss: 1.985071\n",
            "{AlexNet} Train Epoch: 14 [1024/37814 (3%)]\tLoss: 2.019030\n",
            "{AlexNet} Train Epoch: 14 [1536/37814 (4%)]\tLoss: 2.030638\n",
            "{AlexNet} Train Epoch: 14 [2048/37814 (5%)]\tLoss: 1.976912\n",
            "{AlexNet} Train Epoch: 14 [2560/37814 (7%)]\tLoss: 2.041752\n",
            "{AlexNet} Train Epoch: 14 [3072/37814 (8%)]\tLoss: 1.974797\n",
            "{AlexNet} Train Epoch: 14 [3584/37814 (9%)]\tLoss: 2.016559\n",
            "{AlexNet} Train Epoch: 14 [4096/37814 (11%)]\tLoss: 1.999094\n",
            "{AlexNet} Train Epoch: 14 [4608/37814 (12%)]\tLoss: 1.948545\n",
            "{AlexNet} Train Epoch: 14 [5120/37814 (14%)]\tLoss: 1.997698\n",
            "{AlexNet} Train Epoch: 14 [5632/37814 (15%)]\tLoss: 1.998784\n",
            "{AlexNet} Train Epoch: 14 [6144/37814 (16%)]\tLoss: 1.941173\n",
            "{AlexNet} Train Epoch: 14 [6656/37814 (18%)]\tLoss: 1.965830\n",
            "{AlexNet} Train Epoch: 14 [7168/37814 (19%)]\tLoss: 1.993554\n",
            "{AlexNet} Train Epoch: 14 [7680/37814 (20%)]\tLoss: 1.987286\n",
            "{AlexNet} Train Epoch: 14 [8192/37814 (22%)]\tLoss: 1.990816\n",
            "{AlexNet} Train Epoch: 14 [8704/37814 (23%)]\tLoss: 1.959011\n",
            "{AlexNet} Train Epoch: 14 [9216/37814 (24%)]\tLoss: 2.003129\n",
            "{AlexNet} Train Epoch: 14 [9728/37814 (26%)]\tLoss: 1.981405\n",
            "{AlexNet} Train Epoch: 14 [10240/37814 (27%)]\tLoss: 2.008147\n",
            "{AlexNet} Train Epoch: 14 [10752/37814 (28%)]\tLoss: 1.952277\n",
            "{AlexNet} Train Epoch: 14 [11264/37814 (30%)]\tLoss: 1.962761\n",
            "{AlexNet} Train Epoch: 14 [11776/37814 (31%)]\tLoss: 2.005900\n",
            "{AlexNet} Train Epoch: 14 [12288/37814 (32%)]\tLoss: 2.008740\n",
            "{AlexNet} Train Epoch: 14 [12800/37814 (34%)]\tLoss: 1.972639\n",
            "{AlexNet} Train Epoch: 14 [13312/37814 (35%)]\tLoss: 1.957611\n",
            "{AlexNet} Train Epoch: 14 [13824/37814 (36%)]\tLoss: 1.984139\n",
            "{AlexNet} Train Epoch: 14 [14336/37814 (38%)]\tLoss: 2.018788\n",
            "{AlexNet} Train Epoch: 14 [14848/37814 (39%)]\tLoss: 2.004548\n",
            "{AlexNet} Train Epoch: 14 [15360/37814 (41%)]\tLoss: 2.000203\n",
            "{AlexNet} Train Epoch: 14 [15872/37814 (42%)]\tLoss: 2.001354\n",
            "{AlexNet} Train Epoch: 14 [16384/37814 (43%)]\tLoss: 1.961776\n",
            "{AlexNet} Train Epoch: 14 [16896/37814 (45%)]\tLoss: 2.014835\n",
            "{AlexNet} Train Epoch: 14 [17408/37814 (46%)]\tLoss: 2.013405\n",
            "{AlexNet} Train Epoch: 14 [17920/37814 (47%)]\tLoss: 2.020570\n",
            "{AlexNet} Train Epoch: 14 [18432/37814 (49%)]\tLoss: 2.012720\n",
            "{AlexNet} Train Epoch: 14 [18944/37814 (50%)]\tLoss: 1.995229\n",
            "{AlexNet} Train Epoch: 14 [19456/37814 (51%)]\tLoss: 1.933591\n",
            "{AlexNet} Train Epoch: 14 [19968/37814 (53%)]\tLoss: 2.018344\n",
            "{AlexNet} Train Epoch: 14 [20480/37814 (54%)]\tLoss: 2.014490\n",
            "{AlexNet} Train Epoch: 14 [20992/37814 (55%)]\tLoss: 1.986658\n",
            "{AlexNet} Train Epoch: 14 [21504/37814 (57%)]\tLoss: 1.950350\n",
            "{AlexNet} Train Epoch: 14 [22016/37814 (58%)]\tLoss: 1.935607\n",
            "{AlexNet} Train Epoch: 14 [22528/37814 (59%)]\tLoss: 1.986917\n",
            "{AlexNet} Train Epoch: 14 [23040/37814 (61%)]\tLoss: 2.005205\n",
            "{AlexNet} Train Epoch: 14 [23552/37814 (62%)]\tLoss: 2.020765\n",
            "{AlexNet} Train Epoch: 14 [24064/37814 (64%)]\tLoss: 2.009497\n",
            "{AlexNet} Train Epoch: 14 [24576/37814 (65%)]\tLoss: 2.010550\n",
            "{AlexNet} Train Epoch: 14 [25088/37814 (66%)]\tLoss: 1.993454\n",
            "{AlexNet} Train Epoch: 14 [25600/37814 (68%)]\tLoss: 2.005446\n",
            "{AlexNet} Train Epoch: 14 [26112/37814 (69%)]\tLoss: 1.982444\n",
            "{AlexNet} Train Epoch: 14 [26624/37814 (70%)]\tLoss: 1.918515\n",
            "{AlexNet} Train Epoch: 14 [27136/37814 (72%)]\tLoss: 2.013588\n",
            "{AlexNet} Train Epoch: 14 [27648/37814 (73%)]\tLoss: 1.996140\n",
            "{AlexNet} Train Epoch: 14 [28160/37814 (74%)]\tLoss: 2.007090\n",
            "{AlexNet} Train Epoch: 14 [28672/37814 (76%)]\tLoss: 2.019579\n",
            "{AlexNet} Train Epoch: 14 [29184/37814 (77%)]\tLoss: 2.041915\n",
            "{AlexNet} Train Epoch: 14 [29696/37814 (78%)]\tLoss: 2.015098\n",
            "{AlexNet} Train Epoch: 14 [30208/37814 (80%)]\tLoss: 2.046836\n",
            "{AlexNet} Train Epoch: 14 [30720/37814 (81%)]\tLoss: 1.980075\n",
            "{AlexNet} Train Epoch: 14 [31232/37814 (82%)]\tLoss: 2.041289\n",
            "{AlexNet} Train Epoch: 14 [31744/37814 (84%)]\tLoss: 1.980683\n",
            "{AlexNet} Train Epoch: 14 [32256/37814 (85%)]\tLoss: 1.942902\n",
            "{AlexNet} Train Epoch: 14 [32768/37814 (86%)]\tLoss: 2.016132\n",
            "{AlexNet} Train Epoch: 14 [33280/37814 (88%)]\tLoss: 1.962345\n",
            "{AlexNet} Train Epoch: 14 [33792/37814 (89%)]\tLoss: 1.935688\n",
            "{AlexNet} Train Epoch: 14 [34304/37814 (91%)]\tLoss: 1.976001\n",
            "{AlexNet} Train Epoch: 14 [34816/37814 (92%)]\tLoss: 1.998030\n",
            "{AlexNet} Train Epoch: 14 [35328/37814 (93%)]\tLoss: 2.012785\n",
            "{AlexNet} Train Epoch: 14 [35840/37814 (95%)]\tLoss: 2.031633\n",
            "{AlexNet} Train Epoch: 14 [36352/37814 (96%)]\tLoss: 1.984780\n",
            "{AlexNet} Train Epoch: 14 [36864/37814 (97%)]\tLoss: 2.012254\n",
            "{AlexNet} Train Epoch: 14 [31974/37814 (99%)]\tLoss: 2.013029\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9866, Accuracy: 1035/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 28.028069734573364 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 14 [0/37814 (0%)]\tLoss: 1.846684\n",
            "{SqueezeNet} Train Epoch: 14 [512/37814 (1%)]\tLoss: 1.831219\n",
            "{SqueezeNet} Train Epoch: 14 [1024/37814 (3%)]\tLoss: 1.865698\n",
            "{SqueezeNet} Train Epoch: 14 [1536/37814 (4%)]\tLoss: 1.828766\n",
            "{SqueezeNet} Train Epoch: 14 [2048/37814 (5%)]\tLoss: 1.832350\n",
            "{SqueezeNet} Train Epoch: 14 [2560/37814 (7%)]\tLoss: 1.838333\n",
            "{SqueezeNet} Train Epoch: 14 [3072/37814 (8%)]\tLoss: 1.826205\n",
            "{SqueezeNet} Train Epoch: 14 [3584/37814 (9%)]\tLoss: 1.831976\n",
            "{SqueezeNet} Train Epoch: 14 [4096/37814 (11%)]\tLoss: 1.830085\n",
            "{SqueezeNet} Train Epoch: 14 [4608/37814 (12%)]\tLoss: 1.833003\n",
            "{SqueezeNet} Train Epoch: 14 [5120/37814 (14%)]\tLoss: 1.819178\n",
            "{SqueezeNet} Train Epoch: 14 [5632/37814 (15%)]\tLoss: 1.878215\n",
            "{SqueezeNet} Train Epoch: 14 [6144/37814 (16%)]\tLoss: 1.878966\n",
            "{SqueezeNet} Train Epoch: 14 [6656/37814 (18%)]\tLoss: 1.834661\n",
            "{SqueezeNet} Train Epoch: 14 [7168/37814 (19%)]\tLoss: 1.849249\n",
            "{SqueezeNet} Train Epoch: 14 [7680/37814 (20%)]\tLoss: 1.887074\n",
            "{SqueezeNet} Train Epoch: 14 [8192/37814 (22%)]\tLoss: 1.895060\n",
            "{SqueezeNet} Train Epoch: 14 [8704/37814 (23%)]\tLoss: 1.837112\n",
            "{SqueezeNet} Train Epoch: 14 [9216/37814 (24%)]\tLoss: 1.898550\n",
            "{SqueezeNet} Train Epoch: 14 [9728/37814 (26%)]\tLoss: 1.793140\n",
            "{SqueezeNet} Train Epoch: 14 [10240/37814 (27%)]\tLoss: 1.840474\n",
            "{SqueezeNet} Train Epoch: 14 [10752/37814 (28%)]\tLoss: 1.815107\n",
            "{SqueezeNet} Train Epoch: 14 [11264/37814 (30%)]\tLoss: 1.855603\n",
            "{SqueezeNet} Train Epoch: 14 [11776/37814 (31%)]\tLoss: 1.788178\n",
            "{SqueezeNet} Train Epoch: 14 [12288/37814 (32%)]\tLoss: 1.837333\n",
            "{SqueezeNet} Train Epoch: 14 [12800/37814 (34%)]\tLoss: 1.813992\n",
            "{SqueezeNet} Train Epoch: 14 [13312/37814 (35%)]\tLoss: 1.835691\n",
            "{SqueezeNet} Train Epoch: 14 [13824/37814 (36%)]\tLoss: 1.861663\n",
            "{SqueezeNet} Train Epoch: 14 [14336/37814 (38%)]\tLoss: 1.873605\n",
            "{SqueezeNet} Train Epoch: 14 [14848/37814 (39%)]\tLoss: 1.815719\n",
            "{SqueezeNet} Train Epoch: 14 [15360/37814 (41%)]\tLoss: 1.868688\n",
            "{SqueezeNet} Train Epoch: 14 [15872/37814 (42%)]\tLoss: 1.905300\n",
            "{SqueezeNet} Train Epoch: 14 [16384/37814 (43%)]\tLoss: 1.869182\n",
            "{SqueezeNet} Train Epoch: 14 [16896/37814 (45%)]\tLoss: 1.842357\n",
            "{SqueezeNet} Train Epoch: 14 [17408/37814 (46%)]\tLoss: 1.807764\n",
            "{SqueezeNet} Train Epoch: 14 [17920/37814 (47%)]\tLoss: 1.776829\n",
            "{SqueezeNet} Train Epoch: 14 [18432/37814 (49%)]\tLoss: 1.887970\n",
            "{SqueezeNet} Train Epoch: 14 [18944/37814 (50%)]\tLoss: 1.912290\n",
            "{SqueezeNet} Train Epoch: 14 [19456/37814 (51%)]\tLoss: 1.910753\n",
            "{SqueezeNet} Train Epoch: 14 [19968/37814 (53%)]\tLoss: 1.825566\n",
            "{SqueezeNet} Train Epoch: 14 [20480/37814 (54%)]\tLoss: 1.866153\n",
            "{SqueezeNet} Train Epoch: 14 [20992/37814 (55%)]\tLoss: 1.849235\n",
            "{SqueezeNet} Train Epoch: 14 [21504/37814 (57%)]\tLoss: 1.871343\n",
            "{SqueezeNet} Train Epoch: 14 [22016/37814 (58%)]\tLoss: 1.891661\n",
            "{SqueezeNet} Train Epoch: 14 [22528/37814 (59%)]\tLoss: 1.865518\n",
            "{SqueezeNet} Train Epoch: 14 [23040/37814 (61%)]\tLoss: 1.843831\n",
            "{SqueezeNet} Train Epoch: 14 [23552/37814 (62%)]\tLoss: 1.896643\n",
            "{SqueezeNet} Train Epoch: 14 [24064/37814 (64%)]\tLoss: 1.827268\n",
            "{SqueezeNet} Train Epoch: 14 [24576/37814 (65%)]\tLoss: 1.911508\n",
            "{SqueezeNet} Train Epoch: 14 [25088/37814 (66%)]\tLoss: 1.858871\n",
            "{SqueezeNet} Train Epoch: 14 [25600/37814 (68%)]\tLoss: 1.853417\n",
            "{SqueezeNet} Train Epoch: 14 [26112/37814 (69%)]\tLoss: 1.809630\n",
            "{SqueezeNet} Train Epoch: 14 [26624/37814 (70%)]\tLoss: 1.859900\n",
            "{SqueezeNet} Train Epoch: 14 [27136/37814 (72%)]\tLoss: 1.861551\n",
            "{SqueezeNet} Train Epoch: 14 [27648/37814 (73%)]\tLoss: 1.870609\n",
            "{SqueezeNet} Train Epoch: 14 [28160/37814 (74%)]\tLoss: 1.867910\n",
            "{SqueezeNet} Train Epoch: 14 [28672/37814 (76%)]\tLoss: 1.833797\n",
            "{SqueezeNet} Train Epoch: 14 [29184/37814 (77%)]\tLoss: 1.862209\n",
            "{SqueezeNet} Train Epoch: 14 [29696/37814 (78%)]\tLoss: 1.836463\n",
            "{SqueezeNet} Train Epoch: 14 [30208/37814 (80%)]\tLoss: 1.865614\n",
            "{SqueezeNet} Train Epoch: 14 [30720/37814 (81%)]\tLoss: 1.899411\n",
            "{SqueezeNet} Train Epoch: 14 [31232/37814 (82%)]\tLoss: 1.844736\n",
            "{SqueezeNet} Train Epoch: 14 [31744/37814 (84%)]\tLoss: 1.816867\n",
            "{SqueezeNet} Train Epoch: 14 [32256/37814 (85%)]\tLoss: 1.845178\n",
            "{SqueezeNet} Train Epoch: 14 [32768/37814 (86%)]\tLoss: 1.800910\n",
            "{SqueezeNet} Train Epoch: 14 [33280/37814 (88%)]\tLoss: 1.796365\n",
            "{SqueezeNet} Train Epoch: 14 [33792/37814 (89%)]\tLoss: 1.874834\n",
            "{SqueezeNet} Train Epoch: 14 [34304/37814 (91%)]\tLoss: 1.844693\n",
            "{SqueezeNet} Train Epoch: 14 [34816/37814 (92%)]\tLoss: 1.801465\n",
            "{SqueezeNet} Train Epoch: 14 [35328/37814 (93%)]\tLoss: 1.850493\n",
            "{SqueezeNet} Train Epoch: 14 [35840/37814 (95%)]\tLoss: 1.869574\n",
            "{SqueezeNet} Train Epoch: 14 [36352/37814 (96%)]\tLoss: 1.829444\n",
            "{SqueezeNet} Train Epoch: 14 [36864/37814 (97%)]\tLoss: 1.873365\n",
            "{SqueezeNet} Train Epoch: 14 [31974/37814 (99%)]\tLoss: 1.862956\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8259, Accuracy: 1588/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.224146366119385 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f3c7dd4-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"3d1ac882-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_17f0cef1da"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f3e14fa-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_98896a77e5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f3e5578-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_4c28efed8f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f3ea140-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"5f3e5578-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_da0943e263"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f6c2b56-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"5f3e14fa-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4225de49f7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f6f0790-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_3a801223ea"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f6f7036-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_f58c4052d6"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5f6fd6de-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"5f6f7036-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_3c436ff304"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 15 [0/37814 (0%)]\tLoss: 1.965501\n",
            "{AlexNet} Train Epoch: 15 [512/37814 (1%)]\tLoss: 1.980081\n",
            "{AlexNet} Train Epoch: 15 [1024/37814 (3%)]\tLoss: 1.954116\n",
            "{AlexNet} Train Epoch: 15 [1536/37814 (4%)]\tLoss: 2.040871\n",
            "{AlexNet} Train Epoch: 15 [2048/37814 (5%)]\tLoss: 2.020485\n",
            "{AlexNet} Train Epoch: 15 [2560/37814 (7%)]\tLoss: 1.979545\n",
            "{AlexNet} Train Epoch: 15 [3072/37814 (8%)]\tLoss: 2.001270\n",
            "{AlexNet} Train Epoch: 15 [3584/37814 (9%)]\tLoss: 2.018375\n",
            "{AlexNet} Train Epoch: 15 [4096/37814 (11%)]\tLoss: 1.980201\n",
            "{AlexNet} Train Epoch: 15 [4608/37814 (12%)]\tLoss: 1.998047\n",
            "{AlexNet} Train Epoch: 15 [5120/37814 (14%)]\tLoss: 2.021950\n",
            "{AlexNet} Train Epoch: 15 [5632/37814 (15%)]\tLoss: 2.024335\n",
            "{AlexNet} Train Epoch: 15 [6144/37814 (16%)]\tLoss: 1.985607\n",
            "{AlexNet} Train Epoch: 15 [6656/37814 (18%)]\tLoss: 2.013396\n",
            "{AlexNet} Train Epoch: 15 [7168/37814 (19%)]\tLoss: 1.975102\n",
            "{AlexNet} Train Epoch: 15 [7680/37814 (20%)]\tLoss: 1.976273\n",
            "{AlexNet} Train Epoch: 15 [8192/37814 (22%)]\tLoss: 1.975237\n",
            "{AlexNet} Train Epoch: 15 [8704/37814 (23%)]\tLoss: 1.982706\n",
            "{AlexNet} Train Epoch: 15 [9216/37814 (24%)]\tLoss: 1.971397\n",
            "{AlexNet} Train Epoch: 15 [9728/37814 (26%)]\tLoss: 1.990017\n",
            "{AlexNet} Train Epoch: 15 [10240/37814 (27%)]\tLoss: 2.019174\n",
            "{AlexNet} Train Epoch: 15 [10752/37814 (28%)]\tLoss: 1.986874\n",
            "{AlexNet} Train Epoch: 15 [11264/37814 (30%)]\tLoss: 1.985731\n",
            "{AlexNet} Train Epoch: 15 [11776/37814 (31%)]\tLoss: 2.005270\n",
            "{AlexNet} Train Epoch: 15 [12288/37814 (32%)]\tLoss: 1.977279\n",
            "{AlexNet} Train Epoch: 15 [12800/37814 (34%)]\tLoss: 1.998392\n",
            "{AlexNet} Train Epoch: 15 [13312/37814 (35%)]\tLoss: 2.017025\n",
            "{AlexNet} Train Epoch: 15 [13824/37814 (36%)]\tLoss: 2.003351\n",
            "{AlexNet} Train Epoch: 15 [14336/37814 (38%)]\tLoss: 1.968660\n",
            "{AlexNet} Train Epoch: 15 [14848/37814 (39%)]\tLoss: 2.009244\n",
            "{AlexNet} Train Epoch: 15 [15360/37814 (41%)]\tLoss: 1.986319\n",
            "{AlexNet} Train Epoch: 15 [15872/37814 (42%)]\tLoss: 1.968356\n",
            "{AlexNet} Train Epoch: 15 [16384/37814 (43%)]\tLoss: 2.021344\n",
            "{AlexNet} Train Epoch: 15 [16896/37814 (45%)]\tLoss: 1.980423\n",
            "{AlexNet} Train Epoch: 15 [17408/37814 (46%)]\tLoss: 1.971834\n",
            "{AlexNet} Train Epoch: 15 [17920/37814 (47%)]\tLoss: 2.030575\n",
            "{AlexNet} Train Epoch: 15 [18432/37814 (49%)]\tLoss: 1.991123\n",
            "{AlexNet} Train Epoch: 15 [18944/37814 (50%)]\tLoss: 2.014764\n",
            "{AlexNet} Train Epoch: 15 [19456/37814 (51%)]\tLoss: 2.006160\n",
            "{AlexNet} Train Epoch: 15 [19968/37814 (53%)]\tLoss: 1.944708\n",
            "{AlexNet} Train Epoch: 15 [20480/37814 (54%)]\tLoss: 1.962123\n",
            "{AlexNet} Train Epoch: 15 [20992/37814 (55%)]\tLoss: 1.993739\n",
            "{AlexNet} Train Epoch: 15 [21504/37814 (57%)]\tLoss: 1.987958\n",
            "{AlexNet} Train Epoch: 15 [22016/37814 (58%)]\tLoss: 2.013699\n",
            "{AlexNet} Train Epoch: 15 [22528/37814 (59%)]\tLoss: 1.964065\n",
            "{AlexNet} Train Epoch: 15 [23040/37814 (61%)]\tLoss: 2.005381\n",
            "{AlexNet} Train Epoch: 15 [23552/37814 (62%)]\tLoss: 2.029094\n",
            "{AlexNet} Train Epoch: 15 [24064/37814 (64%)]\tLoss: 1.994061\n",
            "{AlexNet} Train Epoch: 15 [24576/37814 (65%)]\tLoss: 2.002558\n",
            "{AlexNet} Train Epoch: 15 [25088/37814 (66%)]\tLoss: 1.955317\n",
            "{AlexNet} Train Epoch: 15 [25600/37814 (68%)]\tLoss: 1.994005\n",
            "{AlexNet} Train Epoch: 15 [26112/37814 (69%)]\tLoss: 1.961710\n",
            "{AlexNet} Train Epoch: 15 [26624/37814 (70%)]\tLoss: 1.955956\n",
            "{AlexNet} Train Epoch: 15 [27136/37814 (72%)]\tLoss: 1.986051\n",
            "{AlexNet} Train Epoch: 15 [27648/37814 (73%)]\tLoss: 2.019635\n",
            "{AlexNet} Train Epoch: 15 [28160/37814 (74%)]\tLoss: 2.014648\n",
            "{AlexNet} Train Epoch: 15 [28672/37814 (76%)]\tLoss: 1.988459\n",
            "{AlexNet} Train Epoch: 15 [29184/37814 (77%)]\tLoss: 1.989415\n",
            "{AlexNet} Train Epoch: 15 [29696/37814 (78%)]\tLoss: 1.999279\n",
            "{AlexNet} Train Epoch: 15 [30208/37814 (80%)]\tLoss: 1.981037\n",
            "{AlexNet} Train Epoch: 15 [30720/37814 (81%)]\tLoss: 1.954499\n",
            "{AlexNet} Train Epoch: 15 [31232/37814 (82%)]\tLoss: 2.001414\n",
            "{AlexNet} Train Epoch: 15 [31744/37814 (84%)]\tLoss: 2.019432\n",
            "{AlexNet} Train Epoch: 15 [32256/37814 (85%)]\tLoss: 1.985750\n",
            "{AlexNet} Train Epoch: 15 [32768/37814 (86%)]\tLoss: 1.980375\n",
            "{AlexNet} Train Epoch: 15 [33280/37814 (88%)]\tLoss: 1.974853\n",
            "{AlexNet} Train Epoch: 15 [33792/37814 (89%)]\tLoss: 1.973444\n",
            "{AlexNet} Train Epoch: 15 [34304/37814 (91%)]\tLoss: 1.976560\n",
            "{AlexNet} Train Epoch: 15 [34816/37814 (92%)]\tLoss: 2.007656\n",
            "{AlexNet} Train Epoch: 15 [35328/37814 (93%)]\tLoss: 2.008811\n",
            "{AlexNet} Train Epoch: 15 [35840/37814 (95%)]\tLoss: 1.987679\n",
            "{AlexNet} Train Epoch: 15 [36352/37814 (96%)]\tLoss: 1.981657\n",
            "{AlexNet} Train Epoch: 15 [36864/37814 (97%)]\tLoss: 1.995365\n",
            "{AlexNet} Train Epoch: 15 [31974/37814 (99%)]\tLoss: 1.962633\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9825, Accuracy: 1051/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.82236886024475 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 15 [0/37814 (0%)]\tLoss: 1.888014\n",
            "{SqueezeNet} Train Epoch: 15 [512/37814 (1%)]\tLoss: 1.864878\n",
            "{SqueezeNet} Train Epoch: 15 [1024/37814 (3%)]\tLoss: 1.840304\n",
            "{SqueezeNet} Train Epoch: 15 [1536/37814 (4%)]\tLoss: 1.787355\n",
            "{SqueezeNet} Train Epoch: 15 [2048/37814 (5%)]\tLoss: 1.792176\n",
            "{SqueezeNet} Train Epoch: 15 [2560/37814 (7%)]\tLoss: 1.851202\n",
            "{SqueezeNet} Train Epoch: 15 [3072/37814 (8%)]\tLoss: 1.815649\n",
            "{SqueezeNet} Train Epoch: 15 [3584/37814 (9%)]\tLoss: 1.849390\n",
            "{SqueezeNet} Train Epoch: 15 [4096/37814 (11%)]\tLoss: 1.865005\n",
            "{SqueezeNet} Train Epoch: 15 [4608/37814 (12%)]\tLoss: 1.864978\n",
            "{SqueezeNet} Train Epoch: 15 [5120/37814 (14%)]\tLoss: 1.853855\n",
            "{SqueezeNet} Train Epoch: 15 [5632/37814 (15%)]\tLoss: 1.835821\n",
            "{SqueezeNet} Train Epoch: 15 [6144/37814 (16%)]\tLoss: 1.843770\n",
            "{SqueezeNet} Train Epoch: 15 [6656/37814 (18%)]\tLoss: 1.857452\n",
            "{SqueezeNet} Train Epoch: 15 [7168/37814 (19%)]\tLoss: 1.814601\n",
            "{SqueezeNet} Train Epoch: 15 [7680/37814 (20%)]\tLoss: 1.810373\n",
            "{SqueezeNet} Train Epoch: 15 [8192/37814 (22%)]\tLoss: 1.814164\n",
            "{SqueezeNet} Train Epoch: 15 [8704/37814 (23%)]\tLoss: 1.798198\n",
            "{SqueezeNet} Train Epoch: 15 [9216/37814 (24%)]\tLoss: 1.907740\n",
            "{SqueezeNet} Train Epoch: 15 [9728/37814 (26%)]\tLoss: 1.843790\n",
            "{SqueezeNet} Train Epoch: 15 [10240/37814 (27%)]\tLoss: 1.794370\n",
            "{SqueezeNet} Train Epoch: 15 [10752/37814 (28%)]\tLoss: 1.859841\n",
            "{SqueezeNet} Train Epoch: 15 [11264/37814 (30%)]\tLoss: 1.784366\n",
            "{SqueezeNet} Train Epoch: 15 [11776/37814 (31%)]\tLoss: 1.882140\n",
            "{SqueezeNet} Train Epoch: 15 [12288/37814 (32%)]\tLoss: 1.859706\n",
            "{SqueezeNet} Train Epoch: 15 [12800/37814 (34%)]\tLoss: 1.834211\n",
            "{SqueezeNet} Train Epoch: 15 [13312/37814 (35%)]\tLoss: 1.819997\n",
            "{SqueezeNet} Train Epoch: 15 [13824/37814 (36%)]\tLoss: 1.860280\n",
            "{SqueezeNet} Train Epoch: 15 [14336/37814 (38%)]\tLoss: 1.889691\n",
            "{SqueezeNet} Train Epoch: 15 [14848/37814 (39%)]\tLoss: 1.880372\n",
            "{SqueezeNet} Train Epoch: 15 [15360/37814 (41%)]\tLoss: 1.847757\n",
            "{SqueezeNet} Train Epoch: 15 [15872/37814 (42%)]\tLoss: 1.854024\n",
            "{SqueezeNet} Train Epoch: 15 [16384/37814 (43%)]\tLoss: 1.844205\n",
            "{SqueezeNet} Train Epoch: 15 [16896/37814 (45%)]\tLoss: 1.814883\n",
            "{SqueezeNet} Train Epoch: 15 [17408/37814 (46%)]\tLoss: 1.871781\n",
            "{SqueezeNet} Train Epoch: 15 [17920/37814 (47%)]\tLoss: 1.860713\n",
            "{SqueezeNet} Train Epoch: 15 [18432/37814 (49%)]\tLoss: 1.861825\n",
            "{SqueezeNet} Train Epoch: 15 [18944/37814 (50%)]\tLoss: 1.840964\n",
            "{SqueezeNet} Train Epoch: 15 [19456/37814 (51%)]\tLoss: 1.842937\n",
            "{SqueezeNet} Train Epoch: 15 [19968/37814 (53%)]\tLoss: 1.839496\n",
            "{SqueezeNet} Train Epoch: 15 [20480/37814 (54%)]\tLoss: 1.869765\n",
            "{SqueezeNet} Train Epoch: 15 [20992/37814 (55%)]\tLoss: 1.836951\n",
            "{SqueezeNet} Train Epoch: 15 [21504/37814 (57%)]\tLoss: 1.799793\n",
            "{SqueezeNet} Train Epoch: 15 [22016/37814 (58%)]\tLoss: 1.881740\n",
            "{SqueezeNet} Train Epoch: 15 [22528/37814 (59%)]\tLoss: 1.880447\n",
            "{SqueezeNet} Train Epoch: 15 [23040/37814 (61%)]\tLoss: 1.817632\n",
            "{SqueezeNet} Train Epoch: 15 [23552/37814 (62%)]\tLoss: 1.852606\n",
            "{SqueezeNet} Train Epoch: 15 [24064/37814 (64%)]\tLoss: 1.809404\n",
            "{SqueezeNet} Train Epoch: 15 [24576/37814 (65%)]\tLoss: 1.801408\n",
            "{SqueezeNet} Train Epoch: 15 [25088/37814 (66%)]\tLoss: 1.880436\n",
            "{SqueezeNet} Train Epoch: 15 [25600/37814 (68%)]\tLoss: 1.842489\n",
            "{SqueezeNet} Train Epoch: 15 [26112/37814 (69%)]\tLoss: 1.813818\n",
            "{SqueezeNet} Train Epoch: 15 [26624/37814 (70%)]\tLoss: 1.775861\n",
            "{SqueezeNet} Train Epoch: 15 [27136/37814 (72%)]\tLoss: 1.849277\n",
            "{SqueezeNet} Train Epoch: 15 [27648/37814 (73%)]\tLoss: 1.854975\n",
            "{SqueezeNet} Train Epoch: 15 [28160/37814 (74%)]\tLoss: 1.894532\n",
            "{SqueezeNet} Train Epoch: 15 [28672/37814 (76%)]\tLoss: 1.886515\n",
            "{SqueezeNet} Train Epoch: 15 [29184/37814 (77%)]\tLoss: 1.854144\n",
            "{SqueezeNet} Train Epoch: 15 [29696/37814 (78%)]\tLoss: 1.872863\n",
            "{SqueezeNet} Train Epoch: 15 [30208/37814 (80%)]\tLoss: 1.887665\n",
            "{SqueezeNet} Train Epoch: 15 [30720/37814 (81%)]\tLoss: 1.815556\n",
            "{SqueezeNet} Train Epoch: 15 [31232/37814 (82%)]\tLoss: 1.896427\n",
            "{SqueezeNet} Train Epoch: 15 [31744/37814 (84%)]\tLoss: 1.878907\n",
            "{SqueezeNet} Train Epoch: 15 [32256/37814 (85%)]\tLoss: 1.826384\n",
            "{SqueezeNet} Train Epoch: 15 [32768/37814 (86%)]\tLoss: 1.872537\n",
            "{SqueezeNet} Train Epoch: 15 [33280/37814 (88%)]\tLoss: 1.853588\n",
            "{SqueezeNet} Train Epoch: 15 [33792/37814 (89%)]\tLoss: 1.884088\n",
            "{SqueezeNet} Train Epoch: 15 [34304/37814 (91%)]\tLoss: 1.852345\n",
            "{SqueezeNet} Train Epoch: 15 [34816/37814 (92%)]\tLoss: 1.796898\n",
            "{SqueezeNet} Train Epoch: 15 [35328/37814 (93%)]\tLoss: 1.804988\n",
            "{SqueezeNet} Train Epoch: 15 [35840/37814 (95%)]\tLoss: 1.845159\n",
            "{SqueezeNet} Train Epoch: 15 [36352/37814 (96%)]\tLoss: 1.864348\n",
            "{SqueezeNet} Train Epoch: 15 [36864/37814 (97%)]\tLoss: 1.881307\n",
            "{SqueezeNet} Train Epoch: 15 [31974/37814 (99%)]\tLoss: 1.808445\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8369, Accuracy: 1519/5000 (30%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.349366664886475 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"80542026-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"5f6f0790-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e36f2da832"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"80560274-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_ff63911270"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"805654ae-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_66cccead53"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"80569784-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"805654ae-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_17658f710d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"807db490-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"80560274-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6932ba4f6c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"807e9388-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_a06840b816"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"807ec10a-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_4aabc239f3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"807ef0bc-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"807ec10a-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b2bdbf6a67"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 16 [0/37814 (0%)]\tLoss: 2.021500\n",
            "{AlexNet} Train Epoch: 16 [512/37814 (1%)]\tLoss: 2.010675\n",
            "{AlexNet} Train Epoch: 16 [1024/37814 (3%)]\tLoss: 2.016329\n",
            "{AlexNet} Train Epoch: 16 [1536/37814 (4%)]\tLoss: 1.965400\n",
            "{AlexNet} Train Epoch: 16 [2048/37814 (5%)]\tLoss: 2.029588\n",
            "{AlexNet} Train Epoch: 16 [2560/37814 (7%)]\tLoss: 1.942869\n",
            "{AlexNet} Train Epoch: 16 [3072/37814 (8%)]\tLoss: 2.024088\n",
            "{AlexNet} Train Epoch: 16 [3584/37814 (9%)]\tLoss: 1.977911\n",
            "{AlexNet} Train Epoch: 16 [4096/37814 (11%)]\tLoss: 2.018492\n",
            "{AlexNet} Train Epoch: 16 [4608/37814 (12%)]\tLoss: 2.003653\n",
            "{AlexNet} Train Epoch: 16 [5120/37814 (14%)]\tLoss: 1.946667\n",
            "{AlexNet} Train Epoch: 16 [5632/37814 (15%)]\tLoss: 2.000573\n",
            "{AlexNet} Train Epoch: 16 [6144/37814 (16%)]\tLoss: 2.025323\n",
            "{AlexNet} Train Epoch: 16 [6656/37814 (18%)]\tLoss: 1.986076\n",
            "{AlexNet} Train Epoch: 16 [7168/37814 (19%)]\tLoss: 1.951881\n",
            "{AlexNet} Train Epoch: 16 [7680/37814 (20%)]\tLoss: 2.009527\n",
            "{AlexNet} Train Epoch: 16 [8192/37814 (22%)]\tLoss: 2.020690\n",
            "{AlexNet} Train Epoch: 16 [8704/37814 (23%)]\tLoss: 2.000189\n",
            "{AlexNet} Train Epoch: 16 [9216/37814 (24%)]\tLoss: 1.987195\n",
            "{AlexNet} Train Epoch: 16 [9728/37814 (26%)]\tLoss: 2.036938\n",
            "{AlexNet} Train Epoch: 16 [10240/37814 (27%)]\tLoss: 1.983436\n",
            "{AlexNet} Train Epoch: 16 [10752/37814 (28%)]\tLoss: 1.963727\n",
            "{AlexNet} Train Epoch: 16 [11264/37814 (30%)]\tLoss: 1.939952\n",
            "{AlexNet} Train Epoch: 16 [11776/37814 (31%)]\tLoss: 2.007951\n",
            "{AlexNet} Train Epoch: 16 [12288/37814 (32%)]\tLoss: 1.984942\n",
            "{AlexNet} Train Epoch: 16 [12800/37814 (34%)]\tLoss: 1.964830\n",
            "{AlexNet} Train Epoch: 16 [13312/37814 (35%)]\tLoss: 2.006367\n",
            "{AlexNet} Train Epoch: 16 [13824/37814 (36%)]\tLoss: 1.989651\n",
            "{AlexNet} Train Epoch: 16 [14336/37814 (38%)]\tLoss: 1.922886\n",
            "{AlexNet} Train Epoch: 16 [14848/37814 (39%)]\tLoss: 1.992149\n",
            "{AlexNet} Train Epoch: 16 [15360/37814 (41%)]\tLoss: 1.964541\n",
            "{AlexNet} Train Epoch: 16 [15872/37814 (42%)]\tLoss: 1.980038\n",
            "{AlexNet} Train Epoch: 16 [16384/37814 (43%)]\tLoss: 1.987012\n",
            "{AlexNet} Train Epoch: 16 [16896/37814 (45%)]\tLoss: 1.981255\n",
            "{AlexNet} Train Epoch: 16 [17408/37814 (46%)]\tLoss: 1.941426\n",
            "{AlexNet} Train Epoch: 16 [17920/37814 (47%)]\tLoss: 1.975970\n",
            "{AlexNet} Train Epoch: 16 [18432/37814 (49%)]\tLoss: 1.971591\n",
            "{AlexNet} Train Epoch: 16 [18944/37814 (50%)]\tLoss: 1.991345\n",
            "{AlexNet} Train Epoch: 16 [19456/37814 (51%)]\tLoss: 2.009377\n",
            "{AlexNet} Train Epoch: 16 [19968/37814 (53%)]\tLoss: 2.034780\n",
            "{AlexNet} Train Epoch: 16 [20480/37814 (54%)]\tLoss: 1.971855\n",
            "{AlexNet} Train Epoch: 16 [20992/37814 (55%)]\tLoss: 2.002002\n",
            "{AlexNet} Train Epoch: 16 [21504/37814 (57%)]\tLoss: 1.992815\n",
            "{AlexNet} Train Epoch: 16 [22016/37814 (58%)]\tLoss: 2.052179\n",
            "{AlexNet} Train Epoch: 16 [22528/37814 (59%)]\tLoss: 2.018477\n",
            "{AlexNet} Train Epoch: 16 [23040/37814 (61%)]\tLoss: 1.935582\n",
            "{AlexNet} Train Epoch: 16 [23552/37814 (62%)]\tLoss: 2.038743\n",
            "{AlexNet} Train Epoch: 16 [24064/37814 (64%)]\tLoss: 2.037872\n",
            "{AlexNet} Train Epoch: 16 [24576/37814 (65%)]\tLoss: 1.975726\n",
            "{AlexNet} Train Epoch: 16 [25088/37814 (66%)]\tLoss: 1.972364\n",
            "{AlexNet} Train Epoch: 16 [25600/37814 (68%)]\tLoss: 2.040883\n",
            "{AlexNet} Train Epoch: 16 [26112/37814 (69%)]\tLoss: 2.017113\n",
            "{AlexNet} Train Epoch: 16 [26624/37814 (70%)]\tLoss: 2.025643\n",
            "{AlexNet} Train Epoch: 16 [27136/37814 (72%)]\tLoss: 1.918065\n",
            "{AlexNet} Train Epoch: 16 [27648/37814 (73%)]\tLoss: 1.988214\n",
            "{AlexNet} Train Epoch: 16 [28160/37814 (74%)]\tLoss: 2.031224\n",
            "{AlexNet} Train Epoch: 16 [28672/37814 (76%)]\tLoss: 2.038205\n",
            "{AlexNet} Train Epoch: 16 [29184/37814 (77%)]\tLoss: 1.991447\n",
            "{AlexNet} Train Epoch: 16 [29696/37814 (78%)]\tLoss: 1.998366\n",
            "{AlexNet} Train Epoch: 16 [30208/37814 (80%)]\tLoss: 1.955960\n",
            "{AlexNet} Train Epoch: 16 [30720/37814 (81%)]\tLoss: 1.978846\n",
            "{AlexNet} Train Epoch: 16 [31232/37814 (82%)]\tLoss: 1.975668\n",
            "{AlexNet} Train Epoch: 16 [31744/37814 (84%)]\tLoss: 2.009509\n",
            "{AlexNet} Train Epoch: 16 [32256/37814 (85%)]\tLoss: 1.968065\n",
            "{AlexNet} Train Epoch: 16 [32768/37814 (86%)]\tLoss: 1.984430\n",
            "{AlexNet} Train Epoch: 16 [33280/37814 (88%)]\tLoss: 1.984045\n",
            "{AlexNet} Train Epoch: 16 [33792/37814 (89%)]\tLoss: 2.034416\n",
            "{AlexNet} Train Epoch: 16 [34304/37814 (91%)]\tLoss: 1.965478\n",
            "{AlexNet} Train Epoch: 16 [34816/37814 (92%)]\tLoss: 1.959884\n",
            "{AlexNet} Train Epoch: 16 [35328/37814 (93%)]\tLoss: 1.941112\n",
            "{AlexNet} Train Epoch: 16 [35840/37814 (95%)]\tLoss: 1.994979\n",
            "{AlexNet} Train Epoch: 16 [36352/37814 (96%)]\tLoss: 1.994856\n",
            "{AlexNet} Train Epoch: 16 [36864/37814 (97%)]\tLoss: 1.989665\n",
            "{AlexNet} Train Epoch: 16 [31974/37814 (99%)]\tLoss: 1.992546\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9937, Accuracy: 1026/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.844019412994385 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 16 [0/37814 (0%)]\tLoss: 1.815649\n",
            "{SqueezeNet} Train Epoch: 16 [512/37814 (1%)]\tLoss: 1.829686\n",
            "{SqueezeNet} Train Epoch: 16 [1024/37814 (3%)]\tLoss: 1.856808\n",
            "{SqueezeNet} Train Epoch: 16 [1536/37814 (4%)]\tLoss: 1.811282\n",
            "{SqueezeNet} Train Epoch: 16 [2048/37814 (5%)]\tLoss: 1.875016\n",
            "{SqueezeNet} Train Epoch: 16 [2560/37814 (7%)]\tLoss: 1.851636\n",
            "{SqueezeNet} Train Epoch: 16 [3072/37814 (8%)]\tLoss: 1.843061\n",
            "{SqueezeNet} Train Epoch: 16 [3584/37814 (9%)]\tLoss: 1.807593\n",
            "{SqueezeNet} Train Epoch: 16 [4096/37814 (11%)]\tLoss: 1.877277\n",
            "{SqueezeNet} Train Epoch: 16 [4608/37814 (12%)]\tLoss: 1.777392\n",
            "{SqueezeNet} Train Epoch: 16 [5120/37814 (14%)]\tLoss: 1.813995\n",
            "{SqueezeNet} Train Epoch: 16 [5632/37814 (15%)]\tLoss: 1.798440\n",
            "{SqueezeNet} Train Epoch: 16 [6144/37814 (16%)]\tLoss: 1.878291\n",
            "{SqueezeNet} Train Epoch: 16 [6656/37814 (18%)]\tLoss: 1.869708\n",
            "{SqueezeNet} Train Epoch: 16 [7168/37814 (19%)]\tLoss: 1.886422\n",
            "{SqueezeNet} Train Epoch: 16 [7680/37814 (20%)]\tLoss: 1.852545\n",
            "{SqueezeNet} Train Epoch: 16 [8192/37814 (22%)]\tLoss: 1.843594\n",
            "{SqueezeNet} Train Epoch: 16 [8704/37814 (23%)]\tLoss: 1.828932\n",
            "{SqueezeNet} Train Epoch: 16 [9216/37814 (24%)]\tLoss: 1.803656\n",
            "{SqueezeNet} Train Epoch: 16 [9728/37814 (26%)]\tLoss: 1.860537\n",
            "{SqueezeNet} Train Epoch: 16 [10240/37814 (27%)]\tLoss: 1.853957\n",
            "{SqueezeNet} Train Epoch: 16 [10752/37814 (28%)]\tLoss: 1.799819\n",
            "{SqueezeNet} Train Epoch: 16 [11264/37814 (30%)]\tLoss: 1.851368\n",
            "{SqueezeNet} Train Epoch: 16 [11776/37814 (31%)]\tLoss: 1.818420\n",
            "{SqueezeNet} Train Epoch: 16 [12288/37814 (32%)]\tLoss: 1.817919\n",
            "{SqueezeNet} Train Epoch: 16 [12800/37814 (34%)]\tLoss: 1.831248\n",
            "{SqueezeNet} Train Epoch: 16 [13312/37814 (35%)]\tLoss: 1.828184\n",
            "{SqueezeNet} Train Epoch: 16 [13824/37814 (36%)]\tLoss: 1.817528\n",
            "{SqueezeNet} Train Epoch: 16 [14336/37814 (38%)]\tLoss: 1.860102\n",
            "{SqueezeNet} Train Epoch: 16 [14848/37814 (39%)]\tLoss: 1.890882\n",
            "{SqueezeNet} Train Epoch: 16 [15360/37814 (41%)]\tLoss: 1.792868\n",
            "{SqueezeNet} Train Epoch: 16 [15872/37814 (42%)]\tLoss: 1.814134\n",
            "{SqueezeNet} Train Epoch: 16 [16384/37814 (43%)]\tLoss: 1.863242\n",
            "{SqueezeNet} Train Epoch: 16 [16896/37814 (45%)]\tLoss: 1.835844\n",
            "{SqueezeNet} Train Epoch: 16 [17408/37814 (46%)]\tLoss: 1.851543\n",
            "{SqueezeNet} Train Epoch: 16 [17920/37814 (47%)]\tLoss: 1.874254\n",
            "{SqueezeNet} Train Epoch: 16 [18432/37814 (49%)]\tLoss: 1.827083\n",
            "{SqueezeNet} Train Epoch: 16 [18944/37814 (50%)]\tLoss: 1.810385\n",
            "{SqueezeNet} Train Epoch: 16 [19456/37814 (51%)]\tLoss: 1.849980\n",
            "{SqueezeNet} Train Epoch: 16 [19968/37814 (53%)]\tLoss: 1.800229\n",
            "{SqueezeNet} Train Epoch: 16 [20480/37814 (54%)]\tLoss: 1.865377\n",
            "{SqueezeNet} Train Epoch: 16 [20992/37814 (55%)]\tLoss: 1.780078\n",
            "{SqueezeNet} Train Epoch: 16 [21504/37814 (57%)]\tLoss: 1.883118\n",
            "{SqueezeNet} Train Epoch: 16 [22016/37814 (58%)]\tLoss: 1.793139\n",
            "{SqueezeNet} Train Epoch: 16 [22528/37814 (59%)]\tLoss: 1.860100\n",
            "{SqueezeNet} Train Epoch: 16 [23040/37814 (61%)]\tLoss: 1.801965\n",
            "{SqueezeNet} Train Epoch: 16 [23552/37814 (62%)]\tLoss: 1.843930\n",
            "{SqueezeNet} Train Epoch: 16 [24064/37814 (64%)]\tLoss: 1.795059\n",
            "{SqueezeNet} Train Epoch: 16 [24576/37814 (65%)]\tLoss: 1.867500\n",
            "{SqueezeNet} Train Epoch: 16 [25088/37814 (66%)]\tLoss: 1.874836\n",
            "{SqueezeNet} Train Epoch: 16 [25600/37814 (68%)]\tLoss: 1.816528\n",
            "{SqueezeNet} Train Epoch: 16 [26112/37814 (69%)]\tLoss: 1.827394\n",
            "{SqueezeNet} Train Epoch: 16 [26624/37814 (70%)]\tLoss: 1.854345\n",
            "{SqueezeNet} Train Epoch: 16 [27136/37814 (72%)]\tLoss: 1.890114\n",
            "{SqueezeNet} Train Epoch: 16 [27648/37814 (73%)]\tLoss: 1.870969\n",
            "{SqueezeNet} Train Epoch: 16 [28160/37814 (74%)]\tLoss: 1.810757\n",
            "{SqueezeNet} Train Epoch: 16 [28672/37814 (76%)]\tLoss: 1.879264\n",
            "{SqueezeNet} Train Epoch: 16 [29184/37814 (77%)]\tLoss: 1.808359\n",
            "{SqueezeNet} Train Epoch: 16 [29696/37814 (78%)]\tLoss: 1.886026\n",
            "{SqueezeNet} Train Epoch: 16 [30208/37814 (80%)]\tLoss: 1.802925\n",
            "{SqueezeNet} Train Epoch: 16 [30720/37814 (81%)]\tLoss: 1.880946\n",
            "{SqueezeNet} Train Epoch: 16 [31232/37814 (82%)]\tLoss: 1.862289\n",
            "{SqueezeNet} Train Epoch: 16 [31744/37814 (84%)]\tLoss: 1.850992\n",
            "{SqueezeNet} Train Epoch: 16 [32256/37814 (85%)]\tLoss: 1.841563\n",
            "{SqueezeNet} Train Epoch: 16 [32768/37814 (86%)]\tLoss: 1.844249\n",
            "{SqueezeNet} Train Epoch: 16 [33280/37814 (88%)]\tLoss: 1.835638\n",
            "{SqueezeNet} Train Epoch: 16 [33792/37814 (89%)]\tLoss: 1.856442\n",
            "{SqueezeNet} Train Epoch: 16 [34304/37814 (91%)]\tLoss: 1.828981\n",
            "{SqueezeNet} Train Epoch: 16 [34816/37814 (92%)]\tLoss: 1.822612\n",
            "{SqueezeNet} Train Epoch: 16 [35328/37814 (93%)]\tLoss: 1.782728\n",
            "{SqueezeNet} Train Epoch: 16 [35840/37814 (95%)]\tLoss: 1.903339\n",
            "{SqueezeNet} Train Epoch: 16 [36352/37814 (96%)]\tLoss: 1.857047\n",
            "{SqueezeNet} Train Epoch: 16 [36864/37814 (97%)]\tLoss: 1.812748\n",
            "{SqueezeNet} Train Epoch: 16 [31974/37814 (99%)]\tLoss: 1.818079\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8250, Accuracy: 1569/5000 (31%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.532550811767578 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a21a2b60-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"807e9388-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_692d604ea9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a21c33ec-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_ce93be000d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a21c74f6-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_ed0b76566b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a21cacaa-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a21c74f6-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_3ea431ffe7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a243792a-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a21c33ec-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cb491ebefe"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a2452ac2-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_3b0e25ab8d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a245880a-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_4afa12cfc8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a245e250-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a245880a-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f23860a7a2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 17 [0/37814 (0%)]\tLoss: 2.018233\n",
            "{AlexNet} Train Epoch: 17 [512/37814 (1%)]\tLoss: 2.004165\n",
            "{AlexNet} Train Epoch: 17 [1024/37814 (3%)]\tLoss: 1.990276\n",
            "{AlexNet} Train Epoch: 17 [1536/37814 (4%)]\tLoss: 1.990417\n",
            "{AlexNet} Train Epoch: 17 [2048/37814 (5%)]\tLoss: 1.994466\n",
            "{AlexNet} Train Epoch: 17 [2560/37814 (7%)]\tLoss: 2.019832\n",
            "{AlexNet} Train Epoch: 17 [3072/37814 (8%)]\tLoss: 1.996403\n",
            "{AlexNet} Train Epoch: 17 [3584/37814 (9%)]\tLoss: 2.005082\n",
            "{AlexNet} Train Epoch: 17 [4096/37814 (11%)]\tLoss: 2.008165\n",
            "{AlexNet} Train Epoch: 17 [4608/37814 (12%)]\tLoss: 1.990480\n",
            "{AlexNet} Train Epoch: 17 [5120/37814 (14%)]\tLoss: 2.016295\n",
            "{AlexNet} Train Epoch: 17 [5632/37814 (15%)]\tLoss: 1.968299\n",
            "{AlexNet} Train Epoch: 17 [6144/37814 (16%)]\tLoss: 2.023111\n",
            "{AlexNet} Train Epoch: 17 [6656/37814 (18%)]\tLoss: 2.058048\n",
            "{AlexNet} Train Epoch: 17 [7168/37814 (19%)]\tLoss: 1.979278\n",
            "{AlexNet} Train Epoch: 17 [7680/37814 (20%)]\tLoss: 2.008833\n",
            "{AlexNet} Train Epoch: 17 [8192/37814 (22%)]\tLoss: 1.936257\n",
            "{AlexNet} Train Epoch: 17 [8704/37814 (23%)]\tLoss: 1.948455\n",
            "{AlexNet} Train Epoch: 17 [9216/37814 (24%)]\tLoss: 1.969797\n",
            "{AlexNet} Train Epoch: 17 [9728/37814 (26%)]\tLoss: 1.976139\n",
            "{AlexNet} Train Epoch: 17 [10240/37814 (27%)]\tLoss: 2.005697\n",
            "{AlexNet} Train Epoch: 17 [10752/37814 (28%)]\tLoss: 2.019932\n",
            "{AlexNet} Train Epoch: 17 [11264/37814 (30%)]\tLoss: 1.961532\n",
            "{AlexNet} Train Epoch: 17 [11776/37814 (31%)]\tLoss: 1.999605\n",
            "{AlexNet} Train Epoch: 17 [12288/37814 (32%)]\tLoss: 1.988526\n",
            "{AlexNet} Train Epoch: 17 [12800/37814 (34%)]\tLoss: 1.969420\n",
            "{AlexNet} Train Epoch: 17 [13312/37814 (35%)]\tLoss: 1.960122\n",
            "{AlexNet} Train Epoch: 17 [13824/37814 (36%)]\tLoss: 1.973909\n",
            "{AlexNet} Train Epoch: 17 [14336/37814 (38%)]\tLoss: 1.982405\n",
            "{AlexNet} Train Epoch: 17 [14848/37814 (39%)]\tLoss: 1.991387\n",
            "{AlexNet} Train Epoch: 17 [15360/37814 (41%)]\tLoss: 2.005097\n",
            "{AlexNet} Train Epoch: 17 [15872/37814 (42%)]\tLoss: 2.021482\n",
            "{AlexNet} Train Epoch: 17 [16384/37814 (43%)]\tLoss: 1.972201\n",
            "{AlexNet} Train Epoch: 17 [16896/37814 (45%)]\tLoss: 1.981403\n",
            "{AlexNet} Train Epoch: 17 [17408/37814 (46%)]\tLoss: 1.986234\n",
            "{AlexNet} Train Epoch: 17 [17920/37814 (47%)]\tLoss: 2.021183\n",
            "{AlexNet} Train Epoch: 17 [18432/37814 (49%)]\tLoss: 1.969981\n",
            "{AlexNet} Train Epoch: 17 [18944/37814 (50%)]\tLoss: 1.972544\n",
            "{AlexNet} Train Epoch: 17 [19456/37814 (51%)]\tLoss: 1.974242\n",
            "{AlexNet} Train Epoch: 17 [19968/37814 (53%)]\tLoss: 1.976756\n",
            "{AlexNet} Train Epoch: 17 [20480/37814 (54%)]\tLoss: 2.029190\n",
            "{AlexNet} Train Epoch: 17 [20992/37814 (55%)]\tLoss: 1.993735\n",
            "{AlexNet} Train Epoch: 17 [21504/37814 (57%)]\tLoss: 1.967471\n",
            "{AlexNet} Train Epoch: 17 [22016/37814 (58%)]\tLoss: 2.003414\n",
            "{AlexNet} Train Epoch: 17 [22528/37814 (59%)]\tLoss: 1.984298\n",
            "{AlexNet} Train Epoch: 17 [23040/37814 (61%)]\tLoss: 1.968844\n",
            "{AlexNet} Train Epoch: 17 [23552/37814 (62%)]\tLoss: 1.972703\n",
            "{AlexNet} Train Epoch: 17 [24064/37814 (64%)]\tLoss: 2.020415\n",
            "{AlexNet} Train Epoch: 17 [24576/37814 (65%)]\tLoss: 1.982650\n",
            "{AlexNet} Train Epoch: 17 [25088/37814 (66%)]\tLoss: 2.011792\n",
            "{AlexNet} Train Epoch: 17 [25600/37814 (68%)]\tLoss: 1.975082\n",
            "{AlexNet} Train Epoch: 17 [26112/37814 (69%)]\tLoss: 1.958016\n",
            "{AlexNet} Train Epoch: 17 [26624/37814 (70%)]\tLoss: 2.005621\n",
            "{AlexNet} Train Epoch: 17 [27136/37814 (72%)]\tLoss: 1.917177\n",
            "{AlexNet} Train Epoch: 17 [27648/37814 (73%)]\tLoss: 1.955599\n",
            "{AlexNet} Train Epoch: 17 [28160/37814 (74%)]\tLoss: 1.997969\n",
            "{AlexNet} Train Epoch: 17 [28672/37814 (76%)]\tLoss: 1.979656\n",
            "{AlexNet} Train Epoch: 17 [29184/37814 (77%)]\tLoss: 2.000036\n",
            "{AlexNet} Train Epoch: 17 [29696/37814 (78%)]\tLoss: 2.028748\n",
            "{AlexNet} Train Epoch: 17 [30208/37814 (80%)]\tLoss: 1.960281\n",
            "{AlexNet} Train Epoch: 17 [30720/37814 (81%)]\tLoss: 1.975897\n",
            "{AlexNet} Train Epoch: 17 [31232/37814 (82%)]\tLoss: 1.984594\n",
            "{AlexNet} Train Epoch: 17 [31744/37814 (84%)]\tLoss: 1.957671\n",
            "{AlexNet} Train Epoch: 17 [32256/37814 (85%)]\tLoss: 2.014328\n",
            "{AlexNet} Train Epoch: 17 [32768/37814 (86%)]\tLoss: 2.002603\n",
            "{AlexNet} Train Epoch: 17 [33280/37814 (88%)]\tLoss: 1.939310\n",
            "{AlexNet} Train Epoch: 17 [33792/37814 (89%)]\tLoss: 1.959932\n",
            "{AlexNet} Train Epoch: 17 [34304/37814 (91%)]\tLoss: 1.989139\n",
            "{AlexNet} Train Epoch: 17 [34816/37814 (92%)]\tLoss: 2.018805\n",
            "{AlexNet} Train Epoch: 17 [35328/37814 (93%)]\tLoss: 2.005596\n",
            "{AlexNet} Train Epoch: 17 [35840/37814 (95%)]\tLoss: 1.981123\n",
            "{AlexNet} Train Epoch: 17 [36352/37814 (96%)]\tLoss: 1.995046\n",
            "{AlexNet} Train Epoch: 17 [36864/37814 (97%)]\tLoss: 1.977086\n",
            "{AlexNet} Train Epoch: 17 [31974/37814 (99%)]\tLoss: 2.004448\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9885, Accuracy: 997/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.81159496307373 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 17 [0/37814 (0%)]\tLoss: 1.871223\n",
            "{SqueezeNet} Train Epoch: 17 [512/37814 (1%)]\tLoss: 1.855002\n",
            "{SqueezeNet} Train Epoch: 17 [1024/37814 (3%)]\tLoss: 1.832728\n",
            "{SqueezeNet} Train Epoch: 17 [1536/37814 (4%)]\tLoss: 1.817080\n",
            "{SqueezeNet} Train Epoch: 17 [2048/37814 (5%)]\tLoss: 1.850400\n",
            "{SqueezeNet} Train Epoch: 17 [2560/37814 (7%)]\tLoss: 1.774073\n",
            "{SqueezeNet} Train Epoch: 17 [3072/37814 (8%)]\tLoss: 1.821029\n",
            "{SqueezeNet} Train Epoch: 17 [3584/37814 (9%)]\tLoss: 1.868211\n",
            "{SqueezeNet} Train Epoch: 17 [4096/37814 (11%)]\tLoss: 1.856444\n",
            "{SqueezeNet} Train Epoch: 17 [4608/37814 (12%)]\tLoss: 1.902906\n",
            "{SqueezeNet} Train Epoch: 17 [5120/37814 (14%)]\tLoss: 1.872469\n",
            "{SqueezeNet} Train Epoch: 17 [5632/37814 (15%)]\tLoss: 1.876493\n",
            "{SqueezeNet} Train Epoch: 17 [6144/37814 (16%)]\tLoss: 1.886445\n",
            "{SqueezeNet} Train Epoch: 17 [6656/37814 (18%)]\tLoss: 1.810025\n",
            "{SqueezeNet} Train Epoch: 17 [7168/37814 (19%)]\tLoss: 1.783713\n",
            "{SqueezeNet} Train Epoch: 17 [7680/37814 (20%)]\tLoss: 1.802405\n",
            "{SqueezeNet} Train Epoch: 17 [8192/37814 (22%)]\tLoss: 1.827899\n",
            "{SqueezeNet} Train Epoch: 17 [8704/37814 (23%)]\tLoss: 1.824473\n",
            "{SqueezeNet} Train Epoch: 17 [9216/37814 (24%)]\tLoss: 1.888511\n",
            "{SqueezeNet} Train Epoch: 17 [9728/37814 (26%)]\tLoss: 1.906588\n",
            "{SqueezeNet} Train Epoch: 17 [10240/37814 (27%)]\tLoss: 1.804960\n",
            "{SqueezeNet} Train Epoch: 17 [10752/37814 (28%)]\tLoss: 1.867813\n",
            "{SqueezeNet} Train Epoch: 17 [11264/37814 (30%)]\tLoss: 1.879690\n",
            "{SqueezeNet} Train Epoch: 17 [11776/37814 (31%)]\tLoss: 1.899430\n",
            "{SqueezeNet} Train Epoch: 17 [12288/37814 (32%)]\tLoss: 1.847665\n",
            "{SqueezeNet} Train Epoch: 17 [12800/37814 (34%)]\tLoss: 1.863678\n",
            "{SqueezeNet} Train Epoch: 17 [13312/37814 (35%)]\tLoss: 1.862984\n",
            "{SqueezeNet} Train Epoch: 17 [13824/37814 (36%)]\tLoss: 1.892680\n",
            "{SqueezeNet} Train Epoch: 17 [14336/37814 (38%)]\tLoss: 1.857978\n",
            "{SqueezeNet} Train Epoch: 17 [14848/37814 (39%)]\tLoss: 1.884523\n",
            "{SqueezeNet} Train Epoch: 17 [15360/37814 (41%)]\tLoss: 1.838234\n",
            "{SqueezeNet} Train Epoch: 17 [15872/37814 (42%)]\tLoss: 1.860839\n",
            "{SqueezeNet} Train Epoch: 17 [16384/37814 (43%)]\tLoss: 1.818793\n",
            "{SqueezeNet} Train Epoch: 17 [16896/37814 (45%)]\tLoss: 1.869718\n",
            "{SqueezeNet} Train Epoch: 17 [17408/37814 (46%)]\tLoss: 1.822596\n",
            "{SqueezeNet} Train Epoch: 17 [17920/37814 (47%)]\tLoss: 1.857449\n",
            "{SqueezeNet} Train Epoch: 17 [18432/37814 (49%)]\tLoss: 1.850183\n",
            "{SqueezeNet} Train Epoch: 17 [18944/37814 (50%)]\tLoss: 1.784982\n",
            "{SqueezeNet} Train Epoch: 17 [19456/37814 (51%)]\tLoss: 1.840361\n",
            "{SqueezeNet} Train Epoch: 17 [19968/37814 (53%)]\tLoss: 1.821838\n",
            "{SqueezeNet} Train Epoch: 17 [20480/37814 (54%)]\tLoss: 1.858444\n",
            "{SqueezeNet} Train Epoch: 17 [20992/37814 (55%)]\tLoss: 1.821069\n",
            "{SqueezeNet} Train Epoch: 17 [21504/37814 (57%)]\tLoss: 1.877473\n",
            "{SqueezeNet} Train Epoch: 17 [22016/37814 (58%)]\tLoss: 1.824707\n",
            "{SqueezeNet} Train Epoch: 17 [22528/37814 (59%)]\tLoss: 1.848576\n",
            "{SqueezeNet} Train Epoch: 17 [23040/37814 (61%)]\tLoss: 1.875801\n",
            "{SqueezeNet} Train Epoch: 17 [23552/37814 (62%)]\tLoss: 1.884657\n",
            "{SqueezeNet} Train Epoch: 17 [24064/37814 (64%)]\tLoss: 1.800039\n",
            "{SqueezeNet} Train Epoch: 17 [24576/37814 (65%)]\tLoss: 1.808809\n",
            "{SqueezeNet} Train Epoch: 17 [25088/37814 (66%)]\tLoss: 1.854597\n",
            "{SqueezeNet} Train Epoch: 17 [25600/37814 (68%)]\tLoss: 1.826577\n",
            "{SqueezeNet} Train Epoch: 17 [26112/37814 (69%)]\tLoss: 1.815399\n",
            "{SqueezeNet} Train Epoch: 17 [26624/37814 (70%)]\tLoss: 1.826147\n",
            "{SqueezeNet} Train Epoch: 17 [27136/37814 (72%)]\tLoss: 1.888472\n",
            "{SqueezeNet} Train Epoch: 17 [27648/37814 (73%)]\tLoss: 1.807529\n",
            "{SqueezeNet} Train Epoch: 17 [28160/37814 (74%)]\tLoss: 1.776311\n",
            "{SqueezeNet} Train Epoch: 17 [28672/37814 (76%)]\tLoss: 1.896534\n",
            "{SqueezeNet} Train Epoch: 17 [29184/37814 (77%)]\tLoss: 1.882516\n",
            "{SqueezeNet} Train Epoch: 17 [29696/37814 (78%)]\tLoss: 1.879662\n",
            "{SqueezeNet} Train Epoch: 17 [30208/37814 (80%)]\tLoss: 1.809578\n",
            "{SqueezeNet} Train Epoch: 17 [30720/37814 (81%)]\tLoss: 1.810852\n",
            "{SqueezeNet} Train Epoch: 17 [31232/37814 (82%)]\tLoss: 1.853833\n",
            "{SqueezeNet} Train Epoch: 17 [31744/37814 (84%)]\tLoss: 1.805053\n",
            "{SqueezeNet} Train Epoch: 17 [32256/37814 (85%)]\tLoss: 1.863515\n",
            "{SqueezeNet} Train Epoch: 17 [32768/37814 (86%)]\tLoss: 1.831037\n",
            "{SqueezeNet} Train Epoch: 17 [33280/37814 (88%)]\tLoss: 1.853848\n",
            "{SqueezeNet} Train Epoch: 17 [33792/37814 (89%)]\tLoss: 1.861185\n",
            "{SqueezeNet} Train Epoch: 17 [34304/37814 (91%)]\tLoss: 1.844787\n",
            "{SqueezeNet} Train Epoch: 17 [34816/37814 (92%)]\tLoss: 1.918358\n",
            "{SqueezeNet} Train Epoch: 17 [35328/37814 (93%)]\tLoss: 1.832523\n",
            "{SqueezeNet} Train Epoch: 17 [35840/37814 (95%)]\tLoss: 1.860466\n",
            "{SqueezeNet} Train Epoch: 17 [36352/37814 (96%)]\tLoss: 1.809670\n",
            "{SqueezeNet} Train Epoch: 17 [36864/37814 (97%)]\tLoss: 1.834433\n",
            "{SqueezeNet} Train Epoch: 17 [31974/37814 (99%)]\tLoss: 1.816407\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8316, Accuracy: 1587/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.48448920249939 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c3d5b936-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a2452ac2-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_99700952d4"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c3d83ff8-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_3e3e842951"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c3d8ad26-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_28ec37acfd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c3d93a48-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"c3d8ad26-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f6cc511b31"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c3fefa1c-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"c3d83ff8-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_3e1467e1e9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c40099da-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_7fa0f4adc9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c400db16-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_5ef64edc0f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"c401198c-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"c400db16-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f3f76da161"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 18 [0/37814 (0%)]\tLoss: 1.981762\n",
            "{AlexNet} Train Epoch: 18 [512/37814 (1%)]\tLoss: 1.989118\n",
            "{AlexNet} Train Epoch: 18 [1024/37814 (3%)]\tLoss: 1.992229\n",
            "{AlexNet} Train Epoch: 18 [1536/37814 (4%)]\tLoss: 2.063287\n",
            "{AlexNet} Train Epoch: 18 [2048/37814 (5%)]\tLoss: 1.975857\n",
            "{AlexNet} Train Epoch: 18 [2560/37814 (7%)]\tLoss: 1.983801\n",
            "{AlexNet} Train Epoch: 18 [3072/37814 (8%)]\tLoss: 1.989070\n",
            "{AlexNet} Train Epoch: 18 [3584/37814 (9%)]\tLoss: 1.998583\n",
            "{AlexNet} Train Epoch: 18 [4096/37814 (11%)]\tLoss: 2.030037\n",
            "{AlexNet} Train Epoch: 18 [4608/37814 (12%)]\tLoss: 2.036814\n",
            "{AlexNet} Train Epoch: 18 [5120/37814 (14%)]\tLoss: 1.960823\n",
            "{AlexNet} Train Epoch: 18 [5632/37814 (15%)]\tLoss: 1.968631\n",
            "{AlexNet} Train Epoch: 18 [6144/37814 (16%)]\tLoss: 1.937658\n",
            "{AlexNet} Train Epoch: 18 [6656/37814 (18%)]\tLoss: 2.008832\n",
            "{AlexNet} Train Epoch: 18 [7168/37814 (19%)]\tLoss: 1.944257\n",
            "{AlexNet} Train Epoch: 18 [7680/37814 (20%)]\tLoss: 1.955918\n",
            "{AlexNet} Train Epoch: 18 [8192/37814 (22%)]\tLoss: 2.002251\n",
            "{AlexNet} Train Epoch: 18 [8704/37814 (23%)]\tLoss: 1.971032\n",
            "{AlexNet} Train Epoch: 18 [9216/37814 (24%)]\tLoss: 1.978039\n",
            "{AlexNet} Train Epoch: 18 [9728/37814 (26%)]\tLoss: 1.963631\n",
            "{AlexNet} Train Epoch: 18 [10240/37814 (27%)]\tLoss: 1.988163\n",
            "{AlexNet} Train Epoch: 18 [10752/37814 (28%)]\tLoss: 1.981861\n",
            "{AlexNet} Train Epoch: 18 [11264/37814 (30%)]\tLoss: 1.989264\n",
            "{AlexNet} Train Epoch: 18 [11776/37814 (31%)]\tLoss: 1.997285\n",
            "{AlexNet} Train Epoch: 18 [12288/37814 (32%)]\tLoss: 1.985435\n",
            "{AlexNet} Train Epoch: 18 [12800/37814 (34%)]\tLoss: 1.975986\n",
            "{AlexNet} Train Epoch: 18 [13312/37814 (35%)]\tLoss: 2.004078\n",
            "{AlexNet} Train Epoch: 18 [13824/37814 (36%)]\tLoss: 1.985195\n",
            "{AlexNet} Train Epoch: 18 [14336/37814 (38%)]\tLoss: 2.000206\n",
            "{AlexNet} Train Epoch: 18 [14848/37814 (39%)]\tLoss: 1.992415\n",
            "{AlexNet} Train Epoch: 18 [15360/37814 (41%)]\tLoss: 1.962431\n",
            "{AlexNet} Train Epoch: 18 [15872/37814 (42%)]\tLoss: 1.973687\n",
            "{AlexNet} Train Epoch: 18 [16384/37814 (43%)]\tLoss: 1.955922\n",
            "{AlexNet} Train Epoch: 18 [16896/37814 (45%)]\tLoss: 1.988080\n",
            "{AlexNet} Train Epoch: 18 [17408/37814 (46%)]\tLoss: 1.939877\n",
            "{AlexNet} Train Epoch: 18 [17920/37814 (47%)]\tLoss: 1.927745\n",
            "{AlexNet} Train Epoch: 18 [18432/37814 (49%)]\tLoss: 2.004968\n",
            "{AlexNet} Train Epoch: 18 [18944/37814 (50%)]\tLoss: 1.980054\n",
            "{AlexNet} Train Epoch: 18 [19456/37814 (51%)]\tLoss: 1.990647\n",
            "{AlexNet} Train Epoch: 18 [19968/37814 (53%)]\tLoss: 1.983453\n",
            "{AlexNet} Train Epoch: 18 [20480/37814 (54%)]\tLoss: 1.949147\n",
            "{AlexNet} Train Epoch: 18 [20992/37814 (55%)]\tLoss: 1.958956\n",
            "{AlexNet} Train Epoch: 18 [21504/37814 (57%)]\tLoss: 1.962249\n",
            "{AlexNet} Train Epoch: 18 [22016/37814 (58%)]\tLoss: 2.007346\n",
            "{AlexNet} Train Epoch: 18 [22528/37814 (59%)]\tLoss: 2.002208\n",
            "{AlexNet} Train Epoch: 18 [23040/37814 (61%)]\tLoss: 1.976744\n",
            "{AlexNet} Train Epoch: 18 [23552/37814 (62%)]\tLoss: 2.038244\n",
            "{AlexNet} Train Epoch: 18 [24064/37814 (64%)]\tLoss: 1.956681\n",
            "{AlexNet} Train Epoch: 18 [24576/37814 (65%)]\tLoss: 2.010750\n",
            "{AlexNet} Train Epoch: 18 [25088/37814 (66%)]\tLoss: 1.990227\n",
            "{AlexNet} Train Epoch: 18 [25600/37814 (68%)]\tLoss: 2.006464\n",
            "{AlexNet} Train Epoch: 18 [26112/37814 (69%)]\tLoss: 1.955884\n",
            "{AlexNet} Train Epoch: 18 [26624/37814 (70%)]\tLoss: 1.986218\n",
            "{AlexNet} Train Epoch: 18 [27136/37814 (72%)]\tLoss: 2.017208\n",
            "{AlexNet} Train Epoch: 18 [27648/37814 (73%)]\tLoss: 1.969608\n",
            "{AlexNet} Train Epoch: 18 [28160/37814 (74%)]\tLoss: 1.950993\n",
            "{AlexNet} Train Epoch: 18 [28672/37814 (76%)]\tLoss: 2.005990\n",
            "{AlexNet} Train Epoch: 18 [29184/37814 (77%)]\tLoss: 1.970133\n",
            "{AlexNet} Train Epoch: 18 [29696/37814 (78%)]\tLoss: 1.973102\n",
            "{AlexNet} Train Epoch: 18 [30208/37814 (80%)]\tLoss: 1.990851\n",
            "{AlexNet} Train Epoch: 18 [30720/37814 (81%)]\tLoss: 1.975785\n",
            "{AlexNet} Train Epoch: 18 [31232/37814 (82%)]\tLoss: 1.989302\n",
            "{AlexNet} Train Epoch: 18 [31744/37814 (84%)]\tLoss: 2.005474\n",
            "{AlexNet} Train Epoch: 18 [32256/37814 (85%)]\tLoss: 2.019089\n",
            "{AlexNet} Train Epoch: 18 [32768/37814 (86%)]\tLoss: 1.993585\n",
            "{AlexNet} Train Epoch: 18 [33280/37814 (88%)]\tLoss: 1.977313\n",
            "{AlexNet} Train Epoch: 18 [33792/37814 (89%)]\tLoss: 1.948097\n",
            "{AlexNet} Train Epoch: 18 [34304/37814 (91%)]\tLoss: 2.022213\n",
            "{AlexNet} Train Epoch: 18 [34816/37814 (92%)]\tLoss: 1.992179\n",
            "{AlexNet} Train Epoch: 18 [35328/37814 (93%)]\tLoss: 1.985409\n",
            "{AlexNet} Train Epoch: 18 [35840/37814 (95%)]\tLoss: 1.986783\n",
            "{AlexNet} Train Epoch: 18 [36352/37814 (96%)]\tLoss: 1.999698\n",
            "{AlexNet} Train Epoch: 18 [36864/37814 (97%)]\tLoss: 1.986643\n",
            "{AlexNet} Train Epoch: 18 [31974/37814 (99%)]\tLoss: 2.010231\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9805, Accuracy: 1025/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.42712140083313 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 18 [0/37814 (0%)]\tLoss: 1.885439\n",
            "{SqueezeNet} Train Epoch: 18 [512/37814 (1%)]\tLoss: 1.852451\n",
            "{SqueezeNet} Train Epoch: 18 [1024/37814 (3%)]\tLoss: 1.859020\n",
            "{SqueezeNet} Train Epoch: 18 [1536/37814 (4%)]\tLoss: 1.803698\n",
            "{SqueezeNet} Train Epoch: 18 [2048/37814 (5%)]\tLoss: 1.783630\n",
            "{SqueezeNet} Train Epoch: 18 [2560/37814 (7%)]\tLoss: 1.840117\n",
            "{SqueezeNet} Train Epoch: 18 [3072/37814 (8%)]\tLoss: 1.851951\n",
            "{SqueezeNet} Train Epoch: 18 [3584/37814 (9%)]\tLoss: 1.854621\n",
            "{SqueezeNet} Train Epoch: 18 [4096/37814 (11%)]\tLoss: 1.858583\n",
            "{SqueezeNet} Train Epoch: 18 [4608/37814 (12%)]\tLoss: 1.856462\n",
            "{SqueezeNet} Train Epoch: 18 [5120/37814 (14%)]\tLoss: 1.872273\n",
            "{SqueezeNet} Train Epoch: 18 [5632/37814 (15%)]\tLoss: 1.789640\n",
            "{SqueezeNet} Train Epoch: 18 [6144/37814 (16%)]\tLoss: 1.844974\n",
            "{SqueezeNet} Train Epoch: 18 [6656/37814 (18%)]\tLoss: 1.823504\n",
            "{SqueezeNet} Train Epoch: 18 [7168/37814 (19%)]\tLoss: 1.859984\n",
            "{SqueezeNet} Train Epoch: 18 [7680/37814 (20%)]\tLoss: 1.794248\n",
            "{SqueezeNet} Train Epoch: 18 [8192/37814 (22%)]\tLoss: 1.890149\n",
            "{SqueezeNet} Train Epoch: 18 [8704/37814 (23%)]\tLoss: 1.793776\n",
            "{SqueezeNet} Train Epoch: 18 [9216/37814 (24%)]\tLoss: 1.805713\n",
            "{SqueezeNet} Train Epoch: 18 [9728/37814 (26%)]\tLoss: 1.783429\n",
            "{SqueezeNet} Train Epoch: 18 [10240/37814 (27%)]\tLoss: 1.848741\n",
            "{SqueezeNet} Train Epoch: 18 [10752/37814 (28%)]\tLoss: 1.864777\n",
            "{SqueezeNet} Train Epoch: 18 [11264/37814 (30%)]\tLoss: 1.907675\n",
            "{SqueezeNet} Train Epoch: 18 [11776/37814 (31%)]\tLoss: 1.829288\n",
            "{SqueezeNet} Train Epoch: 18 [12288/37814 (32%)]\tLoss: 1.835276\n",
            "{SqueezeNet} Train Epoch: 18 [12800/37814 (34%)]\tLoss: 1.854354\n",
            "{SqueezeNet} Train Epoch: 18 [13312/37814 (35%)]\tLoss: 1.881709\n",
            "{SqueezeNet} Train Epoch: 18 [13824/37814 (36%)]\tLoss: 1.861686\n",
            "{SqueezeNet} Train Epoch: 18 [14336/37814 (38%)]\tLoss: 1.858604\n",
            "{SqueezeNet} Train Epoch: 18 [14848/37814 (39%)]\tLoss: 1.808784\n",
            "{SqueezeNet} Train Epoch: 18 [15360/37814 (41%)]\tLoss: 1.863795\n",
            "{SqueezeNet} Train Epoch: 18 [15872/37814 (42%)]\tLoss: 1.847156\n",
            "{SqueezeNet} Train Epoch: 18 [16384/37814 (43%)]\tLoss: 1.768888\n",
            "{SqueezeNet} Train Epoch: 18 [16896/37814 (45%)]\tLoss: 1.768256\n",
            "{SqueezeNet} Train Epoch: 18 [17408/37814 (46%)]\tLoss: 1.855311\n",
            "{SqueezeNet} Train Epoch: 18 [17920/37814 (47%)]\tLoss: 1.824323\n",
            "{SqueezeNet} Train Epoch: 18 [18432/37814 (49%)]\tLoss: 1.844593\n",
            "{SqueezeNet} Train Epoch: 18 [18944/37814 (50%)]\tLoss: 1.858841\n",
            "{SqueezeNet} Train Epoch: 18 [19456/37814 (51%)]\tLoss: 1.820495\n",
            "{SqueezeNet} Train Epoch: 18 [19968/37814 (53%)]\tLoss: 1.853120\n",
            "{SqueezeNet} Train Epoch: 18 [20480/37814 (54%)]\tLoss: 1.850963\n",
            "{SqueezeNet} Train Epoch: 18 [20992/37814 (55%)]\tLoss: 1.836794\n",
            "{SqueezeNet} Train Epoch: 18 [21504/37814 (57%)]\tLoss: 1.852063\n",
            "{SqueezeNet} Train Epoch: 18 [22016/37814 (58%)]\tLoss: 1.880715\n",
            "{SqueezeNet} Train Epoch: 18 [22528/37814 (59%)]\tLoss: 1.786721\n",
            "{SqueezeNet} Train Epoch: 18 [23040/37814 (61%)]\tLoss: 1.840328\n",
            "{SqueezeNet} Train Epoch: 18 [23552/37814 (62%)]\tLoss: 1.849179\n",
            "{SqueezeNet} Train Epoch: 18 [24064/37814 (64%)]\tLoss: 1.856237\n",
            "{SqueezeNet} Train Epoch: 18 [24576/37814 (65%)]\tLoss: 1.828529\n",
            "{SqueezeNet} Train Epoch: 18 [25088/37814 (66%)]\tLoss: 1.834181\n",
            "{SqueezeNet} Train Epoch: 18 [25600/37814 (68%)]\tLoss: 1.832933\n",
            "{SqueezeNet} Train Epoch: 18 [26112/37814 (69%)]\tLoss: 1.795122\n",
            "{SqueezeNet} Train Epoch: 18 [26624/37814 (70%)]\tLoss: 1.875169\n",
            "{SqueezeNet} Train Epoch: 18 [27136/37814 (72%)]\tLoss: 1.881309\n",
            "{SqueezeNet} Train Epoch: 18 [27648/37814 (73%)]\tLoss: 1.846553\n",
            "{SqueezeNet} Train Epoch: 18 [28160/37814 (74%)]\tLoss: 1.817929\n",
            "{SqueezeNet} Train Epoch: 18 [28672/37814 (76%)]\tLoss: 1.755392\n",
            "{SqueezeNet} Train Epoch: 18 [29184/37814 (77%)]\tLoss: 1.845533\n",
            "{SqueezeNet} Train Epoch: 18 [29696/37814 (78%)]\tLoss: 1.833408\n",
            "{SqueezeNet} Train Epoch: 18 [30208/37814 (80%)]\tLoss: 1.881259\n",
            "{SqueezeNet} Train Epoch: 18 [30720/37814 (81%)]\tLoss: 1.791576\n",
            "{SqueezeNet} Train Epoch: 18 [31232/37814 (82%)]\tLoss: 1.795925\n",
            "{SqueezeNet} Train Epoch: 18 [31744/37814 (84%)]\tLoss: 1.851192\n",
            "{SqueezeNet} Train Epoch: 18 [32256/37814 (85%)]\tLoss: 1.835615\n",
            "{SqueezeNet} Train Epoch: 18 [32768/37814 (86%)]\tLoss: 1.883657\n",
            "{SqueezeNet} Train Epoch: 18 [33280/37814 (88%)]\tLoss: 1.863915\n",
            "{SqueezeNet} Train Epoch: 18 [33792/37814 (89%)]\tLoss: 1.867923\n",
            "{SqueezeNet} Train Epoch: 18 [34304/37814 (91%)]\tLoss: 1.845376\n",
            "{SqueezeNet} Train Epoch: 18 [34816/37814 (92%)]\tLoss: 1.773675\n",
            "{SqueezeNet} Train Epoch: 18 [35328/37814 (93%)]\tLoss: 1.848109\n",
            "{SqueezeNet} Train Epoch: 18 [35840/37814 (95%)]\tLoss: 1.864532\n",
            "{SqueezeNet} Train Epoch: 18 [36352/37814 (96%)]\tLoss: 1.840842\n",
            "{SqueezeNet} Train Epoch: 18 [36864/37814 (97%)]\tLoss: 1.871697\n",
            "{SqueezeNet} Train Epoch: 18 [31974/37814 (99%)]\tLoss: 1.826440\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8277, Accuracy: 1581/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.491756916046143 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e5569f4e-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"c40099da-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2b5854c14e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e5583e30-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_eadd52c1a0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e5589ad8-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_4f01c47e14"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e558e376-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"e5589ad8-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8e58869e44"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e57e51d8-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"e5583e30-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_69a9f33e55"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e57f8ff8-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_c699a2b21b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e57fcb94-60e4-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_7813eae8f5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"e5800a28-60e4-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"e57fcb94-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8c99f0042e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 19 [0/37814 (0%)]\tLoss: 1.976184\n",
            "{AlexNet} Train Epoch: 19 [512/37814 (1%)]\tLoss: 1.988113\n",
            "{AlexNet} Train Epoch: 19 [1024/37814 (3%)]\tLoss: 1.976394\n",
            "{AlexNet} Train Epoch: 19 [1536/37814 (4%)]\tLoss: 1.983562\n",
            "{AlexNet} Train Epoch: 19 [2048/37814 (5%)]\tLoss: 1.985929\n",
            "{AlexNet} Train Epoch: 19 [2560/37814 (7%)]\tLoss: 1.972906\n",
            "{AlexNet} Train Epoch: 19 [3072/37814 (8%)]\tLoss: 1.956901\n",
            "{AlexNet} Train Epoch: 19 [3584/37814 (9%)]\tLoss: 1.988874\n",
            "{AlexNet} Train Epoch: 19 [4096/37814 (11%)]\tLoss: 1.949443\n",
            "{AlexNet} Train Epoch: 19 [4608/37814 (12%)]\tLoss: 1.996374\n",
            "{AlexNet} Train Epoch: 19 [5120/37814 (14%)]\tLoss: 1.990074\n",
            "{AlexNet} Train Epoch: 19 [5632/37814 (15%)]\tLoss: 1.986671\n",
            "{AlexNet} Train Epoch: 19 [6144/37814 (16%)]\tLoss: 1.964766\n",
            "{AlexNet} Train Epoch: 19 [6656/37814 (18%)]\tLoss: 1.931787\n",
            "{AlexNet} Train Epoch: 19 [7168/37814 (19%)]\tLoss: 2.003530\n",
            "{AlexNet} Train Epoch: 19 [7680/37814 (20%)]\tLoss: 1.984675\n",
            "{AlexNet} Train Epoch: 19 [8192/37814 (22%)]\tLoss: 1.961151\n",
            "{AlexNet} Train Epoch: 19 [8704/37814 (23%)]\tLoss: 1.932375\n",
            "{AlexNet} Train Epoch: 19 [9216/37814 (24%)]\tLoss: 1.987929\n",
            "{AlexNet} Train Epoch: 19 [9728/37814 (26%)]\tLoss: 2.035478\n",
            "{AlexNet} Train Epoch: 19 [10240/37814 (27%)]\tLoss: 1.998961\n",
            "{AlexNet} Train Epoch: 19 [10752/37814 (28%)]\tLoss: 1.965315\n",
            "{AlexNet} Train Epoch: 19 [11264/37814 (30%)]\tLoss: 1.955009\n",
            "{AlexNet} Train Epoch: 19 [11776/37814 (31%)]\tLoss: 1.927359\n",
            "{AlexNet} Train Epoch: 19 [12288/37814 (32%)]\tLoss: 1.999083\n",
            "{AlexNet} Train Epoch: 19 [12800/37814 (34%)]\tLoss: 1.954898\n",
            "{AlexNet} Train Epoch: 19 [13312/37814 (35%)]\tLoss: 1.924679\n",
            "{AlexNet} Train Epoch: 19 [13824/37814 (36%)]\tLoss: 1.996818\n",
            "{AlexNet} Train Epoch: 19 [14336/37814 (38%)]\tLoss: 2.005676\n",
            "{AlexNet} Train Epoch: 19 [14848/37814 (39%)]\tLoss: 2.017603\n",
            "{AlexNet} Train Epoch: 19 [15360/37814 (41%)]\tLoss: 2.012239\n",
            "{AlexNet} Train Epoch: 19 [15872/37814 (42%)]\tLoss: 2.002564\n",
            "{AlexNet} Train Epoch: 19 [16384/37814 (43%)]\tLoss: 1.982463\n",
            "{AlexNet} Train Epoch: 19 [16896/37814 (45%)]\tLoss: 1.983932\n",
            "{AlexNet} Train Epoch: 19 [17408/37814 (46%)]\tLoss: 1.968228\n",
            "{AlexNet} Train Epoch: 19 [17920/37814 (47%)]\tLoss: 2.012835\n",
            "{AlexNet} Train Epoch: 19 [18432/37814 (49%)]\tLoss: 2.015954\n",
            "{AlexNet} Train Epoch: 19 [18944/37814 (50%)]\tLoss: 1.990394\n",
            "{AlexNet} Train Epoch: 19 [19456/37814 (51%)]\tLoss: 1.973686\n",
            "{AlexNet} Train Epoch: 19 [19968/37814 (53%)]\tLoss: 1.950099\n",
            "{AlexNet} Train Epoch: 19 [20480/37814 (54%)]\tLoss: 2.011907\n",
            "{AlexNet} Train Epoch: 19 [20992/37814 (55%)]\tLoss: 2.044476\n",
            "{AlexNet} Train Epoch: 19 [21504/37814 (57%)]\tLoss: 1.954606\n",
            "{AlexNet} Train Epoch: 19 [22016/37814 (58%)]\tLoss: 1.989998\n",
            "{AlexNet} Train Epoch: 19 [22528/37814 (59%)]\tLoss: 1.992349\n",
            "{AlexNet} Train Epoch: 19 [23040/37814 (61%)]\tLoss: 1.968341\n",
            "{AlexNet} Train Epoch: 19 [23552/37814 (62%)]\tLoss: 1.974257\n",
            "{AlexNet} Train Epoch: 19 [24064/37814 (64%)]\tLoss: 1.997723\n",
            "{AlexNet} Train Epoch: 19 [24576/37814 (65%)]\tLoss: 2.004957\n",
            "{AlexNet} Train Epoch: 19 [25088/37814 (66%)]\tLoss: 1.958330\n",
            "{AlexNet} Train Epoch: 19 [25600/37814 (68%)]\tLoss: 1.960628\n",
            "{AlexNet} Train Epoch: 19 [26112/37814 (69%)]\tLoss: 1.974376\n",
            "{AlexNet} Train Epoch: 19 [26624/37814 (70%)]\tLoss: 1.976188\n",
            "{AlexNet} Train Epoch: 19 [27136/37814 (72%)]\tLoss: 1.998551\n",
            "{AlexNet} Train Epoch: 19 [27648/37814 (73%)]\tLoss: 1.974981\n",
            "{AlexNet} Train Epoch: 19 [28160/37814 (74%)]\tLoss: 1.987741\n",
            "{AlexNet} Train Epoch: 19 [28672/37814 (76%)]\tLoss: 1.973336\n",
            "{AlexNet} Train Epoch: 19 [29184/37814 (77%)]\tLoss: 2.011650\n",
            "{AlexNet} Train Epoch: 19 [29696/37814 (78%)]\tLoss: 2.029729\n",
            "{AlexNet} Train Epoch: 19 [30208/37814 (80%)]\tLoss: 2.015253\n",
            "{AlexNet} Train Epoch: 19 [30720/37814 (81%)]\tLoss: 2.035514\n",
            "{AlexNet} Train Epoch: 19 [31232/37814 (82%)]\tLoss: 1.973708\n",
            "{AlexNet} Train Epoch: 19 [31744/37814 (84%)]\tLoss: 2.014318\n",
            "{AlexNet} Train Epoch: 19 [32256/37814 (85%)]\tLoss: 1.951078\n",
            "{AlexNet} Train Epoch: 19 [32768/37814 (86%)]\tLoss: 1.966002\n",
            "{AlexNet} Train Epoch: 19 [33280/37814 (88%)]\tLoss: 1.945309\n",
            "{AlexNet} Train Epoch: 19 [33792/37814 (89%)]\tLoss: 1.960488\n",
            "{AlexNet} Train Epoch: 19 [34304/37814 (91%)]\tLoss: 1.976121\n",
            "{AlexNet} Train Epoch: 19 [34816/37814 (92%)]\tLoss: 2.026067\n",
            "{AlexNet} Train Epoch: 19 [35328/37814 (93%)]\tLoss: 1.936430\n",
            "{AlexNet} Train Epoch: 19 [35840/37814 (95%)]\tLoss: 1.969113\n",
            "{AlexNet} Train Epoch: 19 [36352/37814 (96%)]\tLoss: 1.997934\n",
            "{AlexNet} Train Epoch: 19 [36864/37814 (97%)]\tLoss: 1.990330\n",
            "{AlexNet} Train Epoch: 19 [31974/37814 (99%)]\tLoss: 2.026582\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9716, Accuracy: 1042/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.87492609024048 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 19 [0/37814 (0%)]\tLoss: 1.845607\n",
            "{SqueezeNet} Train Epoch: 19 [512/37814 (1%)]\tLoss: 1.878120\n",
            "{SqueezeNet} Train Epoch: 19 [1024/37814 (3%)]\tLoss: 1.834758\n",
            "{SqueezeNet} Train Epoch: 19 [1536/37814 (4%)]\tLoss: 1.859095\n",
            "{SqueezeNet} Train Epoch: 19 [2048/37814 (5%)]\tLoss: 1.838828\n",
            "{SqueezeNet} Train Epoch: 19 [2560/37814 (7%)]\tLoss: 1.833285\n",
            "{SqueezeNet} Train Epoch: 19 [3072/37814 (8%)]\tLoss: 1.891839\n",
            "{SqueezeNet} Train Epoch: 19 [3584/37814 (9%)]\tLoss: 1.866611\n",
            "{SqueezeNet} Train Epoch: 19 [4096/37814 (11%)]\tLoss: 1.817628\n",
            "{SqueezeNet} Train Epoch: 19 [4608/37814 (12%)]\tLoss: 1.852068\n",
            "{SqueezeNet} Train Epoch: 19 [5120/37814 (14%)]\tLoss: 1.825339\n",
            "{SqueezeNet} Train Epoch: 19 [5632/37814 (15%)]\tLoss: 1.834710\n",
            "{SqueezeNet} Train Epoch: 19 [6144/37814 (16%)]\tLoss: 1.811043\n",
            "{SqueezeNet} Train Epoch: 19 [6656/37814 (18%)]\tLoss: 1.888835\n",
            "{SqueezeNet} Train Epoch: 19 [7168/37814 (19%)]\tLoss: 1.815127\n",
            "{SqueezeNet} Train Epoch: 19 [7680/37814 (20%)]\tLoss: 1.911808\n",
            "{SqueezeNet} Train Epoch: 19 [8192/37814 (22%)]\tLoss: 1.792743\n",
            "{SqueezeNet} Train Epoch: 19 [8704/37814 (23%)]\tLoss: 1.816485\n",
            "{SqueezeNet} Train Epoch: 19 [9216/37814 (24%)]\tLoss: 1.826550\n",
            "{SqueezeNet} Train Epoch: 19 [9728/37814 (26%)]\tLoss: 1.798695\n",
            "{SqueezeNet} Train Epoch: 19 [10240/37814 (27%)]\tLoss: 1.876537\n",
            "{SqueezeNet} Train Epoch: 19 [10752/37814 (28%)]\tLoss: 1.860316\n",
            "{SqueezeNet} Train Epoch: 19 [11264/37814 (30%)]\tLoss: 1.853486\n",
            "{SqueezeNet} Train Epoch: 19 [11776/37814 (31%)]\tLoss: 1.895348\n",
            "{SqueezeNet} Train Epoch: 19 [12288/37814 (32%)]\tLoss: 1.851948\n",
            "{SqueezeNet} Train Epoch: 19 [12800/37814 (34%)]\tLoss: 1.876753\n",
            "{SqueezeNet} Train Epoch: 19 [13312/37814 (35%)]\tLoss: 1.809966\n",
            "{SqueezeNet} Train Epoch: 19 [13824/37814 (36%)]\tLoss: 1.795189\n",
            "{SqueezeNet} Train Epoch: 19 [14336/37814 (38%)]\tLoss: 1.858916\n",
            "{SqueezeNet} Train Epoch: 19 [14848/37814 (39%)]\tLoss: 1.820506\n",
            "{SqueezeNet} Train Epoch: 19 [15360/37814 (41%)]\tLoss: 1.787973\n",
            "{SqueezeNet} Train Epoch: 19 [15872/37814 (42%)]\tLoss: 1.828549\n",
            "{SqueezeNet} Train Epoch: 19 [16384/37814 (43%)]\tLoss: 1.808007\n",
            "{SqueezeNet} Train Epoch: 19 [16896/37814 (45%)]\tLoss: 1.827122\n",
            "{SqueezeNet} Train Epoch: 19 [17408/37814 (46%)]\tLoss: 1.849039\n",
            "{SqueezeNet} Train Epoch: 19 [17920/37814 (47%)]\tLoss: 1.782724\n",
            "{SqueezeNet} Train Epoch: 19 [18432/37814 (49%)]\tLoss: 1.893642\n",
            "{SqueezeNet} Train Epoch: 19 [18944/37814 (50%)]\tLoss: 1.843342\n",
            "{SqueezeNet} Train Epoch: 19 [19456/37814 (51%)]\tLoss: 1.840970\n",
            "{SqueezeNet} Train Epoch: 19 [19968/37814 (53%)]\tLoss: 1.792554\n",
            "{SqueezeNet} Train Epoch: 19 [20480/37814 (54%)]\tLoss: 1.842082\n",
            "{SqueezeNet} Train Epoch: 19 [20992/37814 (55%)]\tLoss: 1.843283\n",
            "{SqueezeNet} Train Epoch: 19 [21504/37814 (57%)]\tLoss: 1.817460\n",
            "{SqueezeNet} Train Epoch: 19 [22016/37814 (58%)]\tLoss: 1.844083\n",
            "{SqueezeNet} Train Epoch: 19 [22528/37814 (59%)]\tLoss: 1.787726\n",
            "{SqueezeNet} Train Epoch: 19 [23040/37814 (61%)]\tLoss: 1.868408\n",
            "{SqueezeNet} Train Epoch: 19 [23552/37814 (62%)]\tLoss: 1.792442\n",
            "{SqueezeNet} Train Epoch: 19 [24064/37814 (64%)]\tLoss: 1.871670\n",
            "{SqueezeNet} Train Epoch: 19 [24576/37814 (65%)]\tLoss: 1.878345\n",
            "{SqueezeNet} Train Epoch: 19 [25088/37814 (66%)]\tLoss: 1.855285\n",
            "{SqueezeNet} Train Epoch: 19 [25600/37814 (68%)]\tLoss: 1.864087\n",
            "{SqueezeNet} Train Epoch: 19 [26112/37814 (69%)]\tLoss: 1.818928\n",
            "{SqueezeNet} Train Epoch: 19 [26624/37814 (70%)]\tLoss: 1.805895\n",
            "{SqueezeNet} Train Epoch: 19 [27136/37814 (72%)]\tLoss: 1.801846\n",
            "{SqueezeNet} Train Epoch: 19 [27648/37814 (73%)]\tLoss: 1.803959\n",
            "{SqueezeNet} Train Epoch: 19 [28160/37814 (74%)]\tLoss: 1.882756\n",
            "{SqueezeNet} Train Epoch: 19 [28672/37814 (76%)]\tLoss: 1.806206\n",
            "{SqueezeNet} Train Epoch: 19 [29184/37814 (77%)]\tLoss: 1.819938\n",
            "{SqueezeNet} Train Epoch: 19 [29696/37814 (78%)]\tLoss: 1.856662\n",
            "{SqueezeNet} Train Epoch: 19 [30208/37814 (80%)]\tLoss: 1.801412\n",
            "{SqueezeNet} Train Epoch: 19 [30720/37814 (81%)]\tLoss: 1.830152\n",
            "{SqueezeNet} Train Epoch: 19 [31232/37814 (82%)]\tLoss: 1.823272\n",
            "{SqueezeNet} Train Epoch: 19 [31744/37814 (84%)]\tLoss: 1.821882\n",
            "{SqueezeNet} Train Epoch: 19 [32256/37814 (85%)]\tLoss: 1.834186\n",
            "{SqueezeNet} Train Epoch: 19 [32768/37814 (86%)]\tLoss: 1.784950\n",
            "{SqueezeNet} Train Epoch: 19 [33280/37814 (88%)]\tLoss: 1.739293\n",
            "{SqueezeNet} Train Epoch: 19 [33792/37814 (89%)]\tLoss: 1.827852\n",
            "{SqueezeNet} Train Epoch: 19 [34304/37814 (91%)]\tLoss: 1.798335\n",
            "{SqueezeNet} Train Epoch: 19 [34816/37814 (92%)]\tLoss: 1.853958\n",
            "{SqueezeNet} Train Epoch: 19 [35328/37814 (93%)]\tLoss: 1.862340\n",
            "{SqueezeNet} Train Epoch: 19 [35840/37814 (95%)]\tLoss: 1.835650\n",
            "{SqueezeNet} Train Epoch: 19 [36352/37814 (96%)]\tLoss: 1.828193\n",
            "{SqueezeNet} Train Epoch: 19 [36864/37814 (97%)]\tLoss: 1.832679\n",
            "{SqueezeNet} Train Epoch: 19 [31974/37814 (99%)]\tLoss: 1.886981\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8081, Accuracy: 1615/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.667575120925903 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07cdb53a-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"e57f8ff8-60e4-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4b444d794a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07cfc668-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_d3905d913d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07d00c90-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_188798f77e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07d04fe8-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"07d00c90-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_747a097ffb"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07f3e5ca-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"07cfc668-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e90debf5f1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07f4c59e-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_820e268bd1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07f4f19a-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_ca94e48462"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"07f52cc8-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"07f4f19a-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b1286add38"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 20 [0/37814 (0%)]\tLoss: 2.002464\n",
            "{AlexNet} Train Epoch: 20 [512/37814 (1%)]\tLoss: 1.964025\n",
            "{AlexNet} Train Epoch: 20 [1024/37814 (3%)]\tLoss: 1.966204\n",
            "{AlexNet} Train Epoch: 20 [1536/37814 (4%)]\tLoss: 2.006392\n",
            "{AlexNet} Train Epoch: 20 [2048/37814 (5%)]\tLoss: 1.998730\n",
            "{AlexNet} Train Epoch: 20 [2560/37814 (7%)]\tLoss: 2.003937\n",
            "{AlexNet} Train Epoch: 20 [3072/37814 (8%)]\tLoss: 1.990140\n",
            "{AlexNet} Train Epoch: 20 [3584/37814 (9%)]\tLoss: 1.999348\n",
            "{AlexNet} Train Epoch: 20 [4096/37814 (11%)]\tLoss: 1.971778\n",
            "{AlexNet} Train Epoch: 20 [4608/37814 (12%)]\tLoss: 1.971744\n",
            "{AlexNet} Train Epoch: 20 [5120/37814 (14%)]\tLoss: 1.930286\n",
            "{AlexNet} Train Epoch: 20 [5632/37814 (15%)]\tLoss: 2.003530\n",
            "{AlexNet} Train Epoch: 20 [6144/37814 (16%)]\tLoss: 1.984761\n",
            "{AlexNet} Train Epoch: 20 [6656/37814 (18%)]\tLoss: 1.993864\n",
            "{AlexNet} Train Epoch: 20 [7168/37814 (19%)]\tLoss: 1.988946\n",
            "{AlexNet} Train Epoch: 20 [7680/37814 (20%)]\tLoss: 2.012267\n",
            "{AlexNet} Train Epoch: 20 [8192/37814 (22%)]\tLoss: 1.981389\n",
            "{AlexNet} Train Epoch: 20 [8704/37814 (23%)]\tLoss: 1.935290\n",
            "{AlexNet} Train Epoch: 20 [9216/37814 (24%)]\tLoss: 1.986793\n",
            "{AlexNet} Train Epoch: 20 [9728/37814 (26%)]\tLoss: 2.006146\n",
            "{AlexNet} Train Epoch: 20 [10240/37814 (27%)]\tLoss: 1.985441\n",
            "{AlexNet} Train Epoch: 20 [10752/37814 (28%)]\tLoss: 1.956866\n",
            "{AlexNet} Train Epoch: 20 [11264/37814 (30%)]\tLoss: 1.902739\n",
            "{AlexNet} Train Epoch: 20 [11776/37814 (31%)]\tLoss: 1.972955\n",
            "{AlexNet} Train Epoch: 20 [12288/37814 (32%)]\tLoss: 2.025915\n",
            "{AlexNet} Train Epoch: 20 [12800/37814 (34%)]\tLoss: 1.975737\n",
            "{AlexNet} Train Epoch: 20 [13312/37814 (35%)]\tLoss: 1.954518\n",
            "{AlexNet} Train Epoch: 20 [13824/37814 (36%)]\tLoss: 1.987377\n",
            "{AlexNet} Train Epoch: 20 [14336/37814 (38%)]\tLoss: 2.005442\n",
            "{AlexNet} Train Epoch: 20 [14848/37814 (39%)]\tLoss: 1.954371\n",
            "{AlexNet} Train Epoch: 20 [15360/37814 (41%)]\tLoss: 1.949414\n",
            "{AlexNet} Train Epoch: 20 [15872/37814 (42%)]\tLoss: 1.960590\n",
            "{AlexNet} Train Epoch: 20 [16384/37814 (43%)]\tLoss: 1.999238\n",
            "{AlexNet} Train Epoch: 20 [16896/37814 (45%)]\tLoss: 1.992007\n",
            "{AlexNet} Train Epoch: 20 [17408/37814 (46%)]\tLoss: 1.961621\n",
            "{AlexNet} Train Epoch: 20 [17920/37814 (47%)]\tLoss: 1.956175\n",
            "{AlexNet} Train Epoch: 20 [18432/37814 (49%)]\tLoss: 1.975143\n",
            "{AlexNet} Train Epoch: 20 [18944/37814 (50%)]\tLoss: 1.958752\n",
            "{AlexNet} Train Epoch: 20 [19456/37814 (51%)]\tLoss: 1.937795\n",
            "{AlexNet} Train Epoch: 20 [19968/37814 (53%)]\tLoss: 1.991115\n",
            "{AlexNet} Train Epoch: 20 [20480/37814 (54%)]\tLoss: 1.999480\n",
            "{AlexNet} Train Epoch: 20 [20992/37814 (55%)]\tLoss: 1.976790\n",
            "{AlexNet} Train Epoch: 20 [21504/37814 (57%)]\tLoss: 1.977556\n",
            "{AlexNet} Train Epoch: 20 [22016/37814 (58%)]\tLoss: 2.002199\n",
            "{AlexNet} Train Epoch: 20 [22528/37814 (59%)]\tLoss: 1.934170\n",
            "{AlexNet} Train Epoch: 20 [23040/37814 (61%)]\tLoss: 1.969243\n",
            "{AlexNet} Train Epoch: 20 [23552/37814 (62%)]\tLoss: 2.009600\n",
            "{AlexNet} Train Epoch: 20 [24064/37814 (64%)]\tLoss: 1.997243\n",
            "{AlexNet} Train Epoch: 20 [24576/37814 (65%)]\tLoss: 1.948808\n",
            "{AlexNet} Train Epoch: 20 [25088/37814 (66%)]\tLoss: 1.994015\n",
            "{AlexNet} Train Epoch: 20 [25600/37814 (68%)]\tLoss: 1.973962\n",
            "{AlexNet} Train Epoch: 20 [26112/37814 (69%)]\tLoss: 1.946833\n",
            "{AlexNet} Train Epoch: 20 [26624/37814 (70%)]\tLoss: 1.989843\n",
            "{AlexNet} Train Epoch: 20 [27136/37814 (72%)]\tLoss: 1.957778\n",
            "{AlexNet} Train Epoch: 20 [27648/37814 (73%)]\tLoss: 1.928981\n",
            "{AlexNet} Train Epoch: 20 [28160/37814 (74%)]\tLoss: 1.995622\n",
            "{AlexNet} Train Epoch: 20 [28672/37814 (76%)]\tLoss: 1.923914\n",
            "{AlexNet} Train Epoch: 20 [29184/37814 (77%)]\tLoss: 2.015563\n",
            "{AlexNet} Train Epoch: 20 [29696/37814 (78%)]\tLoss: 1.972881\n",
            "{AlexNet} Train Epoch: 20 [30208/37814 (80%)]\tLoss: 1.968202\n",
            "{AlexNet} Train Epoch: 20 [30720/37814 (81%)]\tLoss: 1.991175\n",
            "{AlexNet} Train Epoch: 20 [31232/37814 (82%)]\tLoss: 1.970368\n",
            "{AlexNet} Train Epoch: 20 [31744/37814 (84%)]\tLoss: 1.939000\n",
            "{AlexNet} Train Epoch: 20 [32256/37814 (85%)]\tLoss: 1.994105\n",
            "{AlexNet} Train Epoch: 20 [32768/37814 (86%)]\tLoss: 1.964176\n",
            "{AlexNet} Train Epoch: 20 [33280/37814 (88%)]\tLoss: 2.046301\n",
            "{AlexNet} Train Epoch: 20 [33792/37814 (89%)]\tLoss: 1.994010\n",
            "{AlexNet} Train Epoch: 20 [34304/37814 (91%)]\tLoss: 1.959328\n",
            "{AlexNet} Train Epoch: 20 [34816/37814 (92%)]\tLoss: 1.968828\n",
            "{AlexNet} Train Epoch: 20 [35328/37814 (93%)]\tLoss: 2.013520\n",
            "{AlexNet} Train Epoch: 20 [35840/37814 (95%)]\tLoss: 2.011012\n",
            "{AlexNet} Train Epoch: 20 [36352/37814 (96%)]\tLoss: 1.956190\n",
            "{AlexNet} Train Epoch: 20 [36864/37814 (97%)]\tLoss: 1.945065\n",
            "{AlexNet} Train Epoch: 20 [31974/37814 (99%)]\tLoss: 2.041618\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9757, Accuracy: 1066/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.821600914001465 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 20 [0/37814 (0%)]\tLoss: 1.800713\n",
            "{SqueezeNet} Train Epoch: 20 [512/37814 (1%)]\tLoss: 1.782845\n",
            "{SqueezeNet} Train Epoch: 20 [1024/37814 (3%)]\tLoss: 1.845246\n",
            "{SqueezeNet} Train Epoch: 20 [1536/37814 (4%)]\tLoss: 1.839650\n",
            "{SqueezeNet} Train Epoch: 20 [2048/37814 (5%)]\tLoss: 1.787249\n",
            "{SqueezeNet} Train Epoch: 20 [2560/37814 (7%)]\tLoss: 1.856872\n",
            "{SqueezeNet} Train Epoch: 20 [3072/37814 (8%)]\tLoss: 1.770870\n",
            "{SqueezeNet} Train Epoch: 20 [3584/37814 (9%)]\tLoss: 1.886045\n",
            "{SqueezeNet} Train Epoch: 20 [4096/37814 (11%)]\tLoss: 1.840981\n",
            "{SqueezeNet} Train Epoch: 20 [4608/37814 (12%)]\tLoss: 1.841600\n",
            "{SqueezeNet} Train Epoch: 20 [5120/37814 (14%)]\tLoss: 1.857640\n",
            "{SqueezeNet} Train Epoch: 20 [5632/37814 (15%)]\tLoss: 1.793145\n",
            "{SqueezeNet} Train Epoch: 20 [6144/37814 (16%)]\tLoss: 1.851705\n",
            "{SqueezeNet} Train Epoch: 20 [6656/37814 (18%)]\tLoss: 1.760127\n",
            "{SqueezeNet} Train Epoch: 20 [7168/37814 (19%)]\tLoss: 1.832879\n",
            "{SqueezeNet} Train Epoch: 20 [7680/37814 (20%)]\tLoss: 1.890071\n",
            "{SqueezeNet} Train Epoch: 20 [8192/37814 (22%)]\tLoss: 1.799328\n",
            "{SqueezeNet} Train Epoch: 20 [8704/37814 (23%)]\tLoss: 1.866305\n",
            "{SqueezeNet} Train Epoch: 20 [9216/37814 (24%)]\tLoss: 1.819636\n",
            "{SqueezeNet} Train Epoch: 20 [9728/37814 (26%)]\tLoss: 1.806827\n",
            "{SqueezeNet} Train Epoch: 20 [10240/37814 (27%)]\tLoss: 1.881042\n",
            "{SqueezeNet} Train Epoch: 20 [10752/37814 (28%)]\tLoss: 1.851894\n",
            "{SqueezeNet} Train Epoch: 20 [11264/37814 (30%)]\tLoss: 1.837971\n",
            "{SqueezeNet} Train Epoch: 20 [11776/37814 (31%)]\tLoss: 1.857117\n",
            "{SqueezeNet} Train Epoch: 20 [12288/37814 (32%)]\tLoss: 1.829934\n",
            "{SqueezeNet} Train Epoch: 20 [12800/37814 (34%)]\tLoss: 1.807104\n",
            "{SqueezeNet} Train Epoch: 20 [13312/37814 (35%)]\tLoss: 1.842831\n",
            "{SqueezeNet} Train Epoch: 20 [13824/37814 (36%)]\tLoss: 1.893366\n",
            "{SqueezeNet} Train Epoch: 20 [14336/37814 (38%)]\tLoss: 1.810317\n",
            "{SqueezeNet} Train Epoch: 20 [14848/37814 (39%)]\tLoss: 1.795759\n",
            "{SqueezeNet} Train Epoch: 20 [15360/37814 (41%)]\tLoss: 1.781868\n",
            "{SqueezeNet} Train Epoch: 20 [15872/37814 (42%)]\tLoss: 1.808600\n",
            "{SqueezeNet} Train Epoch: 20 [16384/37814 (43%)]\tLoss: 1.831227\n",
            "{SqueezeNet} Train Epoch: 20 [16896/37814 (45%)]\tLoss: 1.828075\n",
            "{SqueezeNet} Train Epoch: 20 [17408/37814 (46%)]\tLoss: 1.724962\n",
            "{SqueezeNet} Train Epoch: 20 [17920/37814 (47%)]\tLoss: 1.866036\n",
            "{SqueezeNet} Train Epoch: 20 [18432/37814 (49%)]\tLoss: 1.854215\n",
            "{SqueezeNet} Train Epoch: 20 [18944/37814 (50%)]\tLoss: 1.796020\n",
            "{SqueezeNet} Train Epoch: 20 [19456/37814 (51%)]\tLoss: 1.887611\n",
            "{SqueezeNet} Train Epoch: 20 [19968/37814 (53%)]\tLoss: 1.820671\n",
            "{SqueezeNet} Train Epoch: 20 [20480/37814 (54%)]\tLoss: 1.887151\n",
            "{SqueezeNet} Train Epoch: 20 [20992/37814 (55%)]\tLoss: 1.902956\n",
            "{SqueezeNet} Train Epoch: 20 [21504/37814 (57%)]\tLoss: 1.815040\n",
            "{SqueezeNet} Train Epoch: 20 [22016/37814 (58%)]\tLoss: 1.833078\n",
            "{SqueezeNet} Train Epoch: 20 [22528/37814 (59%)]\tLoss: 1.833371\n",
            "{SqueezeNet} Train Epoch: 20 [23040/37814 (61%)]\tLoss: 1.755697\n",
            "{SqueezeNet} Train Epoch: 20 [23552/37814 (62%)]\tLoss: 1.837233\n",
            "{SqueezeNet} Train Epoch: 20 [24064/37814 (64%)]\tLoss: 1.827045\n",
            "{SqueezeNet} Train Epoch: 20 [24576/37814 (65%)]\tLoss: 1.876017\n",
            "{SqueezeNet} Train Epoch: 20 [25088/37814 (66%)]\tLoss: 1.852152\n",
            "{SqueezeNet} Train Epoch: 20 [25600/37814 (68%)]\tLoss: 1.847920\n",
            "{SqueezeNet} Train Epoch: 20 [26112/37814 (69%)]\tLoss: 1.818290\n",
            "{SqueezeNet} Train Epoch: 20 [26624/37814 (70%)]\tLoss: 1.842529\n",
            "{SqueezeNet} Train Epoch: 20 [27136/37814 (72%)]\tLoss: 1.821180\n",
            "{SqueezeNet} Train Epoch: 20 [27648/37814 (73%)]\tLoss: 1.837791\n",
            "{SqueezeNet} Train Epoch: 20 [28160/37814 (74%)]\tLoss: 1.855788\n",
            "{SqueezeNet} Train Epoch: 20 [28672/37814 (76%)]\tLoss: 1.842124\n",
            "{SqueezeNet} Train Epoch: 20 [29184/37814 (77%)]\tLoss: 1.872895\n",
            "{SqueezeNet} Train Epoch: 20 [29696/37814 (78%)]\tLoss: 1.827061\n",
            "{SqueezeNet} Train Epoch: 20 [30208/37814 (80%)]\tLoss: 1.818732\n",
            "{SqueezeNet} Train Epoch: 20 [30720/37814 (81%)]\tLoss: 1.813620\n",
            "{SqueezeNet} Train Epoch: 20 [31232/37814 (82%)]\tLoss: 1.795946\n",
            "{SqueezeNet} Train Epoch: 20 [31744/37814 (84%)]\tLoss: 1.826468\n",
            "{SqueezeNet} Train Epoch: 20 [32256/37814 (85%)]\tLoss: 1.864095\n",
            "{SqueezeNet} Train Epoch: 20 [32768/37814 (86%)]\tLoss: 1.828496\n",
            "{SqueezeNet} Train Epoch: 20 [33280/37814 (88%)]\tLoss: 1.880500\n",
            "{SqueezeNet} Train Epoch: 20 [33792/37814 (89%)]\tLoss: 1.843616\n",
            "{SqueezeNet} Train Epoch: 20 [34304/37814 (91%)]\tLoss: 1.777843\n",
            "{SqueezeNet} Train Epoch: 20 [34816/37814 (92%)]\tLoss: 1.866427\n",
            "{SqueezeNet} Train Epoch: 20 [35328/37814 (93%)]\tLoss: 1.846576\n",
            "{SqueezeNet} Train Epoch: 20 [35840/37814 (95%)]\tLoss: 1.776283\n",
            "{SqueezeNet} Train Epoch: 20 [36352/37814 (96%)]\tLoss: 1.787602\n",
            "{SqueezeNet} Train Epoch: 20 [36864/37814 (97%)]\tLoss: 1.817145\n",
            "{SqueezeNet} Train Epoch: 20 [31974/37814 (99%)]\tLoss: 1.817628\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8215, Accuracy: 1627/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.412113666534424 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"297c3c60-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"07f4c59e-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_d282ea96d3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"297e24da-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9527bc5ac3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"297e6332-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_442b57033e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"297eb328-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"297e6332-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_7a1a43c4a4"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"29a2c6be-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"297e24da-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f44d7101b6"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"29a48698-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_c040929250"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"29a4c9fa-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_c0e96a27c0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"29a50b4a-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"29a4c9fa-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_88930e1535"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 21 [0/37814 (0%)]\tLoss: 2.004787\n",
            "{AlexNet} Train Epoch: 21 [512/37814 (1%)]\tLoss: 1.937458\n",
            "{AlexNet} Train Epoch: 21 [1024/37814 (3%)]\tLoss: 1.936251\n",
            "{AlexNet} Train Epoch: 21 [1536/37814 (4%)]\tLoss: 1.976930\n",
            "{AlexNet} Train Epoch: 21 [2048/37814 (5%)]\tLoss: 1.964708\n",
            "{AlexNet} Train Epoch: 21 [2560/37814 (7%)]\tLoss: 1.974915\n",
            "{AlexNet} Train Epoch: 21 [3072/37814 (8%)]\tLoss: 1.961678\n",
            "{AlexNet} Train Epoch: 21 [3584/37814 (9%)]\tLoss: 1.946913\n",
            "{AlexNet} Train Epoch: 21 [4096/37814 (11%)]\tLoss: 1.997403\n",
            "{AlexNet} Train Epoch: 21 [4608/37814 (12%)]\tLoss: 2.003477\n",
            "{AlexNet} Train Epoch: 21 [5120/37814 (14%)]\tLoss: 1.985532\n",
            "{AlexNet} Train Epoch: 21 [5632/37814 (15%)]\tLoss: 1.973736\n",
            "{AlexNet} Train Epoch: 21 [6144/37814 (16%)]\tLoss: 1.952331\n",
            "{AlexNet} Train Epoch: 21 [6656/37814 (18%)]\tLoss: 1.993853\n",
            "{AlexNet} Train Epoch: 21 [7168/37814 (19%)]\tLoss: 1.943162\n",
            "{AlexNet} Train Epoch: 21 [7680/37814 (20%)]\tLoss: 1.961930\n",
            "{AlexNet} Train Epoch: 21 [8192/37814 (22%)]\tLoss: 1.937012\n",
            "{AlexNet} Train Epoch: 21 [8704/37814 (23%)]\tLoss: 1.967675\n",
            "{AlexNet} Train Epoch: 21 [9216/37814 (24%)]\tLoss: 1.999369\n",
            "{AlexNet} Train Epoch: 21 [9728/37814 (26%)]\tLoss: 1.969318\n",
            "{AlexNet} Train Epoch: 21 [10240/37814 (27%)]\tLoss: 1.972971\n",
            "{AlexNet} Train Epoch: 21 [10752/37814 (28%)]\tLoss: 1.979485\n",
            "{AlexNet} Train Epoch: 21 [11264/37814 (30%)]\tLoss: 1.966030\n",
            "{AlexNet} Train Epoch: 21 [11776/37814 (31%)]\tLoss: 1.973445\n",
            "{AlexNet} Train Epoch: 21 [12288/37814 (32%)]\tLoss: 1.967701\n",
            "{AlexNet} Train Epoch: 21 [12800/37814 (34%)]\tLoss: 1.931193\n",
            "{AlexNet} Train Epoch: 21 [13312/37814 (35%)]\tLoss: 1.975134\n",
            "{AlexNet} Train Epoch: 21 [13824/37814 (36%)]\tLoss: 2.005282\n",
            "{AlexNet} Train Epoch: 21 [14336/37814 (38%)]\tLoss: 1.960612\n",
            "{AlexNet} Train Epoch: 21 [14848/37814 (39%)]\tLoss: 1.959347\n",
            "{AlexNet} Train Epoch: 21 [15360/37814 (41%)]\tLoss: 1.956600\n",
            "{AlexNet} Train Epoch: 21 [15872/37814 (42%)]\tLoss: 1.991842\n",
            "{AlexNet} Train Epoch: 21 [16384/37814 (43%)]\tLoss: 2.003760\n",
            "{AlexNet} Train Epoch: 21 [16896/37814 (45%)]\tLoss: 1.990230\n",
            "{AlexNet} Train Epoch: 21 [17408/37814 (46%)]\tLoss: 1.982059\n",
            "{AlexNet} Train Epoch: 21 [17920/37814 (47%)]\tLoss: 1.974668\n",
            "{AlexNet} Train Epoch: 21 [18432/37814 (49%)]\tLoss: 1.948066\n",
            "{AlexNet} Train Epoch: 21 [18944/37814 (50%)]\tLoss: 2.004089\n",
            "{AlexNet} Train Epoch: 21 [19456/37814 (51%)]\tLoss: 2.025358\n",
            "{AlexNet} Train Epoch: 21 [19968/37814 (53%)]\tLoss: 1.986077\n",
            "{AlexNet} Train Epoch: 21 [20480/37814 (54%)]\tLoss: 2.024861\n",
            "{AlexNet} Train Epoch: 21 [20992/37814 (55%)]\tLoss: 1.996946\n",
            "{AlexNet} Train Epoch: 21 [21504/37814 (57%)]\tLoss: 1.979757\n",
            "{AlexNet} Train Epoch: 21 [22016/37814 (58%)]\tLoss: 1.930141\n",
            "{AlexNet} Train Epoch: 21 [22528/37814 (59%)]\tLoss: 1.972558\n",
            "{AlexNet} Train Epoch: 21 [23040/37814 (61%)]\tLoss: 1.997842\n",
            "{AlexNet} Train Epoch: 21 [23552/37814 (62%)]\tLoss: 1.953337\n",
            "{AlexNet} Train Epoch: 21 [24064/37814 (64%)]\tLoss: 1.964284\n",
            "{AlexNet} Train Epoch: 21 [24576/37814 (65%)]\tLoss: 1.936359\n",
            "{AlexNet} Train Epoch: 21 [25088/37814 (66%)]\tLoss: 1.980341\n",
            "{AlexNet} Train Epoch: 21 [25600/37814 (68%)]\tLoss: 2.043967\n",
            "{AlexNet} Train Epoch: 21 [26112/37814 (69%)]\tLoss: 2.001140\n",
            "{AlexNet} Train Epoch: 21 [26624/37814 (70%)]\tLoss: 2.007968\n",
            "{AlexNet} Train Epoch: 21 [27136/37814 (72%)]\tLoss: 2.001031\n",
            "{AlexNet} Train Epoch: 21 [27648/37814 (73%)]\tLoss: 1.999255\n",
            "{AlexNet} Train Epoch: 21 [28160/37814 (74%)]\tLoss: 1.963730\n",
            "{AlexNet} Train Epoch: 21 [28672/37814 (76%)]\tLoss: 1.964725\n",
            "{AlexNet} Train Epoch: 21 [29184/37814 (77%)]\tLoss: 2.005164\n",
            "{AlexNet} Train Epoch: 21 [29696/37814 (78%)]\tLoss: 1.945865\n",
            "{AlexNet} Train Epoch: 21 [30208/37814 (80%)]\tLoss: 1.969913\n",
            "{AlexNet} Train Epoch: 21 [30720/37814 (81%)]\tLoss: 1.981287\n",
            "{AlexNet} Train Epoch: 21 [31232/37814 (82%)]\tLoss: 1.984963\n",
            "{AlexNet} Train Epoch: 21 [31744/37814 (84%)]\tLoss: 1.998636\n",
            "{AlexNet} Train Epoch: 21 [32256/37814 (85%)]\tLoss: 1.954496\n",
            "{AlexNet} Train Epoch: 21 [32768/37814 (86%)]\tLoss: 1.959325\n",
            "{AlexNet} Train Epoch: 21 [33280/37814 (88%)]\tLoss: 1.995192\n",
            "{AlexNet} Train Epoch: 21 [33792/37814 (89%)]\tLoss: 1.997225\n",
            "{AlexNet} Train Epoch: 21 [34304/37814 (91%)]\tLoss: 2.011979\n",
            "{AlexNet} Train Epoch: 21 [34816/37814 (92%)]\tLoss: 2.006779\n",
            "{AlexNet} Train Epoch: 21 [35328/37814 (93%)]\tLoss: 1.943564\n",
            "{AlexNet} Train Epoch: 21 [35840/37814 (95%)]\tLoss: 1.948652\n",
            "{AlexNet} Train Epoch: 21 [36352/37814 (96%)]\tLoss: 1.969209\n",
            "{AlexNet} Train Epoch: 21 [36864/37814 (97%)]\tLoss: 1.957350\n",
            "{AlexNet} Train Epoch: 21 [31974/37814 (99%)]\tLoss: 1.916747\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9879, Accuracy: 1010/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.80949902534485 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 21 [0/37814 (0%)]\tLoss: 1.860529\n",
            "{SqueezeNet} Train Epoch: 21 [512/37814 (1%)]\tLoss: 1.936754\n",
            "{SqueezeNet} Train Epoch: 21 [1024/37814 (3%)]\tLoss: 1.861167\n",
            "{SqueezeNet} Train Epoch: 21 [1536/37814 (4%)]\tLoss: 1.886167\n",
            "{SqueezeNet} Train Epoch: 21 [2048/37814 (5%)]\tLoss: 1.847921\n",
            "{SqueezeNet} Train Epoch: 21 [2560/37814 (7%)]\tLoss: 1.814231\n",
            "{SqueezeNet} Train Epoch: 21 [3072/37814 (8%)]\tLoss: 1.841793\n",
            "{SqueezeNet} Train Epoch: 21 [3584/37814 (9%)]\tLoss: 1.831125\n",
            "{SqueezeNet} Train Epoch: 21 [4096/37814 (11%)]\tLoss: 1.802062\n",
            "{SqueezeNet} Train Epoch: 21 [4608/37814 (12%)]\tLoss: 1.819469\n",
            "{SqueezeNet} Train Epoch: 21 [5120/37814 (14%)]\tLoss: 1.863073\n",
            "{SqueezeNet} Train Epoch: 21 [5632/37814 (15%)]\tLoss: 1.846626\n",
            "{SqueezeNet} Train Epoch: 21 [6144/37814 (16%)]\tLoss: 1.744005\n",
            "{SqueezeNet} Train Epoch: 21 [6656/37814 (18%)]\tLoss: 1.856109\n",
            "{SqueezeNet} Train Epoch: 21 [7168/37814 (19%)]\tLoss: 1.856228\n",
            "{SqueezeNet} Train Epoch: 21 [7680/37814 (20%)]\tLoss: 1.817904\n",
            "{SqueezeNet} Train Epoch: 21 [8192/37814 (22%)]\tLoss: 1.839435\n",
            "{SqueezeNet} Train Epoch: 21 [8704/37814 (23%)]\tLoss: 1.812962\n",
            "{SqueezeNet} Train Epoch: 21 [9216/37814 (24%)]\tLoss: 1.846569\n",
            "{SqueezeNet} Train Epoch: 21 [9728/37814 (26%)]\tLoss: 1.855087\n",
            "{SqueezeNet} Train Epoch: 21 [10240/37814 (27%)]\tLoss: 1.803314\n",
            "{SqueezeNet} Train Epoch: 21 [10752/37814 (28%)]\tLoss: 1.867364\n",
            "{SqueezeNet} Train Epoch: 21 [11264/37814 (30%)]\tLoss: 1.790862\n",
            "{SqueezeNet} Train Epoch: 21 [11776/37814 (31%)]\tLoss: 1.855268\n",
            "{SqueezeNet} Train Epoch: 21 [12288/37814 (32%)]\tLoss: 1.885872\n",
            "{SqueezeNet} Train Epoch: 21 [12800/37814 (34%)]\tLoss: 1.822620\n",
            "{SqueezeNet} Train Epoch: 21 [13312/37814 (35%)]\tLoss: 1.840600\n",
            "{SqueezeNet} Train Epoch: 21 [13824/37814 (36%)]\tLoss: 1.825653\n",
            "{SqueezeNet} Train Epoch: 21 [14336/37814 (38%)]\tLoss: 1.849943\n",
            "{SqueezeNet} Train Epoch: 21 [14848/37814 (39%)]\tLoss: 1.845091\n",
            "{SqueezeNet} Train Epoch: 21 [15360/37814 (41%)]\tLoss: 1.868507\n",
            "{SqueezeNet} Train Epoch: 21 [15872/37814 (42%)]\tLoss: 1.741615\n",
            "{SqueezeNet} Train Epoch: 21 [16384/37814 (43%)]\tLoss: 1.816786\n",
            "{SqueezeNet} Train Epoch: 21 [16896/37814 (45%)]\tLoss: 1.895885\n",
            "{SqueezeNet} Train Epoch: 21 [17408/37814 (46%)]\tLoss: 1.897447\n",
            "{SqueezeNet} Train Epoch: 21 [17920/37814 (47%)]\tLoss: 1.828087\n",
            "{SqueezeNet} Train Epoch: 21 [18432/37814 (49%)]\tLoss: 1.839079\n",
            "{SqueezeNet} Train Epoch: 21 [18944/37814 (50%)]\tLoss: 1.880266\n",
            "{SqueezeNet} Train Epoch: 21 [19456/37814 (51%)]\tLoss: 1.785687\n",
            "{SqueezeNet} Train Epoch: 21 [19968/37814 (53%)]\tLoss: 1.862168\n",
            "{SqueezeNet} Train Epoch: 21 [20480/37814 (54%)]\tLoss: 1.820042\n",
            "{SqueezeNet} Train Epoch: 21 [20992/37814 (55%)]\tLoss: 1.784994\n",
            "{SqueezeNet} Train Epoch: 21 [21504/37814 (57%)]\tLoss: 1.783509\n",
            "{SqueezeNet} Train Epoch: 21 [22016/37814 (58%)]\tLoss: 1.852628\n",
            "{SqueezeNet} Train Epoch: 21 [22528/37814 (59%)]\tLoss: 1.849656\n",
            "{SqueezeNet} Train Epoch: 21 [23040/37814 (61%)]\tLoss: 1.797533\n",
            "{SqueezeNet} Train Epoch: 21 [23552/37814 (62%)]\tLoss: 1.849768\n",
            "{SqueezeNet} Train Epoch: 21 [24064/37814 (64%)]\tLoss: 1.822275\n",
            "{SqueezeNet} Train Epoch: 21 [24576/37814 (65%)]\tLoss: 1.780560\n",
            "{SqueezeNet} Train Epoch: 21 [25088/37814 (66%)]\tLoss: 1.827077\n",
            "{SqueezeNet} Train Epoch: 21 [25600/37814 (68%)]\tLoss: 1.872027\n",
            "{SqueezeNet} Train Epoch: 21 [26112/37814 (69%)]\tLoss: 1.763521\n",
            "{SqueezeNet} Train Epoch: 21 [26624/37814 (70%)]\tLoss: 1.799915\n",
            "{SqueezeNet} Train Epoch: 21 [27136/37814 (72%)]\tLoss: 1.769022\n",
            "{SqueezeNet} Train Epoch: 21 [27648/37814 (73%)]\tLoss: 1.791936\n",
            "{SqueezeNet} Train Epoch: 21 [28160/37814 (74%)]\tLoss: 1.761366\n",
            "{SqueezeNet} Train Epoch: 21 [28672/37814 (76%)]\tLoss: 1.817157\n",
            "{SqueezeNet} Train Epoch: 21 [29184/37814 (77%)]\tLoss: 1.812999\n",
            "{SqueezeNet} Train Epoch: 21 [29696/37814 (78%)]\tLoss: 1.931403\n",
            "{SqueezeNet} Train Epoch: 21 [30208/37814 (80%)]\tLoss: 1.844748\n",
            "{SqueezeNet} Train Epoch: 21 [30720/37814 (81%)]\tLoss: 1.830749\n",
            "{SqueezeNet} Train Epoch: 21 [31232/37814 (82%)]\tLoss: 1.840424\n",
            "{SqueezeNet} Train Epoch: 21 [31744/37814 (84%)]\tLoss: 1.836639\n",
            "{SqueezeNet} Train Epoch: 21 [32256/37814 (85%)]\tLoss: 1.803875\n",
            "{SqueezeNet} Train Epoch: 21 [32768/37814 (86%)]\tLoss: 1.841273\n",
            "{SqueezeNet} Train Epoch: 21 [33280/37814 (88%)]\tLoss: 1.767074\n",
            "{SqueezeNet} Train Epoch: 21 [33792/37814 (89%)]\tLoss: 1.866087\n",
            "{SqueezeNet} Train Epoch: 21 [34304/37814 (91%)]\tLoss: 1.809101\n",
            "{SqueezeNet} Train Epoch: 21 [34816/37814 (92%)]\tLoss: 1.747113\n",
            "{SqueezeNet} Train Epoch: 21 [35328/37814 (93%)]\tLoss: 1.770166\n",
            "{SqueezeNet} Train Epoch: 21 [35840/37814 (95%)]\tLoss: 1.863473\n",
            "{SqueezeNet} Train Epoch: 21 [36352/37814 (96%)]\tLoss: 1.807019\n",
            "{SqueezeNet} Train Epoch: 21 [36864/37814 (97%)]\tLoss: 1.847000\n",
            "{SqueezeNet} Train Epoch: 21 [31974/37814 (99%)]\tLoss: 1.866955\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8326, Accuracy: 1591/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.786460399627686 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4ac9d968-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"29a48698-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_a5059e1836"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4acc1674-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_34ef581864"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4acc62dc-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_a8c912b85a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4accc59c-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"4acc62dc-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_88160c4209"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4af9ed38-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"4acc1674-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8c00bd2457"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4afb7fc2-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_cf31f4d947"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4afbed2c-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_8d7f7c4344"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4afc5960-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"4afbed2c-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f9eb354a7a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 22 [0/37814 (0%)]\tLoss: 1.971905\n",
            "{AlexNet} Train Epoch: 22 [512/37814 (1%)]\tLoss: 1.959102\n",
            "{AlexNet} Train Epoch: 22 [1024/37814 (3%)]\tLoss: 1.973304\n",
            "{AlexNet} Train Epoch: 22 [1536/37814 (4%)]\tLoss: 2.006525\n",
            "{AlexNet} Train Epoch: 22 [2048/37814 (5%)]\tLoss: 1.995455\n",
            "{AlexNet} Train Epoch: 22 [2560/37814 (7%)]\tLoss: 1.977018\n",
            "{AlexNet} Train Epoch: 22 [3072/37814 (8%)]\tLoss: 2.004468\n",
            "{AlexNet} Train Epoch: 22 [3584/37814 (9%)]\tLoss: 1.953133\n",
            "{AlexNet} Train Epoch: 22 [4096/37814 (11%)]\tLoss: 1.998407\n",
            "{AlexNet} Train Epoch: 22 [4608/37814 (12%)]\tLoss: 1.984095\n",
            "{AlexNet} Train Epoch: 22 [5120/37814 (14%)]\tLoss: 1.935587\n",
            "{AlexNet} Train Epoch: 22 [5632/37814 (15%)]\tLoss: 1.962598\n",
            "{AlexNet} Train Epoch: 22 [6144/37814 (16%)]\tLoss: 1.987826\n",
            "{AlexNet} Train Epoch: 22 [6656/37814 (18%)]\tLoss: 1.973213\n",
            "{AlexNet} Train Epoch: 22 [7168/37814 (19%)]\tLoss: 1.990541\n",
            "{AlexNet} Train Epoch: 22 [7680/37814 (20%)]\tLoss: 1.956492\n",
            "{AlexNet} Train Epoch: 22 [8192/37814 (22%)]\tLoss: 1.952587\n",
            "{AlexNet} Train Epoch: 22 [8704/37814 (23%)]\tLoss: 1.995934\n",
            "{AlexNet} Train Epoch: 22 [9216/37814 (24%)]\tLoss: 1.982985\n",
            "{AlexNet} Train Epoch: 22 [9728/37814 (26%)]\tLoss: 1.997095\n",
            "{AlexNet} Train Epoch: 22 [10240/37814 (27%)]\tLoss: 1.958932\n",
            "{AlexNet} Train Epoch: 22 [10752/37814 (28%)]\tLoss: 1.992667\n",
            "{AlexNet} Train Epoch: 22 [11264/37814 (30%)]\tLoss: 1.933339\n",
            "{AlexNet} Train Epoch: 22 [11776/37814 (31%)]\tLoss: 2.008911\n",
            "{AlexNet} Train Epoch: 22 [12288/37814 (32%)]\tLoss: 1.955783\n",
            "{AlexNet} Train Epoch: 22 [12800/37814 (34%)]\tLoss: 2.014941\n",
            "{AlexNet} Train Epoch: 22 [13312/37814 (35%)]\tLoss: 1.919728\n",
            "{AlexNet} Train Epoch: 22 [13824/37814 (36%)]\tLoss: 1.946384\n",
            "{AlexNet} Train Epoch: 22 [14336/37814 (38%)]\tLoss: 1.956746\n",
            "{AlexNet} Train Epoch: 22 [14848/37814 (39%)]\tLoss: 1.992518\n",
            "{AlexNet} Train Epoch: 22 [15360/37814 (41%)]\tLoss: 1.980092\n",
            "{AlexNet} Train Epoch: 22 [15872/37814 (42%)]\tLoss: 1.975249\n",
            "{AlexNet} Train Epoch: 22 [16384/37814 (43%)]\tLoss: 1.903234\n",
            "{AlexNet} Train Epoch: 22 [16896/37814 (45%)]\tLoss: 1.975975\n",
            "{AlexNet} Train Epoch: 22 [17408/37814 (46%)]\tLoss: 1.976795\n",
            "{AlexNet} Train Epoch: 22 [17920/37814 (47%)]\tLoss: 1.948620\n",
            "{AlexNet} Train Epoch: 22 [18432/37814 (49%)]\tLoss: 1.932833\n",
            "{AlexNet} Train Epoch: 22 [18944/37814 (50%)]\tLoss: 1.980884\n",
            "{AlexNet} Train Epoch: 22 [19456/37814 (51%)]\tLoss: 1.982912\n",
            "{AlexNet} Train Epoch: 22 [19968/37814 (53%)]\tLoss: 1.939560\n",
            "{AlexNet} Train Epoch: 22 [20480/37814 (54%)]\tLoss: 2.018119\n",
            "{AlexNet} Train Epoch: 22 [20992/37814 (55%)]\tLoss: 1.957795\n",
            "{AlexNet} Train Epoch: 22 [21504/37814 (57%)]\tLoss: 1.940623\n",
            "{AlexNet} Train Epoch: 22 [22016/37814 (58%)]\tLoss: 2.051916\n",
            "{AlexNet} Train Epoch: 22 [22528/37814 (59%)]\tLoss: 1.975029\n",
            "{AlexNet} Train Epoch: 22 [23040/37814 (61%)]\tLoss: 1.999218\n",
            "{AlexNet} Train Epoch: 22 [23552/37814 (62%)]\tLoss: 1.974754\n",
            "{AlexNet} Train Epoch: 22 [24064/37814 (64%)]\tLoss: 1.986060\n",
            "{AlexNet} Train Epoch: 22 [24576/37814 (65%)]\tLoss: 1.946677\n",
            "{AlexNet} Train Epoch: 22 [25088/37814 (66%)]\tLoss: 1.976474\n",
            "{AlexNet} Train Epoch: 22 [25600/37814 (68%)]\tLoss: 1.953306\n",
            "{AlexNet} Train Epoch: 22 [26112/37814 (69%)]\tLoss: 2.011987\n",
            "{AlexNet} Train Epoch: 22 [26624/37814 (70%)]\tLoss: 1.915004\n",
            "{AlexNet} Train Epoch: 22 [27136/37814 (72%)]\tLoss: 1.998107\n",
            "{AlexNet} Train Epoch: 22 [27648/37814 (73%)]\tLoss: 1.967567\n",
            "{AlexNet} Train Epoch: 22 [28160/37814 (74%)]\tLoss: 1.936313\n",
            "{AlexNet} Train Epoch: 22 [28672/37814 (76%)]\tLoss: 1.963982\n",
            "{AlexNet} Train Epoch: 22 [29184/37814 (77%)]\tLoss: 1.939710\n",
            "{AlexNet} Train Epoch: 22 [29696/37814 (78%)]\tLoss: 1.983915\n",
            "{AlexNet} Train Epoch: 22 [30208/37814 (80%)]\tLoss: 1.957244\n",
            "{AlexNet} Train Epoch: 22 [30720/37814 (81%)]\tLoss: 1.963582\n",
            "{AlexNet} Train Epoch: 22 [31232/37814 (82%)]\tLoss: 1.942067\n",
            "{AlexNet} Train Epoch: 22 [31744/37814 (84%)]\tLoss: 2.019334\n",
            "{AlexNet} Train Epoch: 22 [32256/37814 (85%)]\tLoss: 1.997295\n",
            "{AlexNet} Train Epoch: 22 [32768/37814 (86%)]\tLoss: 1.998063\n",
            "{AlexNet} Train Epoch: 22 [33280/37814 (88%)]\tLoss: 2.019184\n",
            "{AlexNet} Train Epoch: 22 [33792/37814 (89%)]\tLoss: 2.007727\n",
            "{AlexNet} Train Epoch: 22 [34304/37814 (91%)]\tLoss: 1.965316\n",
            "{AlexNet} Train Epoch: 22 [34816/37814 (92%)]\tLoss: 1.950366\n",
            "{AlexNet} Train Epoch: 22 [35328/37814 (93%)]\tLoss: 1.946694\n",
            "{AlexNet} Train Epoch: 22 [35840/37814 (95%)]\tLoss: 1.984751\n",
            "{AlexNet} Train Epoch: 22 [36352/37814 (96%)]\tLoss: 1.950624\n",
            "{AlexNet} Train Epoch: 22 [36864/37814 (97%)]\tLoss: 1.987747\n",
            "{AlexNet} Train Epoch: 22 [31974/37814 (99%)]\tLoss: 1.967149\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9646, Accuracy: 1035/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.53894877433777 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 22 [0/37814 (0%)]\tLoss: 1.803623\n",
            "{SqueezeNet} Train Epoch: 22 [512/37814 (1%)]\tLoss: 1.832291\n",
            "{SqueezeNet} Train Epoch: 22 [1024/37814 (3%)]\tLoss: 1.774321\n",
            "{SqueezeNet} Train Epoch: 22 [1536/37814 (4%)]\tLoss: 1.902951\n",
            "{SqueezeNet} Train Epoch: 22 [2048/37814 (5%)]\tLoss: 1.779595\n",
            "{SqueezeNet} Train Epoch: 22 [2560/37814 (7%)]\tLoss: 1.833295\n",
            "{SqueezeNet} Train Epoch: 22 [3072/37814 (8%)]\tLoss: 1.846786\n",
            "{SqueezeNet} Train Epoch: 22 [3584/37814 (9%)]\tLoss: 1.872426\n",
            "{SqueezeNet} Train Epoch: 22 [4096/37814 (11%)]\tLoss: 1.810076\n",
            "{SqueezeNet} Train Epoch: 22 [4608/37814 (12%)]\tLoss: 1.865039\n",
            "{SqueezeNet} Train Epoch: 22 [5120/37814 (14%)]\tLoss: 1.921161\n",
            "{SqueezeNet} Train Epoch: 22 [5632/37814 (15%)]\tLoss: 1.800515\n",
            "{SqueezeNet} Train Epoch: 22 [6144/37814 (16%)]\tLoss: 1.803207\n",
            "{SqueezeNet} Train Epoch: 22 [6656/37814 (18%)]\tLoss: 1.822718\n",
            "{SqueezeNet} Train Epoch: 22 [7168/37814 (19%)]\tLoss: 1.818612\n",
            "{SqueezeNet} Train Epoch: 22 [7680/37814 (20%)]\tLoss: 1.838683\n",
            "{SqueezeNet} Train Epoch: 22 [8192/37814 (22%)]\tLoss: 1.869767\n",
            "{SqueezeNet} Train Epoch: 22 [8704/37814 (23%)]\tLoss: 1.815162\n",
            "{SqueezeNet} Train Epoch: 22 [9216/37814 (24%)]\tLoss: 1.820250\n",
            "{SqueezeNet} Train Epoch: 22 [9728/37814 (26%)]\tLoss: 1.863003\n",
            "{SqueezeNet} Train Epoch: 22 [10240/37814 (27%)]\tLoss: 1.830501\n",
            "{SqueezeNet} Train Epoch: 22 [10752/37814 (28%)]\tLoss: 1.785879\n",
            "{SqueezeNet} Train Epoch: 22 [11264/37814 (30%)]\tLoss: 1.842933\n",
            "{SqueezeNet} Train Epoch: 22 [11776/37814 (31%)]\tLoss: 1.870522\n",
            "{SqueezeNet} Train Epoch: 22 [12288/37814 (32%)]\tLoss: 1.769186\n",
            "{SqueezeNet} Train Epoch: 22 [12800/37814 (34%)]\tLoss: 1.881851\n",
            "{SqueezeNet} Train Epoch: 22 [13312/37814 (35%)]\tLoss: 1.865300\n",
            "{SqueezeNet} Train Epoch: 22 [13824/37814 (36%)]\tLoss: 1.834979\n",
            "{SqueezeNet} Train Epoch: 22 [14336/37814 (38%)]\tLoss: 1.852012\n",
            "{SqueezeNet} Train Epoch: 22 [14848/37814 (39%)]\tLoss: 1.818650\n",
            "{SqueezeNet} Train Epoch: 22 [15360/37814 (41%)]\tLoss: 1.728938\n",
            "{SqueezeNet} Train Epoch: 22 [15872/37814 (42%)]\tLoss: 1.837878\n",
            "{SqueezeNet} Train Epoch: 22 [16384/37814 (43%)]\tLoss: 1.807974\n",
            "{SqueezeNet} Train Epoch: 22 [16896/37814 (45%)]\tLoss: 1.743063\n",
            "{SqueezeNet} Train Epoch: 22 [17408/37814 (46%)]\tLoss: 1.866211\n",
            "{SqueezeNet} Train Epoch: 22 [17920/37814 (47%)]\tLoss: 1.769809\n",
            "{SqueezeNet} Train Epoch: 22 [18432/37814 (49%)]\tLoss: 1.768653\n",
            "{SqueezeNet} Train Epoch: 22 [18944/37814 (50%)]\tLoss: 1.839382\n",
            "{SqueezeNet} Train Epoch: 22 [19456/37814 (51%)]\tLoss: 1.757102\n",
            "{SqueezeNet} Train Epoch: 22 [19968/37814 (53%)]\tLoss: 1.830631\n",
            "{SqueezeNet} Train Epoch: 22 [20480/37814 (54%)]\tLoss: 1.802855\n",
            "{SqueezeNet} Train Epoch: 22 [20992/37814 (55%)]\tLoss: 1.835293\n",
            "{SqueezeNet} Train Epoch: 22 [21504/37814 (57%)]\tLoss: 1.873479\n",
            "{SqueezeNet} Train Epoch: 22 [22016/37814 (58%)]\tLoss: 1.845537\n",
            "{SqueezeNet} Train Epoch: 22 [22528/37814 (59%)]\tLoss: 1.827007\n",
            "{SqueezeNet} Train Epoch: 22 [23040/37814 (61%)]\tLoss: 1.780831\n",
            "{SqueezeNet} Train Epoch: 22 [23552/37814 (62%)]\tLoss: 1.816218\n",
            "{SqueezeNet} Train Epoch: 22 [24064/37814 (64%)]\tLoss: 1.774137\n",
            "{SqueezeNet} Train Epoch: 22 [24576/37814 (65%)]\tLoss: 1.864383\n",
            "{SqueezeNet} Train Epoch: 22 [25088/37814 (66%)]\tLoss: 1.843132\n",
            "{SqueezeNet} Train Epoch: 22 [25600/37814 (68%)]\tLoss: 1.872507\n",
            "{SqueezeNet} Train Epoch: 22 [26112/37814 (69%)]\tLoss: 1.910979\n",
            "{SqueezeNet} Train Epoch: 22 [26624/37814 (70%)]\tLoss: 1.776885\n",
            "{SqueezeNet} Train Epoch: 22 [27136/37814 (72%)]\tLoss: 1.808824\n",
            "{SqueezeNet} Train Epoch: 22 [27648/37814 (73%)]\tLoss: 1.842654\n",
            "{SqueezeNet} Train Epoch: 22 [28160/37814 (74%)]\tLoss: 1.869526\n",
            "{SqueezeNet} Train Epoch: 22 [28672/37814 (76%)]\tLoss: 1.806035\n",
            "{SqueezeNet} Train Epoch: 22 [29184/37814 (77%)]\tLoss: 1.800588\n",
            "{SqueezeNet} Train Epoch: 22 [29696/37814 (78%)]\tLoss: 1.834997\n",
            "{SqueezeNet} Train Epoch: 22 [30208/37814 (80%)]\tLoss: 1.806700\n",
            "{SqueezeNet} Train Epoch: 22 [30720/37814 (81%)]\tLoss: 1.865164\n",
            "{SqueezeNet} Train Epoch: 22 [31232/37814 (82%)]\tLoss: 1.887943\n",
            "{SqueezeNet} Train Epoch: 22 [31744/37814 (84%)]\tLoss: 1.795233\n",
            "{SqueezeNet} Train Epoch: 22 [32256/37814 (85%)]\tLoss: 1.807850\n",
            "{SqueezeNet} Train Epoch: 22 [32768/37814 (86%)]\tLoss: 1.857730\n",
            "{SqueezeNet} Train Epoch: 22 [33280/37814 (88%)]\tLoss: 1.880346\n",
            "{SqueezeNet} Train Epoch: 22 [33792/37814 (89%)]\tLoss: 1.784336\n",
            "{SqueezeNet} Train Epoch: 22 [34304/37814 (91%)]\tLoss: 1.824487\n",
            "{SqueezeNet} Train Epoch: 22 [34816/37814 (92%)]\tLoss: 1.855752\n",
            "{SqueezeNet} Train Epoch: 22 [35328/37814 (93%)]\tLoss: 1.882501\n",
            "{SqueezeNet} Train Epoch: 22 [35840/37814 (95%)]\tLoss: 1.775908\n",
            "{SqueezeNet} Train Epoch: 22 [36352/37814 (96%)]\tLoss: 1.821867\n",
            "{SqueezeNet} Train Epoch: 22 [36864/37814 (97%)]\tLoss: 1.814399\n",
            "{SqueezeNet} Train Epoch: 22 [31974/37814 (99%)]\tLoss: 1.780448\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8121, Accuracy: 1632/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.395522832870483 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c549e06-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"4afb7fc2-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f31222e9f9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c55db90-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_c4e79c7616"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c561600-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_8ddd64a98c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c564e36-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"6c561600-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_a431aa0869"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c7ae700-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"6c55db90-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e40cab9965"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c7be0c4-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_0b7d71e447"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c7c1774-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_b683bf32b9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"6c7c4898-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"6c7c1774-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_754c7c657f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 23 [0/37814 (0%)]\tLoss: 1.950620\n",
            "{AlexNet} Train Epoch: 23 [512/37814 (1%)]\tLoss: 1.935400\n",
            "{AlexNet} Train Epoch: 23 [1024/37814 (3%)]\tLoss: 1.961486\n",
            "{AlexNet} Train Epoch: 23 [1536/37814 (4%)]\tLoss: 1.979875\n",
            "{AlexNet} Train Epoch: 23 [2048/37814 (5%)]\tLoss: 1.985233\n",
            "{AlexNet} Train Epoch: 23 [2560/37814 (7%)]\tLoss: 2.006328\n",
            "{AlexNet} Train Epoch: 23 [3072/37814 (8%)]\tLoss: 1.965175\n",
            "{AlexNet} Train Epoch: 23 [3584/37814 (9%)]\tLoss: 1.935035\n",
            "{AlexNet} Train Epoch: 23 [4096/37814 (11%)]\tLoss: 1.973039\n",
            "{AlexNet} Train Epoch: 23 [4608/37814 (12%)]\tLoss: 1.991689\n",
            "{AlexNet} Train Epoch: 23 [5120/37814 (14%)]\tLoss: 1.975535\n",
            "{AlexNet} Train Epoch: 23 [5632/37814 (15%)]\tLoss: 1.979694\n",
            "{AlexNet} Train Epoch: 23 [6144/37814 (16%)]\tLoss: 1.977409\n",
            "{AlexNet} Train Epoch: 23 [6656/37814 (18%)]\tLoss: 1.988786\n",
            "{AlexNet} Train Epoch: 23 [7168/37814 (19%)]\tLoss: 1.970326\n",
            "{AlexNet} Train Epoch: 23 [7680/37814 (20%)]\tLoss: 2.003978\n",
            "{AlexNet} Train Epoch: 23 [8192/37814 (22%)]\tLoss: 1.980420\n",
            "{AlexNet} Train Epoch: 23 [8704/37814 (23%)]\tLoss: 1.933102\n",
            "{AlexNet} Train Epoch: 23 [9216/37814 (24%)]\tLoss: 1.971313\n",
            "{AlexNet} Train Epoch: 23 [9728/37814 (26%)]\tLoss: 2.033849\n",
            "{AlexNet} Train Epoch: 23 [10240/37814 (27%)]\tLoss: 1.954348\n",
            "{AlexNet} Train Epoch: 23 [10752/37814 (28%)]\tLoss: 1.969232\n",
            "{AlexNet} Train Epoch: 23 [11264/37814 (30%)]\tLoss: 2.015964\n",
            "{AlexNet} Train Epoch: 23 [11776/37814 (31%)]\tLoss: 1.968596\n",
            "{AlexNet} Train Epoch: 23 [12288/37814 (32%)]\tLoss: 2.012211\n",
            "{AlexNet} Train Epoch: 23 [12800/37814 (34%)]\tLoss: 1.924746\n",
            "{AlexNet} Train Epoch: 23 [13312/37814 (35%)]\tLoss: 2.017327\n",
            "{AlexNet} Train Epoch: 23 [13824/37814 (36%)]\tLoss: 2.031542\n",
            "{AlexNet} Train Epoch: 23 [14336/37814 (38%)]\tLoss: 1.980688\n",
            "{AlexNet} Train Epoch: 23 [14848/37814 (39%)]\tLoss: 1.958090\n",
            "{AlexNet} Train Epoch: 23 [15360/37814 (41%)]\tLoss: 2.005207\n",
            "{AlexNet} Train Epoch: 23 [15872/37814 (42%)]\tLoss: 1.971311\n",
            "{AlexNet} Train Epoch: 23 [16384/37814 (43%)]\tLoss: 1.939576\n",
            "{AlexNet} Train Epoch: 23 [16896/37814 (45%)]\tLoss: 1.950272\n",
            "{AlexNet} Train Epoch: 23 [17408/37814 (46%)]\tLoss: 1.991885\n",
            "{AlexNet} Train Epoch: 23 [17920/37814 (47%)]\tLoss: 1.954744\n",
            "{AlexNet} Train Epoch: 23 [18432/37814 (49%)]\tLoss: 2.005085\n",
            "{AlexNet} Train Epoch: 23 [18944/37814 (50%)]\tLoss: 2.006926\n",
            "{AlexNet} Train Epoch: 23 [19456/37814 (51%)]\tLoss: 1.941215\n",
            "{AlexNet} Train Epoch: 23 [19968/37814 (53%)]\tLoss: 1.937975\n",
            "{AlexNet} Train Epoch: 23 [20480/37814 (54%)]\tLoss: 1.982044\n",
            "{AlexNet} Train Epoch: 23 [20992/37814 (55%)]\tLoss: 2.000057\n",
            "{AlexNet} Train Epoch: 23 [21504/37814 (57%)]\tLoss: 1.952282\n",
            "{AlexNet} Train Epoch: 23 [22016/37814 (58%)]\tLoss: 1.981257\n",
            "{AlexNet} Train Epoch: 23 [22528/37814 (59%)]\tLoss: 1.977151\n",
            "{AlexNet} Train Epoch: 23 [23040/37814 (61%)]\tLoss: 1.968658\n",
            "{AlexNet} Train Epoch: 23 [23552/37814 (62%)]\tLoss: 1.975216\n",
            "{AlexNet} Train Epoch: 23 [24064/37814 (64%)]\tLoss: 2.012393\n",
            "{AlexNet} Train Epoch: 23 [24576/37814 (65%)]\tLoss: 1.991006\n",
            "{AlexNet} Train Epoch: 23 [25088/37814 (66%)]\tLoss: 1.980977\n",
            "{AlexNet} Train Epoch: 23 [25600/37814 (68%)]\tLoss: 1.991355\n",
            "{AlexNet} Train Epoch: 23 [26112/37814 (69%)]\tLoss: 1.975869\n",
            "{AlexNet} Train Epoch: 23 [26624/37814 (70%)]\tLoss: 2.042065\n",
            "{AlexNet} Train Epoch: 23 [27136/37814 (72%)]\tLoss: 1.976667\n",
            "{AlexNet} Train Epoch: 23 [27648/37814 (73%)]\tLoss: 1.948866\n",
            "{AlexNet} Train Epoch: 23 [28160/37814 (74%)]\tLoss: 1.938086\n",
            "{AlexNet} Train Epoch: 23 [28672/37814 (76%)]\tLoss: 2.005376\n",
            "{AlexNet} Train Epoch: 23 [29184/37814 (77%)]\tLoss: 1.967481\n",
            "{AlexNet} Train Epoch: 23 [29696/37814 (78%)]\tLoss: 1.943061\n",
            "{AlexNet} Train Epoch: 23 [30208/37814 (80%)]\tLoss: 1.949551\n",
            "{AlexNet} Train Epoch: 23 [30720/37814 (81%)]\tLoss: 2.019212\n",
            "{AlexNet} Train Epoch: 23 [31232/37814 (82%)]\tLoss: 1.976655\n",
            "{AlexNet} Train Epoch: 23 [31744/37814 (84%)]\tLoss: 1.954517\n",
            "{AlexNet} Train Epoch: 23 [32256/37814 (85%)]\tLoss: 2.008718\n",
            "{AlexNet} Train Epoch: 23 [32768/37814 (86%)]\tLoss: 2.031071\n",
            "{AlexNet} Train Epoch: 23 [33280/37814 (88%)]\tLoss: 1.972045\n",
            "{AlexNet} Train Epoch: 23 [33792/37814 (89%)]\tLoss: 1.961528\n",
            "{AlexNet} Train Epoch: 23 [34304/37814 (91%)]\tLoss: 1.964572\n",
            "{AlexNet} Train Epoch: 23 [34816/37814 (92%)]\tLoss: 1.930315\n",
            "{AlexNet} Train Epoch: 23 [35328/37814 (93%)]\tLoss: 1.968377\n",
            "{AlexNet} Train Epoch: 23 [35840/37814 (95%)]\tLoss: 1.988146\n",
            "{AlexNet} Train Epoch: 23 [36352/37814 (96%)]\tLoss: 1.985237\n",
            "{AlexNet} Train Epoch: 23 [36864/37814 (97%)]\tLoss: 1.990019\n",
            "{AlexNet} Train Epoch: 23 [31974/37814 (99%)]\tLoss: 1.938671\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9821, Accuracy: 1057/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.422235250473022 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 23 [0/37814 (0%)]\tLoss: 1.803200\n",
            "{SqueezeNet} Train Epoch: 23 [512/37814 (1%)]\tLoss: 1.874106\n",
            "{SqueezeNet} Train Epoch: 23 [1024/37814 (3%)]\tLoss: 1.805025\n",
            "{SqueezeNet} Train Epoch: 23 [1536/37814 (4%)]\tLoss: 1.820561\n",
            "{SqueezeNet} Train Epoch: 23 [2048/37814 (5%)]\tLoss: 1.876339\n",
            "{SqueezeNet} Train Epoch: 23 [2560/37814 (7%)]\tLoss: 1.835911\n",
            "{SqueezeNet} Train Epoch: 23 [3072/37814 (8%)]\tLoss: 1.821557\n",
            "{SqueezeNet} Train Epoch: 23 [3584/37814 (9%)]\tLoss: 1.803038\n",
            "{SqueezeNet} Train Epoch: 23 [4096/37814 (11%)]\tLoss: 1.869898\n",
            "{SqueezeNet} Train Epoch: 23 [4608/37814 (12%)]\tLoss: 1.783589\n",
            "{SqueezeNet} Train Epoch: 23 [5120/37814 (14%)]\tLoss: 1.859758\n",
            "{SqueezeNet} Train Epoch: 23 [5632/37814 (15%)]\tLoss: 1.780236\n",
            "{SqueezeNet} Train Epoch: 23 [6144/37814 (16%)]\tLoss: 1.806167\n",
            "{SqueezeNet} Train Epoch: 23 [6656/37814 (18%)]\tLoss: 1.786427\n",
            "{SqueezeNet} Train Epoch: 23 [7168/37814 (19%)]\tLoss: 1.789354\n",
            "{SqueezeNet} Train Epoch: 23 [7680/37814 (20%)]\tLoss: 1.839491\n",
            "{SqueezeNet} Train Epoch: 23 [8192/37814 (22%)]\tLoss: 1.838913\n",
            "{SqueezeNet} Train Epoch: 23 [8704/37814 (23%)]\tLoss: 1.810995\n",
            "{SqueezeNet} Train Epoch: 23 [9216/37814 (24%)]\tLoss: 1.813571\n",
            "{SqueezeNet} Train Epoch: 23 [9728/37814 (26%)]\tLoss: 1.847038\n",
            "{SqueezeNet} Train Epoch: 23 [10240/37814 (27%)]\tLoss: 1.806999\n",
            "{SqueezeNet} Train Epoch: 23 [10752/37814 (28%)]\tLoss: 1.821648\n",
            "{SqueezeNet} Train Epoch: 23 [11264/37814 (30%)]\tLoss: 1.824118\n",
            "{SqueezeNet} Train Epoch: 23 [11776/37814 (31%)]\tLoss: 1.791965\n",
            "{SqueezeNet} Train Epoch: 23 [12288/37814 (32%)]\tLoss: 1.840216\n",
            "{SqueezeNet} Train Epoch: 23 [12800/37814 (34%)]\tLoss: 1.838144\n",
            "{SqueezeNet} Train Epoch: 23 [13312/37814 (35%)]\tLoss: 1.853700\n",
            "{SqueezeNet} Train Epoch: 23 [13824/37814 (36%)]\tLoss: 1.825392\n",
            "{SqueezeNet} Train Epoch: 23 [14336/37814 (38%)]\tLoss: 1.859756\n",
            "{SqueezeNet} Train Epoch: 23 [14848/37814 (39%)]\tLoss: 1.844484\n",
            "{SqueezeNet} Train Epoch: 23 [15360/37814 (41%)]\tLoss: 1.817204\n",
            "{SqueezeNet} Train Epoch: 23 [15872/37814 (42%)]\tLoss: 1.770860\n",
            "{SqueezeNet} Train Epoch: 23 [16384/37814 (43%)]\tLoss: 1.872107\n",
            "{SqueezeNet} Train Epoch: 23 [16896/37814 (45%)]\tLoss: 1.826906\n",
            "{SqueezeNet} Train Epoch: 23 [17408/37814 (46%)]\tLoss: 1.799979\n",
            "{SqueezeNet} Train Epoch: 23 [17920/37814 (47%)]\tLoss: 1.830911\n",
            "{SqueezeNet} Train Epoch: 23 [18432/37814 (49%)]\tLoss: 1.877404\n",
            "{SqueezeNet} Train Epoch: 23 [18944/37814 (50%)]\tLoss: 1.866041\n",
            "{SqueezeNet} Train Epoch: 23 [19456/37814 (51%)]\tLoss: 1.839590\n",
            "{SqueezeNet} Train Epoch: 23 [19968/37814 (53%)]\tLoss: 1.816351\n",
            "{SqueezeNet} Train Epoch: 23 [20480/37814 (54%)]\tLoss: 1.788757\n",
            "{SqueezeNet} Train Epoch: 23 [20992/37814 (55%)]\tLoss: 1.752082\n",
            "{SqueezeNet} Train Epoch: 23 [21504/37814 (57%)]\tLoss: 1.889422\n",
            "{SqueezeNet} Train Epoch: 23 [22016/37814 (58%)]\tLoss: 1.805577\n",
            "{SqueezeNet} Train Epoch: 23 [22528/37814 (59%)]\tLoss: 1.772547\n",
            "{SqueezeNet} Train Epoch: 23 [23040/37814 (61%)]\tLoss: 1.802856\n",
            "{SqueezeNet} Train Epoch: 23 [23552/37814 (62%)]\tLoss: 1.826293\n",
            "{SqueezeNet} Train Epoch: 23 [24064/37814 (64%)]\tLoss: 1.880929\n",
            "{SqueezeNet} Train Epoch: 23 [24576/37814 (65%)]\tLoss: 1.797396\n",
            "{SqueezeNet} Train Epoch: 23 [25088/37814 (66%)]\tLoss: 1.835011\n",
            "{SqueezeNet} Train Epoch: 23 [25600/37814 (68%)]\tLoss: 1.860908\n",
            "{SqueezeNet} Train Epoch: 23 [26112/37814 (69%)]\tLoss: 1.866989\n",
            "{SqueezeNet} Train Epoch: 23 [26624/37814 (70%)]\tLoss: 1.834763\n",
            "{SqueezeNet} Train Epoch: 23 [27136/37814 (72%)]\tLoss: 1.782405\n",
            "{SqueezeNet} Train Epoch: 23 [27648/37814 (73%)]\tLoss: 1.799692\n",
            "{SqueezeNet} Train Epoch: 23 [28160/37814 (74%)]\tLoss: 1.816723\n",
            "{SqueezeNet} Train Epoch: 23 [28672/37814 (76%)]\tLoss: 1.832077\n",
            "{SqueezeNet} Train Epoch: 23 [29184/37814 (77%)]\tLoss: 1.871110\n",
            "{SqueezeNet} Train Epoch: 23 [29696/37814 (78%)]\tLoss: 1.786983\n",
            "{SqueezeNet} Train Epoch: 23 [30208/37814 (80%)]\tLoss: 1.776551\n",
            "{SqueezeNet} Train Epoch: 23 [30720/37814 (81%)]\tLoss: 1.833290\n",
            "{SqueezeNet} Train Epoch: 23 [31232/37814 (82%)]\tLoss: 1.778219\n",
            "{SqueezeNet} Train Epoch: 23 [31744/37814 (84%)]\tLoss: 1.807105\n",
            "{SqueezeNet} Train Epoch: 23 [32256/37814 (85%)]\tLoss: 1.841411\n",
            "{SqueezeNet} Train Epoch: 23 [32768/37814 (86%)]\tLoss: 1.856154\n",
            "{SqueezeNet} Train Epoch: 23 [33280/37814 (88%)]\tLoss: 1.823973\n",
            "{SqueezeNet} Train Epoch: 23 [33792/37814 (89%)]\tLoss: 1.855345\n",
            "{SqueezeNet} Train Epoch: 23 [34304/37814 (91%)]\tLoss: 1.877649\n",
            "{SqueezeNet} Train Epoch: 23 [34816/37814 (92%)]\tLoss: 1.823273\n",
            "{SqueezeNet} Train Epoch: 23 [35328/37814 (93%)]\tLoss: 1.817284\n",
            "{SqueezeNet} Train Epoch: 23 [35840/37814 (95%)]\tLoss: 1.854673\n",
            "{SqueezeNet} Train Epoch: 23 [36352/37814 (96%)]\tLoss: 1.852872\n",
            "{SqueezeNet} Train Epoch: 23 [36864/37814 (97%)]\tLoss: 1.753980\n",
            "{SqueezeNet} Train Epoch: 23 [31974/37814 (99%)]\tLoss: 1.815108\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8092, Accuracy: 1614/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.34342622756958 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e533454-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"6c7be0c4-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4f00813891"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e54db06-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_ca832fcbf3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e552520-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_e824ba5505"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e556aee-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"8e552520-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_20b99a39e9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e7b9a52-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"8e54db06-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9b9e32ab4c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e7d4654-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_36ee0f008e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e7d9514-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_2b90e28ecd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"8e7dcbf6-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"8e7d9514-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_d909e298d4"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 24 [0/37814 (0%)]\tLoss: 1.967869\n",
            "{AlexNet} Train Epoch: 24 [512/37814 (1%)]\tLoss: 1.981303\n",
            "{AlexNet} Train Epoch: 24 [1024/37814 (3%)]\tLoss: 1.966379\n",
            "{AlexNet} Train Epoch: 24 [1536/37814 (4%)]\tLoss: 1.980494\n",
            "{AlexNet} Train Epoch: 24 [2048/37814 (5%)]\tLoss: 1.948767\n",
            "{AlexNet} Train Epoch: 24 [2560/37814 (7%)]\tLoss: 1.978749\n",
            "{AlexNet} Train Epoch: 24 [3072/37814 (8%)]\tLoss: 1.985027\n",
            "{AlexNet} Train Epoch: 24 [3584/37814 (9%)]\tLoss: 1.968831\n",
            "{AlexNet} Train Epoch: 24 [4096/37814 (11%)]\tLoss: 2.002461\n",
            "{AlexNet} Train Epoch: 24 [4608/37814 (12%)]\tLoss: 1.909217\n",
            "{AlexNet} Train Epoch: 24 [5120/37814 (14%)]\tLoss: 1.970668\n",
            "{AlexNet} Train Epoch: 24 [5632/37814 (15%)]\tLoss: 2.000507\n",
            "{AlexNet} Train Epoch: 24 [6144/37814 (16%)]\tLoss: 1.948351\n",
            "{AlexNet} Train Epoch: 24 [6656/37814 (18%)]\tLoss: 1.994853\n",
            "{AlexNet} Train Epoch: 24 [7168/37814 (19%)]\tLoss: 2.002926\n",
            "{AlexNet} Train Epoch: 24 [7680/37814 (20%)]\tLoss: 2.016038\n",
            "{AlexNet} Train Epoch: 24 [8192/37814 (22%)]\tLoss: 1.946977\n",
            "{AlexNet} Train Epoch: 24 [8704/37814 (23%)]\tLoss: 1.932710\n",
            "{AlexNet} Train Epoch: 24 [9216/37814 (24%)]\tLoss: 1.957762\n",
            "{AlexNet} Train Epoch: 24 [9728/37814 (26%)]\tLoss: 1.969594\n",
            "{AlexNet} Train Epoch: 24 [10240/37814 (27%)]\tLoss: 1.976455\n",
            "{AlexNet} Train Epoch: 24 [10752/37814 (28%)]\tLoss: 1.960142\n",
            "{AlexNet} Train Epoch: 24 [11264/37814 (30%)]\tLoss: 1.997274\n",
            "{AlexNet} Train Epoch: 24 [11776/37814 (31%)]\tLoss: 1.950794\n",
            "{AlexNet} Train Epoch: 24 [12288/37814 (32%)]\tLoss: 1.955930\n",
            "{AlexNet} Train Epoch: 24 [12800/37814 (34%)]\tLoss: 1.981940\n",
            "{AlexNet} Train Epoch: 24 [13312/37814 (35%)]\tLoss: 1.992214\n",
            "{AlexNet} Train Epoch: 24 [13824/37814 (36%)]\tLoss: 1.940997\n",
            "{AlexNet} Train Epoch: 24 [14336/37814 (38%)]\tLoss: 1.979601\n",
            "{AlexNet} Train Epoch: 24 [14848/37814 (39%)]\tLoss: 2.025960\n",
            "{AlexNet} Train Epoch: 24 [15360/37814 (41%)]\tLoss: 2.004054\n",
            "{AlexNet} Train Epoch: 24 [15872/37814 (42%)]\tLoss: 1.982636\n",
            "{AlexNet} Train Epoch: 24 [16384/37814 (43%)]\tLoss: 1.969198\n",
            "{AlexNet} Train Epoch: 24 [16896/37814 (45%)]\tLoss: 1.984844\n",
            "{AlexNet} Train Epoch: 24 [17408/37814 (46%)]\tLoss: 1.946579\n",
            "{AlexNet} Train Epoch: 24 [17920/37814 (47%)]\tLoss: 1.972658\n",
            "{AlexNet} Train Epoch: 24 [18432/37814 (49%)]\tLoss: 2.013655\n",
            "{AlexNet} Train Epoch: 24 [18944/37814 (50%)]\tLoss: 1.973715\n",
            "{AlexNet} Train Epoch: 24 [19456/37814 (51%)]\tLoss: 2.021157\n",
            "{AlexNet} Train Epoch: 24 [19968/37814 (53%)]\tLoss: 1.951331\n",
            "{AlexNet} Train Epoch: 24 [20480/37814 (54%)]\tLoss: 1.992401\n",
            "{AlexNet} Train Epoch: 24 [20992/37814 (55%)]\tLoss: 1.948281\n",
            "{AlexNet} Train Epoch: 24 [21504/37814 (57%)]\tLoss: 2.003401\n",
            "{AlexNet} Train Epoch: 24 [22016/37814 (58%)]\tLoss: 1.929258\n",
            "{AlexNet} Train Epoch: 24 [22528/37814 (59%)]\tLoss: 1.940449\n",
            "{AlexNet} Train Epoch: 24 [23040/37814 (61%)]\tLoss: 2.031940\n",
            "{AlexNet} Train Epoch: 24 [23552/37814 (62%)]\tLoss: 1.970322\n",
            "{AlexNet} Train Epoch: 24 [24064/37814 (64%)]\tLoss: 1.983597\n",
            "{AlexNet} Train Epoch: 24 [24576/37814 (65%)]\tLoss: 2.014148\n",
            "{AlexNet} Train Epoch: 24 [25088/37814 (66%)]\tLoss: 2.029735\n",
            "{AlexNet} Train Epoch: 24 [25600/37814 (68%)]\tLoss: 1.957615\n",
            "{AlexNet} Train Epoch: 24 [26112/37814 (69%)]\tLoss: 1.962141\n",
            "{AlexNet} Train Epoch: 24 [26624/37814 (70%)]\tLoss: 1.966439\n",
            "{AlexNet} Train Epoch: 24 [27136/37814 (72%)]\tLoss: 2.008088\n",
            "{AlexNet} Train Epoch: 24 [27648/37814 (73%)]\tLoss: 1.970360\n",
            "{AlexNet} Train Epoch: 24 [28160/37814 (74%)]\tLoss: 1.957937\n",
            "{AlexNet} Train Epoch: 24 [28672/37814 (76%)]\tLoss: 2.001514\n",
            "{AlexNet} Train Epoch: 24 [29184/37814 (77%)]\tLoss: 2.005755\n",
            "{AlexNet} Train Epoch: 24 [29696/37814 (78%)]\tLoss: 1.996257\n",
            "{AlexNet} Train Epoch: 24 [30208/37814 (80%)]\tLoss: 1.997488\n",
            "{AlexNet} Train Epoch: 24 [30720/37814 (81%)]\tLoss: 1.982522\n",
            "{AlexNet} Train Epoch: 24 [31232/37814 (82%)]\tLoss: 1.984757\n",
            "{AlexNet} Train Epoch: 24 [31744/37814 (84%)]\tLoss: 1.989107\n",
            "{AlexNet} Train Epoch: 24 [32256/37814 (85%)]\tLoss: 1.972308\n",
            "{AlexNet} Train Epoch: 24 [32768/37814 (86%)]\tLoss: 1.948762\n",
            "{AlexNet} Train Epoch: 24 [33280/37814 (88%)]\tLoss: 1.947990\n",
            "{AlexNet} Train Epoch: 24 [33792/37814 (89%)]\tLoss: 2.019923\n",
            "{AlexNet} Train Epoch: 24 [34304/37814 (91%)]\tLoss: 1.944219\n",
            "{AlexNet} Train Epoch: 24 [34816/37814 (92%)]\tLoss: 1.947657\n",
            "{AlexNet} Train Epoch: 24 [35328/37814 (93%)]\tLoss: 1.984754\n",
            "{AlexNet} Train Epoch: 24 [35840/37814 (95%)]\tLoss: 2.008563\n",
            "{AlexNet} Train Epoch: 24 [36352/37814 (96%)]\tLoss: 1.973069\n",
            "{AlexNet} Train Epoch: 24 [36864/37814 (97%)]\tLoss: 1.960869\n",
            "{AlexNet} Train Epoch: 24 [31974/37814 (99%)]\tLoss: 1.996641\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9769, Accuracy: 1047/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.801952123641968 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 24 [0/37814 (0%)]\tLoss: 1.812603\n",
            "{SqueezeNet} Train Epoch: 24 [512/37814 (1%)]\tLoss: 1.808722\n",
            "{SqueezeNet} Train Epoch: 24 [1024/37814 (3%)]\tLoss: 1.828352\n",
            "{SqueezeNet} Train Epoch: 24 [1536/37814 (4%)]\tLoss: 1.791102\n",
            "{SqueezeNet} Train Epoch: 24 [2048/37814 (5%)]\tLoss: 1.849463\n",
            "{SqueezeNet} Train Epoch: 24 [2560/37814 (7%)]\tLoss: 1.795733\n",
            "{SqueezeNet} Train Epoch: 24 [3072/37814 (8%)]\tLoss: 1.799769\n",
            "{SqueezeNet} Train Epoch: 24 [3584/37814 (9%)]\tLoss: 1.931054\n",
            "{SqueezeNet} Train Epoch: 24 [4096/37814 (11%)]\tLoss: 1.834414\n",
            "{SqueezeNet} Train Epoch: 24 [4608/37814 (12%)]\tLoss: 1.839353\n",
            "{SqueezeNet} Train Epoch: 24 [5120/37814 (14%)]\tLoss: 1.812834\n",
            "{SqueezeNet} Train Epoch: 24 [5632/37814 (15%)]\tLoss: 1.884272\n",
            "{SqueezeNet} Train Epoch: 24 [6144/37814 (16%)]\tLoss: 1.909063\n",
            "{SqueezeNet} Train Epoch: 24 [6656/37814 (18%)]\tLoss: 1.853570\n",
            "{SqueezeNet} Train Epoch: 24 [7168/37814 (19%)]\tLoss: 1.782220\n",
            "{SqueezeNet} Train Epoch: 24 [7680/37814 (20%)]\tLoss: 1.817763\n",
            "{SqueezeNet} Train Epoch: 24 [8192/37814 (22%)]\tLoss: 1.784388\n",
            "{SqueezeNet} Train Epoch: 24 [8704/37814 (23%)]\tLoss: 1.795871\n",
            "{SqueezeNet} Train Epoch: 24 [9216/37814 (24%)]\tLoss: 1.838841\n",
            "{SqueezeNet} Train Epoch: 24 [9728/37814 (26%)]\tLoss: 1.799035\n",
            "{SqueezeNet} Train Epoch: 24 [10240/37814 (27%)]\tLoss: 1.832213\n",
            "{SqueezeNet} Train Epoch: 24 [10752/37814 (28%)]\tLoss: 1.819507\n",
            "{SqueezeNet} Train Epoch: 24 [11264/37814 (30%)]\tLoss: 1.783098\n",
            "{SqueezeNet} Train Epoch: 24 [11776/37814 (31%)]\tLoss: 1.850642\n",
            "{SqueezeNet} Train Epoch: 24 [12288/37814 (32%)]\tLoss: 1.817561\n",
            "{SqueezeNet} Train Epoch: 24 [12800/37814 (34%)]\tLoss: 1.805931\n",
            "{SqueezeNet} Train Epoch: 24 [13312/37814 (35%)]\tLoss: 1.759209\n",
            "{SqueezeNet} Train Epoch: 24 [13824/37814 (36%)]\tLoss: 1.837141\n",
            "{SqueezeNet} Train Epoch: 24 [14336/37814 (38%)]\tLoss: 1.811764\n",
            "{SqueezeNet} Train Epoch: 24 [14848/37814 (39%)]\tLoss: 1.779956\n",
            "{SqueezeNet} Train Epoch: 24 [15360/37814 (41%)]\tLoss: 1.782967\n",
            "{SqueezeNet} Train Epoch: 24 [15872/37814 (42%)]\tLoss: 1.771789\n",
            "{SqueezeNet} Train Epoch: 24 [16384/37814 (43%)]\tLoss: 1.813785\n",
            "{SqueezeNet} Train Epoch: 24 [16896/37814 (45%)]\tLoss: 1.826231\n",
            "{SqueezeNet} Train Epoch: 24 [17408/37814 (46%)]\tLoss: 1.795228\n",
            "{SqueezeNet} Train Epoch: 24 [17920/37814 (47%)]\tLoss: 1.823741\n",
            "{SqueezeNet} Train Epoch: 24 [18432/37814 (49%)]\tLoss: 1.790632\n",
            "{SqueezeNet} Train Epoch: 24 [18944/37814 (50%)]\tLoss: 1.803540\n",
            "{SqueezeNet} Train Epoch: 24 [19456/37814 (51%)]\tLoss: 1.833437\n",
            "{SqueezeNet} Train Epoch: 24 [19968/37814 (53%)]\tLoss: 1.851859\n",
            "{SqueezeNet} Train Epoch: 24 [20480/37814 (54%)]\tLoss: 1.866542\n",
            "{SqueezeNet} Train Epoch: 24 [20992/37814 (55%)]\tLoss: 1.799354\n",
            "{SqueezeNet} Train Epoch: 24 [21504/37814 (57%)]\tLoss: 1.820215\n",
            "{SqueezeNet} Train Epoch: 24 [22016/37814 (58%)]\tLoss: 1.841673\n",
            "{SqueezeNet} Train Epoch: 24 [22528/37814 (59%)]\tLoss: 1.830008\n",
            "{SqueezeNet} Train Epoch: 24 [23040/37814 (61%)]\tLoss: 1.866398\n",
            "{SqueezeNet} Train Epoch: 24 [23552/37814 (62%)]\tLoss: 1.827094\n",
            "{SqueezeNet} Train Epoch: 24 [24064/37814 (64%)]\tLoss: 1.886265\n",
            "{SqueezeNet} Train Epoch: 24 [24576/37814 (65%)]\tLoss: 1.842336\n",
            "{SqueezeNet} Train Epoch: 24 [25088/37814 (66%)]\tLoss: 1.847354\n",
            "{SqueezeNet} Train Epoch: 24 [25600/37814 (68%)]\tLoss: 1.832022\n",
            "{SqueezeNet} Train Epoch: 24 [26112/37814 (69%)]\tLoss: 1.788393\n",
            "{SqueezeNet} Train Epoch: 24 [26624/37814 (70%)]\tLoss: 1.819968\n",
            "{SqueezeNet} Train Epoch: 24 [27136/37814 (72%)]\tLoss: 1.791174\n",
            "{SqueezeNet} Train Epoch: 24 [27648/37814 (73%)]\tLoss: 1.780229\n",
            "{SqueezeNet} Train Epoch: 24 [28160/37814 (74%)]\tLoss: 1.851321\n",
            "{SqueezeNet} Train Epoch: 24 [28672/37814 (76%)]\tLoss: 1.782145\n",
            "{SqueezeNet} Train Epoch: 24 [29184/37814 (77%)]\tLoss: 1.813630\n",
            "{SqueezeNet} Train Epoch: 24 [29696/37814 (78%)]\tLoss: 1.898282\n",
            "{SqueezeNet} Train Epoch: 24 [30208/37814 (80%)]\tLoss: 1.873143\n",
            "{SqueezeNet} Train Epoch: 24 [30720/37814 (81%)]\tLoss: 1.770708\n",
            "{SqueezeNet} Train Epoch: 24 [31232/37814 (82%)]\tLoss: 1.800568\n",
            "{SqueezeNet} Train Epoch: 24 [31744/37814 (84%)]\tLoss: 1.806436\n",
            "{SqueezeNet} Train Epoch: 24 [32256/37814 (85%)]\tLoss: 1.905583\n",
            "{SqueezeNet} Train Epoch: 24 [32768/37814 (86%)]\tLoss: 1.844392\n",
            "{SqueezeNet} Train Epoch: 24 [33280/37814 (88%)]\tLoss: 1.842016\n",
            "{SqueezeNet} Train Epoch: 24 [33792/37814 (89%)]\tLoss: 1.860734\n",
            "{SqueezeNet} Train Epoch: 24 [34304/37814 (91%)]\tLoss: 1.848140\n",
            "{SqueezeNet} Train Epoch: 24 [34816/37814 (92%)]\tLoss: 1.788631\n",
            "{SqueezeNet} Train Epoch: 24 [35328/37814 (93%)]\tLoss: 1.829382\n",
            "{SqueezeNet} Train Epoch: 24 [35840/37814 (95%)]\tLoss: 1.883061\n",
            "{SqueezeNet} Train Epoch: 24 [36352/37814 (96%)]\tLoss: 1.837874\n",
            "{SqueezeNet} Train Epoch: 24 [36864/37814 (97%)]\tLoss: 1.830342\n",
            "{SqueezeNet} Train Epoch: 24 [31974/37814 (99%)]\tLoss: 1.742029\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8062, Accuracy: 1623/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.50954580307007 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b00fc9f4-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"8e7d4654-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_db1b802d60"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b0116ebc-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_c889e3f95a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b011d564-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_08b8051119"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b012355e-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b011d564-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_14c5fc82d0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b040084e-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b0116ebc-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0fa5d77d65"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b0412a3a-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_6a7f972a3a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b04167ac-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_7a33d84840"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b041a442-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b04167ac-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_df8ddaddaf"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 25 [0/37814 (0%)]\tLoss: 2.005100\n",
            "{AlexNet} Train Epoch: 25 [512/37814 (1%)]\tLoss: 2.037818\n",
            "{AlexNet} Train Epoch: 25 [1024/37814 (3%)]\tLoss: 1.948735\n",
            "{AlexNet} Train Epoch: 25 [1536/37814 (4%)]\tLoss: 1.996466\n",
            "{AlexNet} Train Epoch: 25 [2048/37814 (5%)]\tLoss: 1.943338\n",
            "{AlexNet} Train Epoch: 25 [2560/37814 (7%)]\tLoss: 1.993633\n",
            "{AlexNet} Train Epoch: 25 [3072/37814 (8%)]\tLoss: 1.987577\n",
            "{AlexNet} Train Epoch: 25 [3584/37814 (9%)]\tLoss: 2.020573\n",
            "{AlexNet} Train Epoch: 25 [4096/37814 (11%)]\tLoss: 1.953903\n",
            "{AlexNet} Train Epoch: 25 [4608/37814 (12%)]\tLoss: 1.975585\n",
            "{AlexNet} Train Epoch: 25 [5120/37814 (14%)]\tLoss: 1.968266\n",
            "{AlexNet} Train Epoch: 25 [5632/37814 (15%)]\tLoss: 1.950735\n",
            "{AlexNet} Train Epoch: 25 [6144/37814 (16%)]\tLoss: 1.972490\n",
            "{AlexNet} Train Epoch: 25 [6656/37814 (18%)]\tLoss: 1.969501\n",
            "{AlexNet} Train Epoch: 25 [7168/37814 (19%)]\tLoss: 1.975490\n",
            "{AlexNet} Train Epoch: 25 [7680/37814 (20%)]\tLoss: 1.976369\n",
            "{AlexNet} Train Epoch: 25 [8192/37814 (22%)]\tLoss: 2.002053\n",
            "{AlexNet} Train Epoch: 25 [8704/37814 (23%)]\tLoss: 1.971262\n",
            "{AlexNet} Train Epoch: 25 [9216/37814 (24%)]\tLoss: 1.955139\n",
            "{AlexNet} Train Epoch: 25 [9728/37814 (26%)]\tLoss: 1.974054\n",
            "{AlexNet} Train Epoch: 25 [10240/37814 (27%)]\tLoss: 1.983654\n",
            "{AlexNet} Train Epoch: 25 [10752/37814 (28%)]\tLoss: 1.963156\n",
            "{AlexNet} Train Epoch: 25 [11264/37814 (30%)]\tLoss: 1.908685\n",
            "{AlexNet} Train Epoch: 25 [11776/37814 (31%)]\tLoss: 1.921009\n",
            "{AlexNet} Train Epoch: 25 [12288/37814 (32%)]\tLoss: 1.960230\n",
            "{AlexNet} Train Epoch: 25 [12800/37814 (34%)]\tLoss: 1.943863\n",
            "{AlexNet} Train Epoch: 25 [13312/37814 (35%)]\tLoss: 1.972599\n",
            "{AlexNet} Train Epoch: 25 [13824/37814 (36%)]\tLoss: 1.974197\n",
            "{AlexNet} Train Epoch: 25 [14336/37814 (38%)]\tLoss: 2.013701\n",
            "{AlexNet} Train Epoch: 25 [14848/37814 (39%)]\tLoss: 1.987006\n",
            "{AlexNet} Train Epoch: 25 [15360/37814 (41%)]\tLoss: 1.976478\n",
            "{AlexNet} Train Epoch: 25 [15872/37814 (42%)]\tLoss: 1.993384\n",
            "{AlexNet} Train Epoch: 25 [16384/37814 (43%)]\tLoss: 1.995591\n",
            "{AlexNet} Train Epoch: 25 [16896/37814 (45%)]\tLoss: 1.967630\n",
            "{AlexNet} Train Epoch: 25 [17408/37814 (46%)]\tLoss: 1.938162\n",
            "{AlexNet} Train Epoch: 25 [17920/37814 (47%)]\tLoss: 1.958619\n",
            "{AlexNet} Train Epoch: 25 [18432/37814 (49%)]\tLoss: 1.998279\n",
            "{AlexNet} Train Epoch: 25 [18944/37814 (50%)]\tLoss: 1.946804\n",
            "{AlexNet} Train Epoch: 25 [19456/37814 (51%)]\tLoss: 1.958134\n",
            "{AlexNet} Train Epoch: 25 [19968/37814 (53%)]\tLoss: 1.997754\n",
            "{AlexNet} Train Epoch: 25 [20480/37814 (54%)]\tLoss: 2.028703\n",
            "{AlexNet} Train Epoch: 25 [20992/37814 (55%)]\tLoss: 1.982490\n",
            "{AlexNet} Train Epoch: 25 [21504/37814 (57%)]\tLoss: 2.008388\n",
            "{AlexNet} Train Epoch: 25 [22016/37814 (58%)]\tLoss: 1.956054\n",
            "{AlexNet} Train Epoch: 25 [22528/37814 (59%)]\tLoss: 1.957987\n",
            "{AlexNet} Train Epoch: 25 [23040/37814 (61%)]\tLoss: 2.004954\n",
            "{AlexNet} Train Epoch: 25 [23552/37814 (62%)]\tLoss: 1.949023\n",
            "{AlexNet} Train Epoch: 25 [24064/37814 (64%)]\tLoss: 1.977050\n",
            "{AlexNet} Train Epoch: 25 [24576/37814 (65%)]\tLoss: 1.974186\n",
            "{AlexNet} Train Epoch: 25 [25088/37814 (66%)]\tLoss: 2.002929\n",
            "{AlexNet} Train Epoch: 25 [25600/37814 (68%)]\tLoss: 1.995275\n",
            "{AlexNet} Train Epoch: 25 [26112/37814 (69%)]\tLoss: 1.995905\n",
            "{AlexNet} Train Epoch: 25 [26624/37814 (70%)]\tLoss: 2.003099\n",
            "{AlexNet} Train Epoch: 25 [27136/37814 (72%)]\tLoss: 1.974575\n",
            "{AlexNet} Train Epoch: 25 [27648/37814 (73%)]\tLoss: 1.942009\n",
            "{AlexNet} Train Epoch: 25 [28160/37814 (74%)]\tLoss: 1.981365\n",
            "{AlexNet} Train Epoch: 25 [28672/37814 (76%)]\tLoss: 1.944656\n",
            "{AlexNet} Train Epoch: 25 [29184/37814 (77%)]\tLoss: 1.981663\n",
            "{AlexNet} Train Epoch: 25 [29696/37814 (78%)]\tLoss: 2.052597\n",
            "{AlexNet} Train Epoch: 25 [30208/37814 (80%)]\tLoss: 1.938607\n",
            "{AlexNet} Train Epoch: 25 [30720/37814 (81%)]\tLoss: 1.966239\n",
            "{AlexNet} Train Epoch: 25 [31232/37814 (82%)]\tLoss: 1.964503\n",
            "{AlexNet} Train Epoch: 25 [31744/37814 (84%)]\tLoss: 1.965272\n",
            "{AlexNet} Train Epoch: 25 [32256/37814 (85%)]\tLoss: 1.979658\n",
            "{AlexNet} Train Epoch: 25 [32768/37814 (86%)]\tLoss: 1.995147\n",
            "{AlexNet} Train Epoch: 25 [33280/37814 (88%)]\tLoss: 1.964933\n",
            "{AlexNet} Train Epoch: 25 [33792/37814 (89%)]\tLoss: 1.970370\n",
            "{AlexNet} Train Epoch: 25 [34304/37814 (91%)]\tLoss: 2.002414\n",
            "{AlexNet} Train Epoch: 25 [34816/37814 (92%)]\tLoss: 1.967205\n",
            "{AlexNet} Train Epoch: 25 [35328/37814 (93%)]\tLoss: 1.964516\n",
            "{AlexNet} Train Epoch: 25 [35840/37814 (95%)]\tLoss: 1.936102\n",
            "{AlexNet} Train Epoch: 25 [36352/37814 (96%)]\tLoss: 1.961322\n",
            "{AlexNet} Train Epoch: 25 [36864/37814 (97%)]\tLoss: 1.970466\n",
            "{AlexNet} Train Epoch: 25 [31974/37814 (99%)]\tLoss: 2.003690\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9837, Accuracy: 1055/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.87557578086853 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 25 [0/37814 (0%)]\tLoss: 1.815532\n",
            "{SqueezeNet} Train Epoch: 25 [512/37814 (1%)]\tLoss: 1.799943\n",
            "{SqueezeNet} Train Epoch: 25 [1024/37814 (3%)]\tLoss: 1.797001\n",
            "{SqueezeNet} Train Epoch: 25 [1536/37814 (4%)]\tLoss: 1.852071\n",
            "{SqueezeNet} Train Epoch: 25 [2048/37814 (5%)]\tLoss: 1.806880\n",
            "{SqueezeNet} Train Epoch: 25 [2560/37814 (7%)]\tLoss: 1.816217\n",
            "{SqueezeNet} Train Epoch: 25 [3072/37814 (8%)]\tLoss: 1.767416\n",
            "{SqueezeNet} Train Epoch: 25 [3584/37814 (9%)]\tLoss: 1.794159\n",
            "{SqueezeNet} Train Epoch: 25 [4096/37814 (11%)]\tLoss: 1.852126\n",
            "{SqueezeNet} Train Epoch: 25 [4608/37814 (12%)]\tLoss: 1.846238\n",
            "{SqueezeNet} Train Epoch: 25 [5120/37814 (14%)]\tLoss: 1.801566\n",
            "{SqueezeNet} Train Epoch: 25 [5632/37814 (15%)]\tLoss: 1.829669\n",
            "{SqueezeNet} Train Epoch: 25 [6144/37814 (16%)]\tLoss: 1.854605\n",
            "{SqueezeNet} Train Epoch: 25 [6656/37814 (18%)]\tLoss: 1.787264\n",
            "{SqueezeNet} Train Epoch: 25 [7168/37814 (19%)]\tLoss: 1.814285\n",
            "{SqueezeNet} Train Epoch: 25 [7680/37814 (20%)]\tLoss: 1.797138\n",
            "{SqueezeNet} Train Epoch: 25 [8192/37814 (22%)]\tLoss: 1.870839\n",
            "{SqueezeNet} Train Epoch: 25 [8704/37814 (23%)]\tLoss: 1.840078\n",
            "{SqueezeNet} Train Epoch: 25 [9216/37814 (24%)]\tLoss: 1.865021\n",
            "{SqueezeNet} Train Epoch: 25 [9728/37814 (26%)]\tLoss: 1.883650\n",
            "{SqueezeNet} Train Epoch: 25 [10240/37814 (27%)]\tLoss: 1.866179\n",
            "{SqueezeNet} Train Epoch: 25 [10752/37814 (28%)]\tLoss: 1.875088\n",
            "{SqueezeNet} Train Epoch: 25 [11264/37814 (30%)]\tLoss: 1.875429\n",
            "{SqueezeNet} Train Epoch: 25 [11776/37814 (31%)]\tLoss: 1.792311\n",
            "{SqueezeNet} Train Epoch: 25 [12288/37814 (32%)]\tLoss: 1.849613\n",
            "{SqueezeNet} Train Epoch: 25 [12800/37814 (34%)]\tLoss: 1.892166\n",
            "{SqueezeNet} Train Epoch: 25 [13312/37814 (35%)]\tLoss: 1.837716\n",
            "{SqueezeNet} Train Epoch: 25 [13824/37814 (36%)]\tLoss: 1.886229\n",
            "{SqueezeNet} Train Epoch: 25 [14336/37814 (38%)]\tLoss: 1.811172\n",
            "{SqueezeNet} Train Epoch: 25 [14848/37814 (39%)]\tLoss: 1.776577\n",
            "{SqueezeNet} Train Epoch: 25 [15360/37814 (41%)]\tLoss: 1.799901\n",
            "{SqueezeNet} Train Epoch: 25 [15872/37814 (42%)]\tLoss: 1.851517\n",
            "{SqueezeNet} Train Epoch: 25 [16384/37814 (43%)]\tLoss: 1.831753\n",
            "{SqueezeNet} Train Epoch: 25 [16896/37814 (45%)]\tLoss: 1.822962\n",
            "{SqueezeNet} Train Epoch: 25 [17408/37814 (46%)]\tLoss: 1.824170\n",
            "{SqueezeNet} Train Epoch: 25 [17920/37814 (47%)]\tLoss: 1.810514\n",
            "{SqueezeNet} Train Epoch: 25 [18432/37814 (49%)]\tLoss: 1.808745\n",
            "{SqueezeNet} Train Epoch: 25 [18944/37814 (50%)]\tLoss: 1.850777\n",
            "{SqueezeNet} Train Epoch: 25 [19456/37814 (51%)]\tLoss: 1.840917\n",
            "{SqueezeNet} Train Epoch: 25 [19968/37814 (53%)]\tLoss: 1.863884\n",
            "{SqueezeNet} Train Epoch: 25 [20480/37814 (54%)]\tLoss: 1.860507\n",
            "{SqueezeNet} Train Epoch: 25 [20992/37814 (55%)]\tLoss: 1.812474\n",
            "{SqueezeNet} Train Epoch: 25 [21504/37814 (57%)]\tLoss: 1.810870\n",
            "{SqueezeNet} Train Epoch: 25 [22016/37814 (58%)]\tLoss: 1.825844\n",
            "{SqueezeNet} Train Epoch: 25 [22528/37814 (59%)]\tLoss: 1.816892\n",
            "{SqueezeNet} Train Epoch: 25 [23040/37814 (61%)]\tLoss: 1.824985\n",
            "{SqueezeNet} Train Epoch: 25 [23552/37814 (62%)]\tLoss: 1.794103\n",
            "{SqueezeNet} Train Epoch: 25 [24064/37814 (64%)]\tLoss: 1.814038\n",
            "{SqueezeNet} Train Epoch: 25 [24576/37814 (65%)]\tLoss: 1.839894\n",
            "{SqueezeNet} Train Epoch: 25 [25088/37814 (66%)]\tLoss: 1.830587\n",
            "{SqueezeNet} Train Epoch: 25 [25600/37814 (68%)]\tLoss: 1.787028\n",
            "{SqueezeNet} Train Epoch: 25 [26112/37814 (69%)]\tLoss: 1.891680\n",
            "{SqueezeNet} Train Epoch: 25 [26624/37814 (70%)]\tLoss: 1.768060\n",
            "{SqueezeNet} Train Epoch: 25 [27136/37814 (72%)]\tLoss: 1.820081\n",
            "{SqueezeNet} Train Epoch: 25 [27648/37814 (73%)]\tLoss: 1.821892\n",
            "{SqueezeNet} Train Epoch: 25 [28160/37814 (74%)]\tLoss: 1.787649\n",
            "{SqueezeNet} Train Epoch: 25 [28672/37814 (76%)]\tLoss: 1.874282\n",
            "{SqueezeNet} Train Epoch: 25 [29184/37814 (77%)]\tLoss: 1.820978\n",
            "{SqueezeNet} Train Epoch: 25 [29696/37814 (78%)]\tLoss: 1.799487\n",
            "{SqueezeNet} Train Epoch: 25 [30208/37814 (80%)]\tLoss: 1.888765\n",
            "{SqueezeNet} Train Epoch: 25 [30720/37814 (81%)]\tLoss: 1.807176\n",
            "{SqueezeNet} Train Epoch: 25 [31232/37814 (82%)]\tLoss: 1.723116\n",
            "{SqueezeNet} Train Epoch: 25 [31744/37814 (84%)]\tLoss: 1.853714\n",
            "{SqueezeNet} Train Epoch: 25 [32256/37814 (85%)]\tLoss: 1.905946\n",
            "{SqueezeNet} Train Epoch: 25 [32768/37814 (86%)]\tLoss: 1.808226\n",
            "{SqueezeNet} Train Epoch: 25 [33280/37814 (88%)]\tLoss: 1.828324\n",
            "{SqueezeNet} Train Epoch: 25 [33792/37814 (89%)]\tLoss: 1.856665\n",
            "{SqueezeNet} Train Epoch: 25 [34304/37814 (91%)]\tLoss: 1.875456\n",
            "{SqueezeNet} Train Epoch: 25 [34816/37814 (92%)]\tLoss: 1.751263\n",
            "{SqueezeNet} Train Epoch: 25 [35328/37814 (93%)]\tLoss: 1.796699\n",
            "{SqueezeNet} Train Epoch: 25 [35840/37814 (95%)]\tLoss: 1.834414\n",
            "{SqueezeNet} Train Epoch: 25 [36352/37814 (96%)]\tLoss: 1.930668\n",
            "{SqueezeNet} Train Epoch: 25 [36864/37814 (97%)]\tLoss: 1.855414\n",
            "{SqueezeNet} Train Epoch: 25 [31974/37814 (99%)]\tLoss: 1.806988\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8128, Accuracy: 1624/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.466279983520508 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1d7e76a-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b0412a3a-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_23b9de2258"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1d93fd4-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_8df39cde00"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1d98dcc-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_6e7c47568c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1d9d070-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d1d98dcc-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f7a812cd39"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1fe0986-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d1d93fd4-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_eee21f10a9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1ff95a8-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_f300bfb23c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d1ffc99c-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_98036589b1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d20012ee-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d1ffc99c-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_977f42945c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 26 [0/37814 (0%)]\tLoss: 2.006431\n",
            "{AlexNet} Train Epoch: 26 [512/37814 (1%)]\tLoss: 1.936245\n",
            "{AlexNet} Train Epoch: 26 [1024/37814 (3%)]\tLoss: 1.961963\n",
            "{AlexNet} Train Epoch: 26 [1536/37814 (4%)]\tLoss: 1.979615\n",
            "{AlexNet} Train Epoch: 26 [2048/37814 (5%)]\tLoss: 1.919819\n",
            "{AlexNet} Train Epoch: 26 [2560/37814 (7%)]\tLoss: 2.012091\n",
            "{AlexNet} Train Epoch: 26 [3072/37814 (8%)]\tLoss: 1.964557\n",
            "{AlexNet} Train Epoch: 26 [3584/37814 (9%)]\tLoss: 1.966851\n",
            "{AlexNet} Train Epoch: 26 [4096/37814 (11%)]\tLoss: 1.950585\n",
            "{AlexNet} Train Epoch: 26 [4608/37814 (12%)]\tLoss: 2.003117\n",
            "{AlexNet} Train Epoch: 26 [5120/37814 (14%)]\tLoss: 2.014793\n",
            "{AlexNet} Train Epoch: 26 [5632/37814 (15%)]\tLoss: 1.995088\n",
            "{AlexNet} Train Epoch: 26 [6144/37814 (16%)]\tLoss: 2.018419\n",
            "{AlexNet} Train Epoch: 26 [6656/37814 (18%)]\tLoss: 1.963787\n",
            "{AlexNet} Train Epoch: 26 [7168/37814 (19%)]\tLoss: 1.986502\n",
            "{AlexNet} Train Epoch: 26 [7680/37814 (20%)]\tLoss: 1.999696\n",
            "{AlexNet} Train Epoch: 26 [8192/37814 (22%)]\tLoss: 1.949536\n",
            "{AlexNet} Train Epoch: 26 [8704/37814 (23%)]\tLoss: 1.964973\n",
            "{AlexNet} Train Epoch: 26 [9216/37814 (24%)]\tLoss: 1.934836\n",
            "{AlexNet} Train Epoch: 26 [9728/37814 (26%)]\tLoss: 2.008991\n",
            "{AlexNet} Train Epoch: 26 [10240/37814 (27%)]\tLoss: 1.985947\n",
            "{AlexNet} Train Epoch: 26 [10752/37814 (28%)]\tLoss: 1.942591\n",
            "{AlexNet} Train Epoch: 26 [11264/37814 (30%)]\tLoss: 1.986080\n",
            "{AlexNet} Train Epoch: 26 [11776/37814 (31%)]\tLoss: 1.996033\n",
            "{AlexNet} Train Epoch: 26 [12288/37814 (32%)]\tLoss: 1.962040\n",
            "{AlexNet} Train Epoch: 26 [12800/37814 (34%)]\tLoss: 1.941409\n",
            "{AlexNet} Train Epoch: 26 [13312/37814 (35%)]\tLoss: 1.974630\n",
            "{AlexNet} Train Epoch: 26 [13824/37814 (36%)]\tLoss: 2.009182\n",
            "{AlexNet} Train Epoch: 26 [14336/37814 (38%)]\tLoss: 1.979801\n",
            "{AlexNet} Train Epoch: 26 [14848/37814 (39%)]\tLoss: 1.936735\n",
            "{AlexNet} Train Epoch: 26 [15360/37814 (41%)]\tLoss: 1.947682\n",
            "{AlexNet} Train Epoch: 26 [15872/37814 (42%)]\tLoss: 1.956506\n",
            "{AlexNet} Train Epoch: 26 [16384/37814 (43%)]\tLoss: 1.977626\n",
            "{AlexNet} Train Epoch: 26 [16896/37814 (45%)]\tLoss: 2.005390\n",
            "{AlexNet} Train Epoch: 26 [17408/37814 (46%)]\tLoss: 1.982384\n",
            "{AlexNet} Train Epoch: 26 [17920/37814 (47%)]\tLoss: 1.986606\n",
            "{AlexNet} Train Epoch: 26 [18432/37814 (49%)]\tLoss: 1.971678\n",
            "{AlexNet} Train Epoch: 26 [18944/37814 (50%)]\tLoss: 1.971371\n",
            "{AlexNet} Train Epoch: 26 [19456/37814 (51%)]\tLoss: 1.987096\n",
            "{AlexNet} Train Epoch: 26 [19968/37814 (53%)]\tLoss: 2.010038\n",
            "{AlexNet} Train Epoch: 26 [20480/37814 (54%)]\tLoss: 1.940855\n",
            "{AlexNet} Train Epoch: 26 [20992/37814 (55%)]\tLoss: 1.996917\n",
            "{AlexNet} Train Epoch: 26 [21504/37814 (57%)]\tLoss: 1.966901\n",
            "{AlexNet} Train Epoch: 26 [22016/37814 (58%)]\tLoss: 2.004809\n",
            "{AlexNet} Train Epoch: 26 [22528/37814 (59%)]\tLoss: 1.997929\n",
            "{AlexNet} Train Epoch: 26 [23040/37814 (61%)]\tLoss: 2.011103\n",
            "{AlexNet} Train Epoch: 26 [23552/37814 (62%)]\tLoss: 1.994628\n",
            "{AlexNet} Train Epoch: 26 [24064/37814 (64%)]\tLoss: 1.982825\n",
            "{AlexNet} Train Epoch: 26 [24576/37814 (65%)]\tLoss: 1.947547\n",
            "{AlexNet} Train Epoch: 26 [25088/37814 (66%)]\tLoss: 1.963009\n",
            "{AlexNet} Train Epoch: 26 [25600/37814 (68%)]\tLoss: 1.955116\n",
            "{AlexNet} Train Epoch: 26 [26112/37814 (69%)]\tLoss: 1.971384\n",
            "{AlexNet} Train Epoch: 26 [26624/37814 (70%)]\tLoss: 1.978816\n",
            "{AlexNet} Train Epoch: 26 [27136/37814 (72%)]\tLoss: 1.947340\n",
            "{AlexNet} Train Epoch: 26 [27648/37814 (73%)]\tLoss: 1.985812\n",
            "{AlexNet} Train Epoch: 26 [28160/37814 (74%)]\tLoss: 1.970615\n",
            "{AlexNet} Train Epoch: 26 [28672/37814 (76%)]\tLoss: 1.979008\n",
            "{AlexNet} Train Epoch: 26 [29184/37814 (77%)]\tLoss: 1.977883\n",
            "{AlexNet} Train Epoch: 26 [29696/37814 (78%)]\tLoss: 1.922167\n",
            "{AlexNet} Train Epoch: 26 [30208/37814 (80%)]\tLoss: 1.998003\n",
            "{AlexNet} Train Epoch: 26 [30720/37814 (81%)]\tLoss: 1.950342\n",
            "{AlexNet} Train Epoch: 26 [31232/37814 (82%)]\tLoss: 1.971132\n",
            "{AlexNet} Train Epoch: 26 [31744/37814 (84%)]\tLoss: 1.984686\n",
            "{AlexNet} Train Epoch: 26 [32256/37814 (85%)]\tLoss: 1.982430\n",
            "{AlexNet} Train Epoch: 26 [32768/37814 (86%)]\tLoss: 1.955703\n",
            "{AlexNet} Train Epoch: 26 [33280/37814 (88%)]\tLoss: 1.957180\n",
            "{AlexNet} Train Epoch: 26 [33792/37814 (89%)]\tLoss: 1.967493\n",
            "{AlexNet} Train Epoch: 26 [34304/37814 (91%)]\tLoss: 1.971263\n",
            "{AlexNet} Train Epoch: 26 [34816/37814 (92%)]\tLoss: 1.963540\n",
            "{AlexNet} Train Epoch: 26 [35328/37814 (93%)]\tLoss: 1.917845\n",
            "{AlexNet} Train Epoch: 26 [35840/37814 (95%)]\tLoss: 1.997257\n",
            "{AlexNet} Train Epoch: 26 [36352/37814 (96%)]\tLoss: 1.940599\n",
            "{AlexNet} Train Epoch: 26 [36864/37814 (97%)]\tLoss: 1.989480\n",
            "{AlexNet} Train Epoch: 26 [31974/37814 (99%)]\tLoss: 1.954051\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9754, Accuracy: 1024/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.76416850090027 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 26 [0/37814 (0%)]\tLoss: 1.810401\n",
            "{SqueezeNet} Train Epoch: 26 [512/37814 (1%)]\tLoss: 1.801740\n",
            "{SqueezeNet} Train Epoch: 26 [1024/37814 (3%)]\tLoss: 1.834837\n",
            "{SqueezeNet} Train Epoch: 26 [1536/37814 (4%)]\tLoss: 1.866032\n",
            "{SqueezeNet} Train Epoch: 26 [2048/37814 (5%)]\tLoss: 1.897239\n",
            "{SqueezeNet} Train Epoch: 26 [2560/37814 (7%)]\tLoss: 1.798655\n",
            "{SqueezeNet} Train Epoch: 26 [3072/37814 (8%)]\tLoss: 1.762009\n",
            "{SqueezeNet} Train Epoch: 26 [3584/37814 (9%)]\tLoss: 1.811753\n",
            "{SqueezeNet} Train Epoch: 26 [4096/37814 (11%)]\tLoss: 1.867506\n",
            "{SqueezeNet} Train Epoch: 26 [4608/37814 (12%)]\tLoss: 1.841755\n",
            "{SqueezeNet} Train Epoch: 26 [5120/37814 (14%)]\tLoss: 1.806152\n",
            "{SqueezeNet} Train Epoch: 26 [5632/37814 (15%)]\tLoss: 1.833232\n",
            "{SqueezeNet} Train Epoch: 26 [6144/37814 (16%)]\tLoss: 1.780002\n",
            "{SqueezeNet} Train Epoch: 26 [6656/37814 (18%)]\tLoss: 1.853148\n",
            "{SqueezeNet} Train Epoch: 26 [7168/37814 (19%)]\tLoss: 1.861317\n",
            "{SqueezeNet} Train Epoch: 26 [7680/37814 (20%)]\tLoss: 1.851986\n",
            "{SqueezeNet} Train Epoch: 26 [8192/37814 (22%)]\tLoss: 1.866494\n",
            "{SqueezeNet} Train Epoch: 26 [8704/37814 (23%)]\tLoss: 1.778144\n",
            "{SqueezeNet} Train Epoch: 26 [9216/37814 (24%)]\tLoss: 1.838191\n",
            "{SqueezeNet} Train Epoch: 26 [9728/37814 (26%)]\tLoss: 1.823115\n",
            "{SqueezeNet} Train Epoch: 26 [10240/37814 (27%)]\tLoss: 1.847034\n",
            "{SqueezeNet} Train Epoch: 26 [10752/37814 (28%)]\tLoss: 1.852627\n",
            "{SqueezeNet} Train Epoch: 26 [11264/37814 (30%)]\tLoss: 1.773118\n",
            "{SqueezeNet} Train Epoch: 26 [11776/37814 (31%)]\tLoss: 1.820007\n",
            "{SqueezeNet} Train Epoch: 26 [12288/37814 (32%)]\tLoss: 1.816384\n",
            "{SqueezeNet} Train Epoch: 26 [12800/37814 (34%)]\tLoss: 1.903799\n",
            "{SqueezeNet} Train Epoch: 26 [13312/37814 (35%)]\tLoss: 1.823260\n",
            "{SqueezeNet} Train Epoch: 26 [13824/37814 (36%)]\tLoss: 1.823137\n",
            "{SqueezeNet} Train Epoch: 26 [14336/37814 (38%)]\tLoss: 1.804496\n",
            "{SqueezeNet} Train Epoch: 26 [14848/37814 (39%)]\tLoss: 1.915109\n",
            "{SqueezeNet} Train Epoch: 26 [15360/37814 (41%)]\tLoss: 1.847547\n",
            "{SqueezeNet} Train Epoch: 26 [15872/37814 (42%)]\tLoss: 1.864394\n",
            "{SqueezeNet} Train Epoch: 26 [16384/37814 (43%)]\tLoss: 1.838729\n",
            "{SqueezeNet} Train Epoch: 26 [16896/37814 (45%)]\tLoss: 1.867359\n",
            "{SqueezeNet} Train Epoch: 26 [17408/37814 (46%)]\tLoss: 1.814967\n",
            "{SqueezeNet} Train Epoch: 26 [17920/37814 (47%)]\tLoss: 1.827415\n",
            "{SqueezeNet} Train Epoch: 26 [18432/37814 (49%)]\tLoss: 1.851788\n",
            "{SqueezeNet} Train Epoch: 26 [18944/37814 (50%)]\tLoss: 1.822073\n",
            "{SqueezeNet} Train Epoch: 26 [19456/37814 (51%)]\tLoss: 1.802113\n",
            "{SqueezeNet} Train Epoch: 26 [19968/37814 (53%)]\tLoss: 1.814753\n",
            "{SqueezeNet} Train Epoch: 26 [20480/37814 (54%)]\tLoss: 1.810893\n",
            "{SqueezeNet} Train Epoch: 26 [20992/37814 (55%)]\tLoss: 1.768112\n",
            "{SqueezeNet} Train Epoch: 26 [21504/37814 (57%)]\tLoss: 1.820355\n",
            "{SqueezeNet} Train Epoch: 26 [22016/37814 (58%)]\tLoss: 1.825618\n",
            "{SqueezeNet} Train Epoch: 26 [22528/37814 (59%)]\tLoss: 1.853457\n",
            "{SqueezeNet} Train Epoch: 26 [23040/37814 (61%)]\tLoss: 1.808064\n",
            "{SqueezeNet} Train Epoch: 26 [23552/37814 (62%)]\tLoss: 1.848172\n",
            "{SqueezeNet} Train Epoch: 26 [24064/37814 (64%)]\tLoss: 1.830526\n",
            "{SqueezeNet} Train Epoch: 26 [24576/37814 (65%)]\tLoss: 1.838288\n",
            "{SqueezeNet} Train Epoch: 26 [25088/37814 (66%)]\tLoss: 1.914590\n",
            "{SqueezeNet} Train Epoch: 26 [25600/37814 (68%)]\tLoss: 1.863944\n",
            "{SqueezeNet} Train Epoch: 26 [26112/37814 (69%)]\tLoss: 1.834312\n",
            "{SqueezeNet} Train Epoch: 26 [26624/37814 (70%)]\tLoss: 1.800962\n",
            "{SqueezeNet} Train Epoch: 26 [27136/37814 (72%)]\tLoss: 1.874589\n",
            "{SqueezeNet} Train Epoch: 26 [27648/37814 (73%)]\tLoss: 1.762758\n",
            "{SqueezeNet} Train Epoch: 26 [28160/37814 (74%)]\tLoss: 1.827334\n",
            "{SqueezeNet} Train Epoch: 26 [28672/37814 (76%)]\tLoss: 1.770347\n",
            "{SqueezeNet} Train Epoch: 26 [29184/37814 (77%)]\tLoss: 1.797359\n",
            "{SqueezeNet} Train Epoch: 26 [29696/37814 (78%)]\tLoss: 1.828042\n",
            "{SqueezeNet} Train Epoch: 26 [30208/37814 (80%)]\tLoss: 1.835086\n",
            "{SqueezeNet} Train Epoch: 26 [30720/37814 (81%)]\tLoss: 1.848540\n",
            "{SqueezeNet} Train Epoch: 26 [31232/37814 (82%)]\tLoss: 1.798334\n",
            "{SqueezeNet} Train Epoch: 26 [31744/37814 (84%)]\tLoss: 1.835835\n",
            "{SqueezeNet} Train Epoch: 26 [32256/37814 (85%)]\tLoss: 1.865695\n",
            "{SqueezeNet} Train Epoch: 26 [32768/37814 (86%)]\tLoss: 1.813628\n",
            "{SqueezeNet} Train Epoch: 26 [33280/37814 (88%)]\tLoss: 1.759831\n",
            "{SqueezeNet} Train Epoch: 26 [33792/37814 (89%)]\tLoss: 1.850491\n",
            "{SqueezeNet} Train Epoch: 26 [34304/37814 (91%)]\tLoss: 1.851046\n",
            "{SqueezeNet} Train Epoch: 26 [34816/37814 (92%)]\tLoss: 1.792147\n",
            "{SqueezeNet} Train Epoch: 26 [35328/37814 (93%)]\tLoss: 1.801048\n",
            "{SqueezeNet} Train Epoch: 26 [35840/37814 (95%)]\tLoss: 1.824596\n",
            "{SqueezeNet} Train Epoch: 26 [36352/37814 (96%)]\tLoss: 1.844765\n",
            "{SqueezeNet} Train Epoch: 26 [36864/37814 (97%)]\tLoss: 1.838930\n",
            "{SqueezeNet} Train Epoch: 26 [31974/37814 (99%)]\tLoss: 1.830904\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8121, Accuracy: 1591/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.462075233459473 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f38453ee-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d1ff95a8-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_926005cbd0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f3860630-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_e2d74a47dc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f38647e4-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_2cadac5b29"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f386a658-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f38647e4-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cc0bf8cd1d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f3af56de-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f3860630-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_5b5e0efa18"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f3b19d5e-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_e435a23b7c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f3b22c10-60e5-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_4e81f82d49"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f3b25974-60e5-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f3b22c10-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6b3649ca13"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 27 [0/37814 (0%)]\tLoss: 2.019555\n",
            "{AlexNet} Train Epoch: 27 [512/37814 (1%)]\tLoss: 2.006716\n",
            "{AlexNet} Train Epoch: 27 [1024/37814 (3%)]\tLoss: 1.996135\n",
            "{AlexNet} Train Epoch: 27 [1536/37814 (4%)]\tLoss: 1.994073\n",
            "{AlexNet} Train Epoch: 27 [2048/37814 (5%)]\tLoss: 2.002453\n",
            "{AlexNet} Train Epoch: 27 [2560/37814 (7%)]\tLoss: 1.972597\n",
            "{AlexNet} Train Epoch: 27 [3072/37814 (8%)]\tLoss: 1.973751\n",
            "{AlexNet} Train Epoch: 27 [3584/37814 (9%)]\tLoss: 1.970401\n",
            "{AlexNet} Train Epoch: 27 [4096/37814 (11%)]\tLoss: 1.935657\n",
            "{AlexNet} Train Epoch: 27 [4608/37814 (12%)]\tLoss: 1.946275\n",
            "{AlexNet} Train Epoch: 27 [5120/37814 (14%)]\tLoss: 1.973492\n",
            "{AlexNet} Train Epoch: 27 [5632/37814 (15%)]\tLoss: 1.948717\n",
            "{AlexNet} Train Epoch: 27 [6144/37814 (16%)]\tLoss: 1.962796\n",
            "{AlexNet} Train Epoch: 27 [6656/37814 (18%)]\tLoss: 1.948142\n",
            "{AlexNet} Train Epoch: 27 [7168/37814 (19%)]\tLoss: 2.002854\n",
            "{AlexNet} Train Epoch: 27 [7680/37814 (20%)]\tLoss: 1.969487\n",
            "{AlexNet} Train Epoch: 27 [8192/37814 (22%)]\tLoss: 1.974736\n",
            "{AlexNet} Train Epoch: 27 [8704/37814 (23%)]\tLoss: 1.963459\n",
            "{AlexNet} Train Epoch: 27 [9216/37814 (24%)]\tLoss: 1.958006\n",
            "{AlexNet} Train Epoch: 27 [9728/37814 (26%)]\tLoss: 1.920936\n",
            "{AlexNet} Train Epoch: 27 [10240/37814 (27%)]\tLoss: 1.991170\n",
            "{AlexNet} Train Epoch: 27 [10752/37814 (28%)]\tLoss: 1.970047\n",
            "{AlexNet} Train Epoch: 27 [11264/37814 (30%)]\tLoss: 1.965930\n",
            "{AlexNet} Train Epoch: 27 [11776/37814 (31%)]\tLoss: 2.040499\n",
            "{AlexNet} Train Epoch: 27 [12288/37814 (32%)]\tLoss: 2.025558\n",
            "{AlexNet} Train Epoch: 27 [12800/37814 (34%)]\tLoss: 2.002339\n",
            "{AlexNet} Train Epoch: 27 [13312/37814 (35%)]\tLoss: 1.960796\n",
            "{AlexNet} Train Epoch: 27 [13824/37814 (36%)]\tLoss: 1.981585\n",
            "{AlexNet} Train Epoch: 27 [14336/37814 (38%)]\tLoss: 1.984558\n",
            "{AlexNet} Train Epoch: 27 [14848/37814 (39%)]\tLoss: 1.993605\n",
            "{AlexNet} Train Epoch: 27 [15360/37814 (41%)]\tLoss: 1.954098\n",
            "{AlexNet} Train Epoch: 27 [15872/37814 (42%)]\tLoss: 2.009092\n",
            "{AlexNet} Train Epoch: 27 [16384/37814 (43%)]\tLoss: 1.979367\n",
            "{AlexNet} Train Epoch: 27 [16896/37814 (45%)]\tLoss: 1.972043\n",
            "{AlexNet} Train Epoch: 27 [17408/37814 (46%)]\tLoss: 1.986002\n",
            "{AlexNet} Train Epoch: 27 [17920/37814 (47%)]\tLoss: 1.988023\n",
            "{AlexNet} Train Epoch: 27 [18432/37814 (49%)]\tLoss: 2.033990\n",
            "{AlexNet} Train Epoch: 27 [18944/37814 (50%)]\tLoss: 1.929468\n",
            "{AlexNet} Train Epoch: 27 [19456/37814 (51%)]\tLoss: 2.000418\n",
            "{AlexNet} Train Epoch: 27 [19968/37814 (53%)]\tLoss: 1.982028\n",
            "{AlexNet} Train Epoch: 27 [20480/37814 (54%)]\tLoss: 2.029652\n",
            "{AlexNet} Train Epoch: 27 [20992/37814 (55%)]\tLoss: 1.962456\n",
            "{AlexNet} Train Epoch: 27 [21504/37814 (57%)]\tLoss: 1.996330\n",
            "{AlexNet} Train Epoch: 27 [22016/37814 (58%)]\tLoss: 1.980344\n",
            "{AlexNet} Train Epoch: 27 [22528/37814 (59%)]\tLoss: 1.981395\n",
            "{AlexNet} Train Epoch: 27 [23040/37814 (61%)]\tLoss: 1.978371\n",
            "{AlexNet} Train Epoch: 27 [23552/37814 (62%)]\tLoss: 2.005532\n",
            "{AlexNet} Train Epoch: 27 [24064/37814 (64%)]\tLoss: 1.957406\n",
            "{AlexNet} Train Epoch: 27 [24576/37814 (65%)]\tLoss: 2.001519\n",
            "{AlexNet} Train Epoch: 27 [25088/37814 (66%)]\tLoss: 1.975280\n",
            "{AlexNet} Train Epoch: 27 [25600/37814 (68%)]\tLoss: 1.968386\n",
            "{AlexNet} Train Epoch: 27 [26112/37814 (69%)]\tLoss: 1.971193\n",
            "{AlexNet} Train Epoch: 27 [26624/37814 (70%)]\tLoss: 1.981423\n",
            "{AlexNet} Train Epoch: 27 [27136/37814 (72%)]\tLoss: 2.005775\n",
            "{AlexNet} Train Epoch: 27 [27648/37814 (73%)]\tLoss: 1.950691\n",
            "{AlexNet} Train Epoch: 27 [28160/37814 (74%)]\tLoss: 1.950522\n",
            "{AlexNet} Train Epoch: 27 [28672/37814 (76%)]\tLoss: 1.947007\n",
            "{AlexNet} Train Epoch: 27 [29184/37814 (77%)]\tLoss: 1.966095\n",
            "{AlexNet} Train Epoch: 27 [29696/37814 (78%)]\tLoss: 1.959207\n",
            "{AlexNet} Train Epoch: 27 [30208/37814 (80%)]\tLoss: 2.015334\n",
            "{AlexNet} Train Epoch: 27 [30720/37814 (81%)]\tLoss: 1.977477\n",
            "{AlexNet} Train Epoch: 27 [31232/37814 (82%)]\tLoss: 2.022338\n",
            "{AlexNet} Train Epoch: 27 [31744/37814 (84%)]\tLoss: 1.999807\n",
            "{AlexNet} Train Epoch: 27 [32256/37814 (85%)]\tLoss: 1.953009\n",
            "{AlexNet} Train Epoch: 27 [32768/37814 (86%)]\tLoss: 1.969734\n",
            "{AlexNet} Train Epoch: 27 [33280/37814 (88%)]\tLoss: 1.969731\n",
            "{AlexNet} Train Epoch: 27 [33792/37814 (89%)]\tLoss: 1.981065\n",
            "{AlexNet} Train Epoch: 27 [34304/37814 (91%)]\tLoss: 2.019005\n",
            "{AlexNet} Train Epoch: 27 [34816/37814 (92%)]\tLoss: 1.900477\n",
            "{AlexNet} Train Epoch: 27 [35328/37814 (93%)]\tLoss: 1.959579\n",
            "{AlexNet} Train Epoch: 27 [35840/37814 (95%)]\tLoss: 1.971057\n",
            "{AlexNet} Train Epoch: 27 [36352/37814 (96%)]\tLoss: 1.950485\n",
            "{AlexNet} Train Epoch: 27 [36864/37814 (97%)]\tLoss: 1.991087\n",
            "{AlexNet} Train Epoch: 27 [31974/37814 (99%)]\tLoss: 1.989269\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9712, Accuracy: 1023/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.73962640762329 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 27 [0/37814 (0%)]\tLoss: 1.815870\n",
            "{SqueezeNet} Train Epoch: 27 [512/37814 (1%)]\tLoss: 1.841975\n",
            "{SqueezeNet} Train Epoch: 27 [1024/37814 (3%)]\tLoss: 1.862827\n",
            "{SqueezeNet} Train Epoch: 27 [1536/37814 (4%)]\tLoss: 1.767880\n",
            "{SqueezeNet} Train Epoch: 27 [2048/37814 (5%)]\tLoss: 1.874743\n",
            "{SqueezeNet} Train Epoch: 27 [2560/37814 (7%)]\tLoss: 1.795300\n",
            "{SqueezeNet} Train Epoch: 27 [3072/37814 (8%)]\tLoss: 1.785270\n",
            "{SqueezeNet} Train Epoch: 27 [3584/37814 (9%)]\tLoss: 1.856145\n",
            "{SqueezeNet} Train Epoch: 27 [4096/37814 (11%)]\tLoss: 1.841499\n",
            "{SqueezeNet} Train Epoch: 27 [4608/37814 (12%)]\tLoss: 1.796922\n",
            "{SqueezeNet} Train Epoch: 27 [5120/37814 (14%)]\tLoss: 1.801391\n",
            "{SqueezeNet} Train Epoch: 27 [5632/37814 (15%)]\tLoss: 1.840498\n",
            "{SqueezeNet} Train Epoch: 27 [6144/37814 (16%)]\tLoss: 1.857799\n",
            "{SqueezeNet} Train Epoch: 27 [6656/37814 (18%)]\tLoss: 1.802245\n",
            "{SqueezeNet} Train Epoch: 27 [7168/37814 (19%)]\tLoss: 1.839921\n",
            "{SqueezeNet} Train Epoch: 27 [7680/37814 (20%)]\tLoss: 1.798437\n",
            "{SqueezeNet} Train Epoch: 27 [8192/37814 (22%)]\tLoss: 1.849415\n",
            "{SqueezeNet} Train Epoch: 27 [8704/37814 (23%)]\tLoss: 1.835870\n",
            "{SqueezeNet} Train Epoch: 27 [9216/37814 (24%)]\tLoss: 1.844380\n",
            "{SqueezeNet} Train Epoch: 27 [9728/37814 (26%)]\tLoss: 1.828307\n",
            "{SqueezeNet} Train Epoch: 27 [10240/37814 (27%)]\tLoss: 1.829311\n",
            "{SqueezeNet} Train Epoch: 27 [10752/37814 (28%)]\tLoss: 1.892631\n",
            "{SqueezeNet} Train Epoch: 27 [11264/37814 (30%)]\tLoss: 1.843309\n",
            "{SqueezeNet} Train Epoch: 27 [11776/37814 (31%)]\tLoss: 1.880486\n",
            "{SqueezeNet} Train Epoch: 27 [12288/37814 (32%)]\tLoss: 1.797715\n",
            "{SqueezeNet} Train Epoch: 27 [12800/37814 (34%)]\tLoss: 1.865734\n",
            "{SqueezeNet} Train Epoch: 27 [13312/37814 (35%)]\tLoss: 1.798751\n",
            "{SqueezeNet} Train Epoch: 27 [13824/37814 (36%)]\tLoss: 1.837419\n",
            "{SqueezeNet} Train Epoch: 27 [14336/37814 (38%)]\tLoss: 1.814350\n",
            "{SqueezeNet} Train Epoch: 27 [14848/37814 (39%)]\tLoss: 1.795922\n",
            "{SqueezeNet} Train Epoch: 27 [15360/37814 (41%)]\tLoss: 1.833554\n",
            "{SqueezeNet} Train Epoch: 27 [15872/37814 (42%)]\tLoss: 1.791036\n",
            "{SqueezeNet} Train Epoch: 27 [16384/37814 (43%)]\tLoss: 1.824677\n",
            "{SqueezeNet} Train Epoch: 27 [16896/37814 (45%)]\tLoss: 1.786888\n",
            "{SqueezeNet} Train Epoch: 27 [17408/37814 (46%)]\tLoss: 1.812167\n",
            "{SqueezeNet} Train Epoch: 27 [17920/37814 (47%)]\tLoss: 1.879472\n",
            "{SqueezeNet} Train Epoch: 27 [18432/37814 (49%)]\tLoss: 1.863948\n",
            "{SqueezeNet} Train Epoch: 27 [18944/37814 (50%)]\tLoss: 1.871290\n",
            "{SqueezeNet} Train Epoch: 27 [19456/37814 (51%)]\tLoss: 1.814613\n",
            "{SqueezeNet} Train Epoch: 27 [19968/37814 (53%)]\tLoss: 1.818983\n",
            "{SqueezeNet} Train Epoch: 27 [20480/37814 (54%)]\tLoss: 1.778531\n",
            "{SqueezeNet} Train Epoch: 27 [20992/37814 (55%)]\tLoss: 1.866883\n",
            "{SqueezeNet} Train Epoch: 27 [21504/37814 (57%)]\tLoss: 1.805246\n",
            "{SqueezeNet} Train Epoch: 27 [22016/37814 (58%)]\tLoss: 1.784468\n",
            "{SqueezeNet} Train Epoch: 27 [22528/37814 (59%)]\tLoss: 1.809430\n",
            "{SqueezeNet} Train Epoch: 27 [23040/37814 (61%)]\tLoss: 1.820820\n",
            "{SqueezeNet} Train Epoch: 27 [23552/37814 (62%)]\tLoss: 1.769014\n",
            "{SqueezeNet} Train Epoch: 27 [24064/37814 (64%)]\tLoss: 1.796507\n",
            "{SqueezeNet} Train Epoch: 27 [24576/37814 (65%)]\tLoss: 1.849860\n",
            "{SqueezeNet} Train Epoch: 27 [25088/37814 (66%)]\tLoss: 1.769675\n",
            "{SqueezeNet} Train Epoch: 27 [25600/37814 (68%)]\tLoss: 1.832073\n",
            "{SqueezeNet} Train Epoch: 27 [26112/37814 (69%)]\tLoss: 1.794885\n",
            "{SqueezeNet} Train Epoch: 27 [26624/37814 (70%)]\tLoss: 1.821090\n",
            "{SqueezeNet} Train Epoch: 27 [27136/37814 (72%)]\tLoss: 1.848346\n",
            "{SqueezeNet} Train Epoch: 27 [27648/37814 (73%)]\tLoss: 1.893096\n",
            "{SqueezeNet} Train Epoch: 27 [28160/37814 (74%)]\tLoss: 1.760903\n",
            "{SqueezeNet} Train Epoch: 27 [28672/37814 (76%)]\tLoss: 1.855628\n",
            "{SqueezeNet} Train Epoch: 27 [29184/37814 (77%)]\tLoss: 1.808985\n",
            "{SqueezeNet} Train Epoch: 27 [29696/37814 (78%)]\tLoss: 1.818435\n",
            "{SqueezeNet} Train Epoch: 27 [30208/37814 (80%)]\tLoss: 1.840050\n",
            "{SqueezeNet} Train Epoch: 27 [30720/37814 (81%)]\tLoss: 1.805079\n",
            "{SqueezeNet} Train Epoch: 27 [31232/37814 (82%)]\tLoss: 1.809110\n",
            "{SqueezeNet} Train Epoch: 27 [31744/37814 (84%)]\tLoss: 1.862256\n",
            "{SqueezeNet} Train Epoch: 27 [32256/37814 (85%)]\tLoss: 1.802464\n",
            "{SqueezeNet} Train Epoch: 27 [32768/37814 (86%)]\tLoss: 1.835756\n",
            "{SqueezeNet} Train Epoch: 27 [33280/37814 (88%)]\tLoss: 1.870124\n",
            "{SqueezeNet} Train Epoch: 27 [33792/37814 (89%)]\tLoss: 1.842787\n",
            "{SqueezeNet} Train Epoch: 27 [34304/37814 (91%)]\tLoss: 1.797694\n",
            "{SqueezeNet} Train Epoch: 27 [34816/37814 (92%)]\tLoss: 1.832605\n",
            "{SqueezeNet} Train Epoch: 27 [35328/37814 (93%)]\tLoss: 1.867228\n",
            "{SqueezeNet} Train Epoch: 27 [35840/37814 (95%)]\tLoss: 1.839490\n",
            "{SqueezeNet} Train Epoch: 27 [36352/37814 (96%)]\tLoss: 1.838993\n",
            "{SqueezeNet} Train Epoch: 27 [36864/37814 (97%)]\tLoss: 1.847676\n",
            "{SqueezeNet} Train Epoch: 27 [31974/37814 (99%)]\tLoss: 1.785282\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8018, Accuracy: 1607/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.26155924797058 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"15149604-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f3b19d5e-60e5-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0e36d301ef"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1516418e-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9c839af8c9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"15167d7a-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_3a930fccfe"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1516e210-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"15167d7a-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_26e11a3b37"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"153b91d2-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1516418e-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b29a50b75f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"153c92da-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_4938a19249"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"153ccf02-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_8ad1d85ad7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"153d06f2-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"153ccf02-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6829cd8740"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 28 [0/37814 (0%)]\tLoss: 2.001225\n",
            "{AlexNet} Train Epoch: 28 [512/37814 (1%)]\tLoss: 1.994980\n",
            "{AlexNet} Train Epoch: 28 [1024/37814 (3%)]\tLoss: 1.933355\n",
            "{AlexNet} Train Epoch: 28 [1536/37814 (4%)]\tLoss: 1.952535\n",
            "{AlexNet} Train Epoch: 28 [2048/37814 (5%)]\tLoss: 1.971391\n",
            "{AlexNet} Train Epoch: 28 [2560/37814 (7%)]\tLoss: 1.930292\n",
            "{AlexNet} Train Epoch: 28 [3072/37814 (8%)]\tLoss: 1.984375\n",
            "{AlexNet} Train Epoch: 28 [3584/37814 (9%)]\tLoss: 1.975341\n",
            "{AlexNet} Train Epoch: 28 [4096/37814 (11%)]\tLoss: 2.024881\n",
            "{AlexNet} Train Epoch: 28 [4608/37814 (12%)]\tLoss: 1.952390\n",
            "{AlexNet} Train Epoch: 28 [5120/37814 (14%)]\tLoss: 1.976875\n",
            "{AlexNet} Train Epoch: 28 [5632/37814 (15%)]\tLoss: 1.968767\n",
            "{AlexNet} Train Epoch: 28 [6144/37814 (16%)]\tLoss: 1.996958\n",
            "{AlexNet} Train Epoch: 28 [6656/37814 (18%)]\tLoss: 2.014362\n",
            "{AlexNet} Train Epoch: 28 [7168/37814 (19%)]\tLoss: 1.945383\n",
            "{AlexNet} Train Epoch: 28 [7680/37814 (20%)]\tLoss: 2.001534\n",
            "{AlexNet} Train Epoch: 28 [8192/37814 (22%)]\tLoss: 1.973565\n",
            "{AlexNet} Train Epoch: 28 [8704/37814 (23%)]\tLoss: 2.020014\n",
            "{AlexNet} Train Epoch: 28 [9216/37814 (24%)]\tLoss: 2.007202\n",
            "{AlexNet} Train Epoch: 28 [9728/37814 (26%)]\tLoss: 1.938347\n",
            "{AlexNet} Train Epoch: 28 [10240/37814 (27%)]\tLoss: 1.989783\n",
            "{AlexNet} Train Epoch: 28 [10752/37814 (28%)]\tLoss: 1.960772\n",
            "{AlexNet} Train Epoch: 28 [11264/37814 (30%)]\tLoss: 1.961900\n",
            "{AlexNet} Train Epoch: 28 [11776/37814 (31%)]\tLoss: 1.953774\n",
            "{AlexNet} Train Epoch: 28 [12288/37814 (32%)]\tLoss: 1.964137\n",
            "{AlexNet} Train Epoch: 28 [12800/37814 (34%)]\tLoss: 1.968614\n",
            "{AlexNet} Train Epoch: 28 [13312/37814 (35%)]\tLoss: 1.985433\n",
            "{AlexNet} Train Epoch: 28 [13824/37814 (36%)]\tLoss: 1.967498\n",
            "{AlexNet} Train Epoch: 28 [14336/37814 (38%)]\tLoss: 2.008340\n",
            "{AlexNet} Train Epoch: 28 [14848/37814 (39%)]\tLoss: 1.978347\n",
            "{AlexNet} Train Epoch: 28 [15360/37814 (41%)]\tLoss: 1.945043\n",
            "{AlexNet} Train Epoch: 28 [15872/37814 (42%)]\tLoss: 1.944442\n",
            "{AlexNet} Train Epoch: 28 [16384/37814 (43%)]\tLoss: 1.981159\n",
            "{AlexNet} Train Epoch: 28 [16896/37814 (45%)]\tLoss: 1.997247\n",
            "{AlexNet} Train Epoch: 28 [17408/37814 (46%)]\tLoss: 1.964621\n",
            "{AlexNet} Train Epoch: 28 [17920/37814 (47%)]\tLoss: 1.983958\n",
            "{AlexNet} Train Epoch: 28 [18432/37814 (49%)]\tLoss: 2.000580\n",
            "{AlexNet} Train Epoch: 28 [18944/37814 (50%)]\tLoss: 1.954408\n",
            "{AlexNet} Train Epoch: 28 [19456/37814 (51%)]\tLoss: 1.965635\n",
            "{AlexNet} Train Epoch: 28 [19968/37814 (53%)]\tLoss: 1.991394\n",
            "{AlexNet} Train Epoch: 28 [20480/37814 (54%)]\tLoss: 1.946023\n",
            "{AlexNet} Train Epoch: 28 [20992/37814 (55%)]\tLoss: 1.939964\n",
            "{AlexNet} Train Epoch: 28 [21504/37814 (57%)]\tLoss: 1.958424\n",
            "{AlexNet} Train Epoch: 28 [22016/37814 (58%)]\tLoss: 1.922786\n",
            "{AlexNet} Train Epoch: 28 [22528/37814 (59%)]\tLoss: 1.994494\n",
            "{AlexNet} Train Epoch: 28 [23040/37814 (61%)]\tLoss: 1.992390\n",
            "{AlexNet} Train Epoch: 28 [23552/37814 (62%)]\tLoss: 1.969215\n",
            "{AlexNet} Train Epoch: 28 [24064/37814 (64%)]\tLoss: 1.949120\n",
            "{AlexNet} Train Epoch: 28 [24576/37814 (65%)]\tLoss: 1.956132\n",
            "{AlexNet} Train Epoch: 28 [25088/37814 (66%)]\tLoss: 1.988240\n",
            "{AlexNet} Train Epoch: 28 [25600/37814 (68%)]\tLoss: 2.012569\n",
            "{AlexNet} Train Epoch: 28 [26112/37814 (69%)]\tLoss: 1.919387\n",
            "{AlexNet} Train Epoch: 28 [26624/37814 (70%)]\tLoss: 1.967357\n",
            "{AlexNet} Train Epoch: 28 [27136/37814 (72%)]\tLoss: 2.022779\n",
            "{AlexNet} Train Epoch: 28 [27648/37814 (73%)]\tLoss: 2.025751\n",
            "{AlexNet} Train Epoch: 28 [28160/37814 (74%)]\tLoss: 2.031061\n",
            "{AlexNet} Train Epoch: 28 [28672/37814 (76%)]\tLoss: 2.004206\n",
            "{AlexNet} Train Epoch: 28 [29184/37814 (77%)]\tLoss: 1.947593\n",
            "{AlexNet} Train Epoch: 28 [29696/37814 (78%)]\tLoss: 1.987451\n",
            "{AlexNet} Train Epoch: 28 [30208/37814 (80%)]\tLoss: 1.964097\n",
            "{AlexNet} Train Epoch: 28 [30720/37814 (81%)]\tLoss: 1.982428\n",
            "{AlexNet} Train Epoch: 28 [31232/37814 (82%)]\tLoss: 1.960011\n",
            "{AlexNet} Train Epoch: 28 [31744/37814 (84%)]\tLoss: 1.996076\n",
            "{AlexNet} Train Epoch: 28 [32256/37814 (85%)]\tLoss: 2.022385\n",
            "{AlexNet} Train Epoch: 28 [32768/37814 (86%)]\tLoss: 1.987224\n",
            "{AlexNet} Train Epoch: 28 [33280/37814 (88%)]\tLoss: 1.957807\n",
            "{AlexNet} Train Epoch: 28 [33792/37814 (89%)]\tLoss: 2.003286\n",
            "{AlexNet} Train Epoch: 28 [34304/37814 (91%)]\tLoss: 1.929580\n",
            "{AlexNet} Train Epoch: 28 [34816/37814 (92%)]\tLoss: 1.994289\n",
            "{AlexNet} Train Epoch: 28 [35328/37814 (93%)]\tLoss: 1.973705\n",
            "{AlexNet} Train Epoch: 28 [35840/37814 (95%)]\tLoss: 1.949076\n",
            "{AlexNet} Train Epoch: 28 [36352/37814 (96%)]\tLoss: 1.940805\n",
            "{AlexNet} Train Epoch: 28 [36864/37814 (97%)]\tLoss: 1.959754\n",
            "{AlexNet} Train Epoch: 28 [31974/37814 (99%)]\tLoss: 2.001595\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9762, Accuracy: 1033/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.718026399612427 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 28 [0/37814 (0%)]\tLoss: 1.836938\n",
            "{SqueezeNet} Train Epoch: 28 [512/37814 (1%)]\tLoss: 1.854979\n",
            "{SqueezeNet} Train Epoch: 28 [1024/37814 (3%)]\tLoss: 1.862089\n",
            "{SqueezeNet} Train Epoch: 28 [1536/37814 (4%)]\tLoss: 1.835821\n",
            "{SqueezeNet} Train Epoch: 28 [2048/37814 (5%)]\tLoss: 1.850518\n",
            "{SqueezeNet} Train Epoch: 28 [2560/37814 (7%)]\tLoss: 1.851555\n",
            "{SqueezeNet} Train Epoch: 28 [3072/37814 (8%)]\tLoss: 1.879781\n",
            "{SqueezeNet} Train Epoch: 28 [3584/37814 (9%)]\tLoss: 1.844888\n",
            "{SqueezeNet} Train Epoch: 28 [4096/37814 (11%)]\tLoss: 1.821578\n",
            "{SqueezeNet} Train Epoch: 28 [4608/37814 (12%)]\tLoss: 1.842133\n",
            "{SqueezeNet} Train Epoch: 28 [5120/37814 (14%)]\tLoss: 1.879833\n",
            "{SqueezeNet} Train Epoch: 28 [5632/37814 (15%)]\tLoss: 1.819488\n",
            "{SqueezeNet} Train Epoch: 28 [6144/37814 (16%)]\tLoss: 1.845215\n",
            "{SqueezeNet} Train Epoch: 28 [6656/37814 (18%)]\tLoss: 1.844020\n",
            "{SqueezeNet} Train Epoch: 28 [7168/37814 (19%)]\tLoss: 1.815649\n",
            "{SqueezeNet} Train Epoch: 28 [7680/37814 (20%)]\tLoss: 1.840003\n",
            "{SqueezeNet} Train Epoch: 28 [8192/37814 (22%)]\tLoss: 1.820918\n",
            "{SqueezeNet} Train Epoch: 28 [8704/37814 (23%)]\tLoss: 1.836766\n",
            "{SqueezeNet} Train Epoch: 28 [9216/37814 (24%)]\tLoss: 1.806830\n",
            "{SqueezeNet} Train Epoch: 28 [9728/37814 (26%)]\tLoss: 1.822778\n",
            "{SqueezeNet} Train Epoch: 28 [10240/37814 (27%)]\tLoss: 1.790613\n",
            "{SqueezeNet} Train Epoch: 28 [10752/37814 (28%)]\tLoss: 1.847355\n",
            "{SqueezeNet} Train Epoch: 28 [11264/37814 (30%)]\tLoss: 1.850027\n",
            "{SqueezeNet} Train Epoch: 28 [11776/37814 (31%)]\tLoss: 1.856777\n",
            "{SqueezeNet} Train Epoch: 28 [12288/37814 (32%)]\tLoss: 1.803270\n",
            "{SqueezeNet} Train Epoch: 28 [12800/37814 (34%)]\tLoss: 1.891697\n",
            "{SqueezeNet} Train Epoch: 28 [13312/37814 (35%)]\tLoss: 1.817830\n",
            "{SqueezeNet} Train Epoch: 28 [13824/37814 (36%)]\tLoss: 1.854102\n",
            "{SqueezeNet} Train Epoch: 28 [14336/37814 (38%)]\tLoss: 1.791788\n",
            "{SqueezeNet} Train Epoch: 28 [14848/37814 (39%)]\tLoss: 1.837893\n",
            "{SqueezeNet} Train Epoch: 28 [15360/37814 (41%)]\tLoss: 1.798411\n",
            "{SqueezeNet} Train Epoch: 28 [15872/37814 (42%)]\tLoss: 1.858897\n",
            "{SqueezeNet} Train Epoch: 28 [16384/37814 (43%)]\tLoss: 1.791782\n",
            "{SqueezeNet} Train Epoch: 28 [16896/37814 (45%)]\tLoss: 1.780904\n",
            "{SqueezeNet} Train Epoch: 28 [17408/37814 (46%)]\tLoss: 1.827934\n",
            "{SqueezeNet} Train Epoch: 28 [17920/37814 (47%)]\tLoss: 1.875383\n",
            "{SqueezeNet} Train Epoch: 28 [18432/37814 (49%)]\tLoss: 1.854234\n",
            "{SqueezeNet} Train Epoch: 28 [18944/37814 (50%)]\tLoss: 1.811886\n",
            "{SqueezeNet} Train Epoch: 28 [19456/37814 (51%)]\tLoss: 1.788023\n",
            "{SqueezeNet} Train Epoch: 28 [19968/37814 (53%)]\tLoss: 1.845597\n",
            "{SqueezeNet} Train Epoch: 28 [20480/37814 (54%)]\tLoss: 1.836348\n",
            "{SqueezeNet} Train Epoch: 28 [20992/37814 (55%)]\tLoss: 1.787924\n",
            "{SqueezeNet} Train Epoch: 28 [21504/37814 (57%)]\tLoss: 1.797181\n",
            "{SqueezeNet} Train Epoch: 28 [22016/37814 (58%)]\tLoss: 1.758090\n",
            "{SqueezeNet} Train Epoch: 28 [22528/37814 (59%)]\tLoss: 1.831397\n",
            "{SqueezeNet} Train Epoch: 28 [23040/37814 (61%)]\tLoss: 1.831031\n",
            "{SqueezeNet} Train Epoch: 28 [23552/37814 (62%)]\tLoss: 1.791854\n",
            "{SqueezeNet} Train Epoch: 28 [24064/37814 (64%)]\tLoss: 1.856155\n",
            "{SqueezeNet} Train Epoch: 28 [24576/37814 (65%)]\tLoss: 1.836402\n",
            "{SqueezeNet} Train Epoch: 28 [25088/37814 (66%)]\tLoss: 1.817625\n",
            "{SqueezeNet} Train Epoch: 28 [25600/37814 (68%)]\tLoss: 1.892761\n",
            "{SqueezeNet} Train Epoch: 28 [26112/37814 (69%)]\tLoss: 1.792203\n",
            "{SqueezeNet} Train Epoch: 28 [26624/37814 (70%)]\tLoss: 1.853806\n",
            "{SqueezeNet} Train Epoch: 28 [27136/37814 (72%)]\tLoss: 1.904101\n",
            "{SqueezeNet} Train Epoch: 28 [27648/37814 (73%)]\tLoss: 1.879774\n",
            "{SqueezeNet} Train Epoch: 28 [28160/37814 (74%)]\tLoss: 1.844547\n",
            "{SqueezeNet} Train Epoch: 28 [28672/37814 (76%)]\tLoss: 1.856501\n",
            "{SqueezeNet} Train Epoch: 28 [29184/37814 (77%)]\tLoss: 1.907615\n",
            "{SqueezeNet} Train Epoch: 28 [29696/37814 (78%)]\tLoss: 1.825517\n",
            "{SqueezeNet} Train Epoch: 28 [30208/37814 (80%)]\tLoss: 1.834035\n",
            "{SqueezeNet} Train Epoch: 28 [30720/37814 (81%)]\tLoss: 1.856670\n",
            "{SqueezeNet} Train Epoch: 28 [31232/37814 (82%)]\tLoss: 1.831468\n",
            "{SqueezeNet} Train Epoch: 28 [31744/37814 (84%)]\tLoss: 1.777690\n",
            "{SqueezeNet} Train Epoch: 28 [32256/37814 (85%)]\tLoss: 1.808320\n",
            "{SqueezeNet} Train Epoch: 28 [32768/37814 (86%)]\tLoss: 1.814106\n",
            "{SqueezeNet} Train Epoch: 28 [33280/37814 (88%)]\tLoss: 1.878423\n",
            "{SqueezeNet} Train Epoch: 28 [33792/37814 (89%)]\tLoss: 1.821929\n",
            "{SqueezeNet} Train Epoch: 28 [34304/37814 (91%)]\tLoss: 1.811060\n",
            "{SqueezeNet} Train Epoch: 28 [34816/37814 (92%)]\tLoss: 1.802487\n",
            "{SqueezeNet} Train Epoch: 28 [35328/37814 (93%)]\tLoss: 1.810423\n",
            "{SqueezeNet} Train Epoch: 28 [35840/37814 (95%)]\tLoss: 1.845913\n",
            "{SqueezeNet} Train Epoch: 28 [36352/37814 (96%)]\tLoss: 1.823953\n",
            "{SqueezeNet} Train Epoch: 28 [36864/37814 (97%)]\tLoss: 1.821928\n",
            "{SqueezeNet} Train Epoch: 28 [31974/37814 (99%)]\tLoss: 1.749051\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8121, Accuracy: 1600/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.320255041122437 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"360ca0ae-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"153c92da-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0e0748fa57"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"360e799c-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_bc1abba065"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"360ebc04-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_780262e1c5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"360ef9e4-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"360ebc04-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_aafd729dc9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"36332404-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"360e799c-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e137795abd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"36341fd0-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_f49ecd24dc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"3634554a-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_d6e86baee4"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"36348a74-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"3634554a-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_5b7b59c475"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 29 [0/37814 (0%)]\tLoss: 1.976396\n",
            "{AlexNet} Train Epoch: 29 [512/37814 (1%)]\tLoss: 2.013790\n",
            "{AlexNet} Train Epoch: 29 [1024/37814 (3%)]\tLoss: 1.962975\n",
            "{AlexNet} Train Epoch: 29 [1536/37814 (4%)]\tLoss: 1.976375\n",
            "{AlexNet} Train Epoch: 29 [2048/37814 (5%)]\tLoss: 1.980424\n",
            "{AlexNet} Train Epoch: 29 [2560/37814 (7%)]\tLoss: 1.990760\n",
            "{AlexNet} Train Epoch: 29 [3072/37814 (8%)]\tLoss: 1.992453\n",
            "{AlexNet} Train Epoch: 29 [3584/37814 (9%)]\tLoss: 2.018374\n",
            "{AlexNet} Train Epoch: 29 [4096/37814 (11%)]\tLoss: 1.916958\n",
            "{AlexNet} Train Epoch: 29 [4608/37814 (12%)]\tLoss: 1.974558\n",
            "{AlexNet} Train Epoch: 29 [5120/37814 (14%)]\tLoss: 2.006700\n",
            "{AlexNet} Train Epoch: 29 [5632/37814 (15%)]\tLoss: 1.990040\n",
            "{AlexNet} Train Epoch: 29 [6144/37814 (16%)]\tLoss: 1.989856\n",
            "{AlexNet} Train Epoch: 29 [6656/37814 (18%)]\tLoss: 2.003672\n",
            "{AlexNet} Train Epoch: 29 [7168/37814 (19%)]\tLoss: 1.952255\n",
            "{AlexNet} Train Epoch: 29 [7680/37814 (20%)]\tLoss: 2.039050\n",
            "{AlexNet} Train Epoch: 29 [8192/37814 (22%)]\tLoss: 1.950300\n",
            "{AlexNet} Train Epoch: 29 [8704/37814 (23%)]\tLoss: 1.971282\n",
            "{AlexNet} Train Epoch: 29 [9216/37814 (24%)]\tLoss: 1.947800\n",
            "{AlexNet} Train Epoch: 29 [9728/37814 (26%)]\tLoss: 1.983613\n",
            "{AlexNet} Train Epoch: 29 [10240/37814 (27%)]\tLoss: 1.996000\n",
            "{AlexNet} Train Epoch: 29 [10752/37814 (28%)]\tLoss: 1.963469\n",
            "{AlexNet} Train Epoch: 29 [11264/37814 (30%)]\tLoss: 1.976985\n",
            "{AlexNet} Train Epoch: 29 [11776/37814 (31%)]\tLoss: 2.020977\n",
            "{AlexNet} Train Epoch: 29 [12288/37814 (32%)]\tLoss: 1.981435\n",
            "{AlexNet} Train Epoch: 29 [12800/37814 (34%)]\tLoss: 1.958288\n",
            "{AlexNet} Train Epoch: 29 [13312/37814 (35%)]\tLoss: 1.956725\n",
            "{AlexNet} Train Epoch: 29 [13824/37814 (36%)]\tLoss: 1.933442\n",
            "{AlexNet} Train Epoch: 29 [14336/37814 (38%)]\tLoss: 1.982754\n",
            "{AlexNet} Train Epoch: 29 [14848/37814 (39%)]\tLoss: 1.973507\n",
            "{AlexNet} Train Epoch: 29 [15360/37814 (41%)]\tLoss: 1.984436\n",
            "{AlexNet} Train Epoch: 29 [15872/37814 (42%)]\tLoss: 1.944469\n",
            "{AlexNet} Train Epoch: 29 [16384/37814 (43%)]\tLoss: 1.995029\n",
            "{AlexNet} Train Epoch: 29 [16896/37814 (45%)]\tLoss: 1.954186\n",
            "{AlexNet} Train Epoch: 29 [17408/37814 (46%)]\tLoss: 1.989695\n",
            "{AlexNet} Train Epoch: 29 [17920/37814 (47%)]\tLoss: 1.942870\n",
            "{AlexNet} Train Epoch: 29 [18432/37814 (49%)]\tLoss: 1.928962\n",
            "{AlexNet} Train Epoch: 29 [18944/37814 (50%)]\tLoss: 1.927396\n",
            "{AlexNet} Train Epoch: 29 [19456/37814 (51%)]\tLoss: 2.004302\n",
            "{AlexNet} Train Epoch: 29 [19968/37814 (53%)]\tLoss: 1.968577\n",
            "{AlexNet} Train Epoch: 29 [20480/37814 (54%)]\tLoss: 1.962188\n",
            "{AlexNet} Train Epoch: 29 [20992/37814 (55%)]\tLoss: 1.968793\n",
            "{AlexNet} Train Epoch: 29 [21504/37814 (57%)]\tLoss: 1.978965\n",
            "{AlexNet} Train Epoch: 29 [22016/37814 (58%)]\tLoss: 1.957810\n",
            "{AlexNet} Train Epoch: 29 [22528/37814 (59%)]\tLoss: 1.973973\n",
            "{AlexNet} Train Epoch: 29 [23040/37814 (61%)]\tLoss: 1.976300\n",
            "{AlexNet} Train Epoch: 29 [23552/37814 (62%)]\tLoss: 2.010759\n",
            "{AlexNet} Train Epoch: 29 [24064/37814 (64%)]\tLoss: 2.025927\n",
            "{AlexNet} Train Epoch: 29 [24576/37814 (65%)]\tLoss: 1.961164\n",
            "{AlexNet} Train Epoch: 29 [25088/37814 (66%)]\tLoss: 1.939716\n",
            "{AlexNet} Train Epoch: 29 [25600/37814 (68%)]\tLoss: 1.975586\n",
            "{AlexNet} Train Epoch: 29 [26112/37814 (69%)]\tLoss: 1.984610\n",
            "{AlexNet} Train Epoch: 29 [26624/37814 (70%)]\tLoss: 1.926120\n",
            "{AlexNet} Train Epoch: 29 [27136/37814 (72%)]\tLoss: 1.989306\n",
            "{AlexNet} Train Epoch: 29 [27648/37814 (73%)]\tLoss: 2.017525\n",
            "{AlexNet} Train Epoch: 29 [28160/37814 (74%)]\tLoss: 1.956886\n",
            "{AlexNet} Train Epoch: 29 [28672/37814 (76%)]\tLoss: 1.952249\n",
            "{AlexNet} Train Epoch: 29 [29184/37814 (77%)]\tLoss: 1.941995\n",
            "{AlexNet} Train Epoch: 29 [29696/37814 (78%)]\tLoss: 1.978636\n",
            "{AlexNet} Train Epoch: 29 [30208/37814 (80%)]\tLoss: 1.959516\n",
            "{AlexNet} Train Epoch: 29 [30720/37814 (81%)]\tLoss: 1.976187\n",
            "{AlexNet} Train Epoch: 29 [31232/37814 (82%)]\tLoss: 1.971277\n",
            "{AlexNet} Train Epoch: 29 [31744/37814 (84%)]\tLoss: 2.010642\n",
            "{AlexNet} Train Epoch: 29 [32256/37814 (85%)]\tLoss: 1.974377\n",
            "{AlexNet} Train Epoch: 29 [32768/37814 (86%)]\tLoss: 1.946710\n",
            "{AlexNet} Train Epoch: 29 [33280/37814 (88%)]\tLoss: 1.972510\n",
            "{AlexNet} Train Epoch: 29 [33792/37814 (89%)]\tLoss: 1.939332\n",
            "{AlexNet} Train Epoch: 29 [34304/37814 (91%)]\tLoss: 1.957009\n",
            "{AlexNet} Train Epoch: 29 [34816/37814 (92%)]\tLoss: 1.955775\n",
            "{AlexNet} Train Epoch: 29 [35328/37814 (93%)]\tLoss: 1.939644\n",
            "{AlexNet} Train Epoch: 29 [35840/37814 (95%)]\tLoss: 2.028326\n",
            "{AlexNet} Train Epoch: 29 [36352/37814 (96%)]\tLoss: 1.976421\n",
            "{AlexNet} Train Epoch: 29 [36864/37814 (97%)]\tLoss: 1.998628\n",
            "{AlexNet} Train Epoch: 29 [31974/37814 (99%)]\tLoss: 1.945624\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9770, Accuracy: 1054/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.857360124588013 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 29 [0/37814 (0%)]\tLoss: 1.723525\n",
            "{SqueezeNet} Train Epoch: 29 [512/37814 (1%)]\tLoss: 1.829761\n",
            "{SqueezeNet} Train Epoch: 29 [1024/37814 (3%)]\tLoss: 1.760943\n",
            "{SqueezeNet} Train Epoch: 29 [1536/37814 (4%)]\tLoss: 1.819658\n",
            "{SqueezeNet} Train Epoch: 29 [2048/37814 (5%)]\tLoss: 1.798653\n",
            "{SqueezeNet} Train Epoch: 29 [2560/37814 (7%)]\tLoss: 1.805145\n",
            "{SqueezeNet} Train Epoch: 29 [3072/37814 (8%)]\tLoss: 1.823573\n",
            "{SqueezeNet} Train Epoch: 29 [3584/37814 (9%)]\tLoss: 1.871774\n",
            "{SqueezeNet} Train Epoch: 29 [4096/37814 (11%)]\tLoss: 1.862332\n",
            "{SqueezeNet} Train Epoch: 29 [4608/37814 (12%)]\tLoss: 1.812641\n",
            "{SqueezeNet} Train Epoch: 29 [5120/37814 (14%)]\tLoss: 1.798806\n",
            "{SqueezeNet} Train Epoch: 29 [5632/37814 (15%)]\tLoss: 1.804392\n",
            "{SqueezeNet} Train Epoch: 29 [6144/37814 (16%)]\tLoss: 1.833761\n",
            "{SqueezeNet} Train Epoch: 29 [6656/37814 (18%)]\tLoss: 1.828831\n",
            "{SqueezeNet} Train Epoch: 29 [7168/37814 (19%)]\tLoss: 1.818269\n",
            "{SqueezeNet} Train Epoch: 29 [7680/37814 (20%)]\tLoss: 1.848069\n",
            "{SqueezeNet} Train Epoch: 29 [8192/37814 (22%)]\tLoss: 1.869060\n",
            "{SqueezeNet} Train Epoch: 29 [8704/37814 (23%)]\tLoss: 1.834951\n",
            "{SqueezeNet} Train Epoch: 29 [9216/37814 (24%)]\tLoss: 1.892431\n",
            "{SqueezeNet} Train Epoch: 29 [9728/37814 (26%)]\tLoss: 1.836288\n",
            "{SqueezeNet} Train Epoch: 29 [10240/37814 (27%)]\tLoss: 1.801213\n",
            "{SqueezeNet} Train Epoch: 29 [10752/37814 (28%)]\tLoss: 1.867697\n",
            "{SqueezeNet} Train Epoch: 29 [11264/37814 (30%)]\tLoss: 1.837919\n",
            "{SqueezeNet} Train Epoch: 29 [11776/37814 (31%)]\tLoss: 1.804034\n",
            "{SqueezeNet} Train Epoch: 29 [12288/37814 (32%)]\tLoss: 1.802697\n",
            "{SqueezeNet} Train Epoch: 29 [12800/37814 (34%)]\tLoss: 1.816909\n",
            "{SqueezeNet} Train Epoch: 29 [13312/37814 (35%)]\tLoss: 1.831315\n",
            "{SqueezeNet} Train Epoch: 29 [13824/37814 (36%)]\tLoss: 1.818080\n",
            "{SqueezeNet} Train Epoch: 29 [14336/37814 (38%)]\tLoss: 1.839046\n",
            "{SqueezeNet} Train Epoch: 29 [14848/37814 (39%)]\tLoss: 1.831533\n",
            "{SqueezeNet} Train Epoch: 29 [15360/37814 (41%)]\tLoss: 1.776741\n",
            "{SqueezeNet} Train Epoch: 29 [15872/37814 (42%)]\tLoss: 1.835089\n",
            "{SqueezeNet} Train Epoch: 29 [16384/37814 (43%)]\tLoss: 1.778257\n",
            "{SqueezeNet} Train Epoch: 29 [16896/37814 (45%)]\tLoss: 1.861524\n",
            "{SqueezeNet} Train Epoch: 29 [17408/37814 (46%)]\tLoss: 1.801368\n",
            "{SqueezeNet} Train Epoch: 29 [17920/37814 (47%)]\tLoss: 1.845346\n",
            "{SqueezeNet} Train Epoch: 29 [18432/37814 (49%)]\tLoss: 1.864314\n",
            "{SqueezeNet} Train Epoch: 29 [18944/37814 (50%)]\tLoss: 1.771309\n",
            "{SqueezeNet} Train Epoch: 29 [19456/37814 (51%)]\tLoss: 1.840432\n",
            "{SqueezeNet} Train Epoch: 29 [19968/37814 (53%)]\tLoss: 1.858652\n",
            "{SqueezeNet} Train Epoch: 29 [20480/37814 (54%)]\tLoss: 1.836733\n",
            "{SqueezeNet} Train Epoch: 29 [20992/37814 (55%)]\tLoss: 1.793914\n",
            "{SqueezeNet} Train Epoch: 29 [21504/37814 (57%)]\tLoss: 1.816680\n",
            "{SqueezeNet} Train Epoch: 29 [22016/37814 (58%)]\tLoss: 1.814406\n",
            "{SqueezeNet} Train Epoch: 29 [22528/37814 (59%)]\tLoss: 1.885566\n",
            "{SqueezeNet} Train Epoch: 29 [23040/37814 (61%)]\tLoss: 1.802578\n",
            "{SqueezeNet} Train Epoch: 29 [23552/37814 (62%)]\tLoss: 1.825862\n",
            "{SqueezeNet} Train Epoch: 29 [24064/37814 (64%)]\tLoss: 1.842199\n",
            "{SqueezeNet} Train Epoch: 29 [24576/37814 (65%)]\tLoss: 1.843871\n",
            "{SqueezeNet} Train Epoch: 29 [25088/37814 (66%)]\tLoss: 1.841711\n",
            "{SqueezeNet} Train Epoch: 29 [25600/37814 (68%)]\tLoss: 1.929239\n",
            "{SqueezeNet} Train Epoch: 29 [26112/37814 (69%)]\tLoss: 1.820398\n",
            "{SqueezeNet} Train Epoch: 29 [26624/37814 (70%)]\tLoss: 1.792144\n",
            "{SqueezeNet} Train Epoch: 29 [27136/37814 (72%)]\tLoss: 1.787589\n",
            "{SqueezeNet} Train Epoch: 29 [27648/37814 (73%)]\tLoss: 1.846927\n",
            "{SqueezeNet} Train Epoch: 29 [28160/37814 (74%)]\tLoss: 1.746738\n",
            "{SqueezeNet} Train Epoch: 29 [28672/37814 (76%)]\tLoss: 1.856549\n",
            "{SqueezeNet} Train Epoch: 29 [29184/37814 (77%)]\tLoss: 1.817020\n",
            "{SqueezeNet} Train Epoch: 29 [29696/37814 (78%)]\tLoss: 1.820600\n",
            "{SqueezeNet} Train Epoch: 29 [30208/37814 (80%)]\tLoss: 1.861173\n",
            "{SqueezeNet} Train Epoch: 29 [30720/37814 (81%)]\tLoss: 1.781014\n",
            "{SqueezeNet} Train Epoch: 29 [31232/37814 (82%)]\tLoss: 1.888127\n",
            "{SqueezeNet} Train Epoch: 29 [31744/37814 (84%)]\tLoss: 1.829643\n",
            "{SqueezeNet} Train Epoch: 29 [32256/37814 (85%)]\tLoss: 1.828887\n",
            "{SqueezeNet} Train Epoch: 29 [32768/37814 (86%)]\tLoss: 1.847396\n",
            "{SqueezeNet} Train Epoch: 29 [33280/37814 (88%)]\tLoss: 1.810987\n",
            "{SqueezeNet} Train Epoch: 29 [33792/37814 (89%)]\tLoss: 1.781244\n",
            "{SqueezeNet} Train Epoch: 29 [34304/37814 (91%)]\tLoss: 1.764593\n",
            "{SqueezeNet} Train Epoch: 29 [34816/37814 (92%)]\tLoss: 1.832243\n",
            "{SqueezeNet} Train Epoch: 29 [35328/37814 (93%)]\tLoss: 1.800023\n",
            "{SqueezeNet} Train Epoch: 29 [35840/37814 (95%)]\tLoss: 1.754076\n",
            "{SqueezeNet} Train Epoch: 29 [36352/37814 (96%)]\tLoss: 1.851030\n",
            "{SqueezeNet} Train Epoch: 29 [36864/37814 (97%)]\tLoss: 1.853026\n",
            "{SqueezeNet} Train Epoch: 29 [31974/37814 (99%)]\tLoss: 1.821543\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8229, Accuracy: 1591/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.339311599731445 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57b4f5b2-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"36341fd0-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9e5936ebba"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57b69db8-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_e04437bd75"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57b6e278-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_290f2cf12e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57b71838-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"57b6e278-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9c2ebbb274"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57dbf73e-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"57b69db8-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_d35b9e8c42"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57dce9a0-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_44ebb65ad1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57dd1b28-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_241b83d1a0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"57dd4f12-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"57dd1b28-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_866d24490f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 30 [0/37814 (0%)]\tLoss: 1.957925\n",
            "{AlexNet} Train Epoch: 30 [512/37814 (1%)]\tLoss: 2.007075\n",
            "{AlexNet} Train Epoch: 30 [1024/37814 (3%)]\tLoss: 1.980322\n",
            "{AlexNet} Train Epoch: 30 [1536/37814 (4%)]\tLoss: 2.044116\n",
            "{AlexNet} Train Epoch: 30 [2048/37814 (5%)]\tLoss: 1.939873\n",
            "{AlexNet} Train Epoch: 30 [2560/37814 (7%)]\tLoss: 1.931206\n",
            "{AlexNet} Train Epoch: 30 [3072/37814 (8%)]\tLoss: 1.979870\n",
            "{AlexNet} Train Epoch: 30 [3584/37814 (9%)]\tLoss: 1.977246\n",
            "{AlexNet} Train Epoch: 30 [4096/37814 (11%)]\tLoss: 2.011449\n",
            "{AlexNet} Train Epoch: 30 [4608/37814 (12%)]\tLoss: 1.979667\n",
            "{AlexNet} Train Epoch: 30 [5120/37814 (14%)]\tLoss: 1.998809\n",
            "{AlexNet} Train Epoch: 30 [5632/37814 (15%)]\tLoss: 1.959123\n",
            "{AlexNet} Train Epoch: 30 [6144/37814 (16%)]\tLoss: 1.940941\n",
            "{AlexNet} Train Epoch: 30 [6656/37814 (18%)]\tLoss: 1.975754\n",
            "{AlexNet} Train Epoch: 30 [7168/37814 (19%)]\tLoss: 2.018318\n",
            "{AlexNet} Train Epoch: 30 [7680/37814 (20%)]\tLoss: 1.945625\n",
            "{AlexNet} Train Epoch: 30 [8192/37814 (22%)]\tLoss: 1.952788\n",
            "{AlexNet} Train Epoch: 30 [8704/37814 (23%)]\tLoss: 1.918144\n",
            "{AlexNet} Train Epoch: 30 [9216/37814 (24%)]\tLoss: 1.950824\n",
            "{AlexNet} Train Epoch: 30 [9728/37814 (26%)]\tLoss: 1.967387\n",
            "{AlexNet} Train Epoch: 30 [10240/37814 (27%)]\tLoss: 1.953366\n",
            "{AlexNet} Train Epoch: 30 [10752/37814 (28%)]\tLoss: 1.965553\n",
            "{AlexNet} Train Epoch: 30 [11264/37814 (30%)]\tLoss: 1.986198\n",
            "{AlexNet} Train Epoch: 30 [11776/37814 (31%)]\tLoss: 1.978090\n",
            "{AlexNet} Train Epoch: 30 [12288/37814 (32%)]\tLoss: 1.969757\n",
            "{AlexNet} Train Epoch: 30 [12800/37814 (34%)]\tLoss: 1.919426\n",
            "{AlexNet} Train Epoch: 30 [13312/37814 (35%)]\tLoss: 1.997984\n",
            "{AlexNet} Train Epoch: 30 [13824/37814 (36%)]\tLoss: 1.925253\n",
            "{AlexNet} Train Epoch: 30 [14336/37814 (38%)]\tLoss: 1.964767\n",
            "{AlexNet} Train Epoch: 30 [14848/37814 (39%)]\tLoss: 1.987782\n",
            "{AlexNet} Train Epoch: 30 [15360/37814 (41%)]\tLoss: 1.936891\n",
            "{AlexNet} Train Epoch: 30 [15872/37814 (42%)]\tLoss: 1.994323\n",
            "{AlexNet} Train Epoch: 30 [16384/37814 (43%)]\tLoss: 1.991245\n",
            "{AlexNet} Train Epoch: 30 [16896/37814 (45%)]\tLoss: 1.976269\n",
            "{AlexNet} Train Epoch: 30 [17408/37814 (46%)]\tLoss: 2.006416\n",
            "{AlexNet} Train Epoch: 30 [17920/37814 (47%)]\tLoss: 1.996849\n",
            "{AlexNet} Train Epoch: 30 [18432/37814 (49%)]\tLoss: 2.002066\n",
            "{AlexNet} Train Epoch: 30 [18944/37814 (50%)]\tLoss: 1.971856\n",
            "{AlexNet} Train Epoch: 30 [19456/37814 (51%)]\tLoss: 1.934305\n",
            "{AlexNet} Train Epoch: 30 [19968/37814 (53%)]\tLoss: 1.982255\n",
            "{AlexNet} Train Epoch: 30 [20480/37814 (54%)]\tLoss: 1.973168\n",
            "{AlexNet} Train Epoch: 30 [20992/37814 (55%)]\tLoss: 1.983974\n",
            "{AlexNet} Train Epoch: 30 [21504/37814 (57%)]\tLoss: 1.939512\n",
            "{AlexNet} Train Epoch: 30 [22016/37814 (58%)]\tLoss: 1.968858\n",
            "{AlexNet} Train Epoch: 30 [22528/37814 (59%)]\tLoss: 1.949949\n",
            "{AlexNet} Train Epoch: 30 [23040/37814 (61%)]\tLoss: 1.961360\n",
            "{AlexNet} Train Epoch: 30 [23552/37814 (62%)]\tLoss: 1.939386\n",
            "{AlexNet} Train Epoch: 30 [24064/37814 (64%)]\tLoss: 1.993516\n",
            "{AlexNet} Train Epoch: 30 [24576/37814 (65%)]\tLoss: 1.966113\n",
            "{AlexNet} Train Epoch: 30 [25088/37814 (66%)]\tLoss: 1.974572\n",
            "{AlexNet} Train Epoch: 30 [25600/37814 (68%)]\tLoss: 2.037755\n",
            "{AlexNet} Train Epoch: 30 [26112/37814 (69%)]\tLoss: 1.990262\n",
            "{AlexNet} Train Epoch: 30 [26624/37814 (70%)]\tLoss: 2.008277\n",
            "{AlexNet} Train Epoch: 30 [27136/37814 (72%)]\tLoss: 1.938028\n",
            "{AlexNet} Train Epoch: 30 [27648/37814 (73%)]\tLoss: 1.999172\n",
            "{AlexNet} Train Epoch: 30 [28160/37814 (74%)]\tLoss: 1.988541\n",
            "{AlexNet} Train Epoch: 30 [28672/37814 (76%)]\tLoss: 1.976559\n",
            "{AlexNet} Train Epoch: 30 [29184/37814 (77%)]\tLoss: 1.981100\n",
            "{AlexNet} Train Epoch: 30 [29696/37814 (78%)]\tLoss: 1.937083\n",
            "{AlexNet} Train Epoch: 30 [30208/37814 (80%)]\tLoss: 1.978381\n",
            "{AlexNet} Train Epoch: 30 [30720/37814 (81%)]\tLoss: 2.005673\n",
            "{AlexNet} Train Epoch: 30 [31232/37814 (82%)]\tLoss: 1.971795\n",
            "{AlexNet} Train Epoch: 30 [31744/37814 (84%)]\tLoss: 2.002986\n",
            "{AlexNet} Train Epoch: 30 [32256/37814 (85%)]\tLoss: 1.988095\n",
            "{AlexNet} Train Epoch: 30 [32768/37814 (86%)]\tLoss: 1.985572\n",
            "{AlexNet} Train Epoch: 30 [33280/37814 (88%)]\tLoss: 1.946734\n",
            "{AlexNet} Train Epoch: 30 [33792/37814 (89%)]\tLoss: 1.960362\n",
            "{AlexNet} Train Epoch: 30 [34304/37814 (91%)]\tLoss: 1.982905\n",
            "{AlexNet} Train Epoch: 30 [34816/37814 (92%)]\tLoss: 2.028895\n",
            "{AlexNet} Train Epoch: 30 [35328/37814 (93%)]\tLoss: 1.988017\n",
            "{AlexNet} Train Epoch: 30 [35840/37814 (95%)]\tLoss: 2.002663\n",
            "{AlexNet} Train Epoch: 30 [36352/37814 (96%)]\tLoss: 2.011730\n",
            "{AlexNet} Train Epoch: 30 [36864/37814 (97%)]\tLoss: 1.965716\n",
            "{AlexNet} Train Epoch: 30 [31974/37814 (99%)]\tLoss: 1.942491\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9694, Accuracy: 1050/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.83974862098694 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 30 [0/37814 (0%)]\tLoss: 1.792567\n",
            "{SqueezeNet} Train Epoch: 30 [512/37814 (1%)]\tLoss: 1.822173\n",
            "{SqueezeNet} Train Epoch: 30 [1024/37814 (3%)]\tLoss: 1.773820\n",
            "{SqueezeNet} Train Epoch: 30 [1536/37814 (4%)]\tLoss: 1.808246\n",
            "{SqueezeNet} Train Epoch: 30 [2048/37814 (5%)]\tLoss: 1.832683\n",
            "{SqueezeNet} Train Epoch: 30 [2560/37814 (7%)]\tLoss: 1.856215\n",
            "{SqueezeNet} Train Epoch: 30 [3072/37814 (8%)]\tLoss: 1.818524\n",
            "{SqueezeNet} Train Epoch: 30 [3584/37814 (9%)]\tLoss: 1.832239\n",
            "{SqueezeNet} Train Epoch: 30 [4096/37814 (11%)]\tLoss: 1.849948\n",
            "{SqueezeNet} Train Epoch: 30 [4608/37814 (12%)]\tLoss: 1.790084\n",
            "{SqueezeNet} Train Epoch: 30 [5120/37814 (14%)]\tLoss: 1.760749\n",
            "{SqueezeNet} Train Epoch: 30 [5632/37814 (15%)]\tLoss: 1.855824\n",
            "{SqueezeNet} Train Epoch: 30 [6144/37814 (16%)]\tLoss: 1.809905\n",
            "{SqueezeNet} Train Epoch: 30 [6656/37814 (18%)]\tLoss: 1.739209\n",
            "{SqueezeNet} Train Epoch: 30 [7168/37814 (19%)]\tLoss: 1.840250\n",
            "{SqueezeNet} Train Epoch: 30 [7680/37814 (20%)]\tLoss: 1.822111\n",
            "{SqueezeNet} Train Epoch: 30 [8192/37814 (22%)]\tLoss: 1.809362\n",
            "{SqueezeNet} Train Epoch: 30 [8704/37814 (23%)]\tLoss: 1.828605\n",
            "{SqueezeNet} Train Epoch: 30 [9216/37814 (24%)]\tLoss: 1.855607\n",
            "{SqueezeNet} Train Epoch: 30 [9728/37814 (26%)]\tLoss: 1.843452\n",
            "{SqueezeNet} Train Epoch: 30 [10240/37814 (27%)]\tLoss: 1.792708\n",
            "{SqueezeNet} Train Epoch: 30 [10752/37814 (28%)]\tLoss: 1.822808\n",
            "{SqueezeNet} Train Epoch: 30 [11264/37814 (30%)]\tLoss: 1.780296\n",
            "{SqueezeNet} Train Epoch: 30 [11776/37814 (31%)]\tLoss: 1.847409\n",
            "{SqueezeNet} Train Epoch: 30 [12288/37814 (32%)]\tLoss: 1.830997\n",
            "{SqueezeNet} Train Epoch: 30 [12800/37814 (34%)]\tLoss: 1.846275\n",
            "{SqueezeNet} Train Epoch: 30 [13312/37814 (35%)]\tLoss: 1.797713\n",
            "{SqueezeNet} Train Epoch: 30 [13824/37814 (36%)]\tLoss: 1.814739\n",
            "{SqueezeNet} Train Epoch: 30 [14336/37814 (38%)]\tLoss: 1.796700\n",
            "{SqueezeNet} Train Epoch: 30 [14848/37814 (39%)]\tLoss: 1.753698\n",
            "{SqueezeNet} Train Epoch: 30 [15360/37814 (41%)]\tLoss: 1.774655\n",
            "{SqueezeNet} Train Epoch: 30 [15872/37814 (42%)]\tLoss: 1.900431\n",
            "{SqueezeNet} Train Epoch: 30 [16384/37814 (43%)]\tLoss: 1.825323\n",
            "{SqueezeNet} Train Epoch: 30 [16896/37814 (45%)]\tLoss: 1.826175\n",
            "{SqueezeNet} Train Epoch: 30 [17408/37814 (46%)]\tLoss: 1.795540\n",
            "{SqueezeNet} Train Epoch: 30 [17920/37814 (47%)]\tLoss: 1.858100\n",
            "{SqueezeNet} Train Epoch: 30 [18432/37814 (49%)]\tLoss: 1.835514\n",
            "{SqueezeNet} Train Epoch: 30 [18944/37814 (50%)]\tLoss: 1.789917\n",
            "{SqueezeNet} Train Epoch: 30 [19456/37814 (51%)]\tLoss: 1.911065\n",
            "{SqueezeNet} Train Epoch: 30 [19968/37814 (53%)]\tLoss: 1.884949\n",
            "{SqueezeNet} Train Epoch: 30 [20480/37814 (54%)]\tLoss: 1.889202\n",
            "{SqueezeNet} Train Epoch: 30 [20992/37814 (55%)]\tLoss: 1.898184\n",
            "{SqueezeNet} Train Epoch: 30 [21504/37814 (57%)]\tLoss: 1.819757\n",
            "{SqueezeNet} Train Epoch: 30 [22016/37814 (58%)]\tLoss: 1.858544\n",
            "{SqueezeNet} Train Epoch: 30 [22528/37814 (59%)]\tLoss: 1.816012\n",
            "{SqueezeNet} Train Epoch: 30 [23040/37814 (61%)]\tLoss: 1.803788\n",
            "{SqueezeNet} Train Epoch: 30 [23552/37814 (62%)]\tLoss: 1.831966\n",
            "{SqueezeNet} Train Epoch: 30 [24064/37814 (64%)]\tLoss: 1.839484\n",
            "{SqueezeNet} Train Epoch: 30 [24576/37814 (65%)]\tLoss: 1.776380\n",
            "{SqueezeNet} Train Epoch: 30 [25088/37814 (66%)]\tLoss: 1.820881\n",
            "{SqueezeNet} Train Epoch: 30 [25600/37814 (68%)]\tLoss: 1.777121\n",
            "{SqueezeNet} Train Epoch: 30 [26112/37814 (69%)]\tLoss: 1.865435\n",
            "{SqueezeNet} Train Epoch: 30 [26624/37814 (70%)]\tLoss: 1.846565\n",
            "{SqueezeNet} Train Epoch: 30 [27136/37814 (72%)]\tLoss: 1.825214\n",
            "{SqueezeNet} Train Epoch: 30 [27648/37814 (73%)]\tLoss: 1.829342\n",
            "{SqueezeNet} Train Epoch: 30 [28160/37814 (74%)]\tLoss: 1.808376\n",
            "{SqueezeNet} Train Epoch: 30 [28672/37814 (76%)]\tLoss: 1.810117\n",
            "{SqueezeNet} Train Epoch: 30 [29184/37814 (77%)]\tLoss: 1.843107\n",
            "{SqueezeNet} Train Epoch: 30 [29696/37814 (78%)]\tLoss: 1.766204\n",
            "{SqueezeNet} Train Epoch: 30 [30208/37814 (80%)]\tLoss: 1.828732\n",
            "{SqueezeNet} Train Epoch: 30 [30720/37814 (81%)]\tLoss: 1.853655\n",
            "{SqueezeNet} Train Epoch: 30 [31232/37814 (82%)]\tLoss: 1.791098\n",
            "{SqueezeNet} Train Epoch: 30 [31744/37814 (84%)]\tLoss: 1.851679\n",
            "{SqueezeNet} Train Epoch: 30 [32256/37814 (85%)]\tLoss: 1.863320\n",
            "{SqueezeNet} Train Epoch: 30 [32768/37814 (86%)]\tLoss: 1.810941\n",
            "{SqueezeNet} Train Epoch: 30 [33280/37814 (88%)]\tLoss: 1.764521\n",
            "{SqueezeNet} Train Epoch: 30 [33792/37814 (89%)]\tLoss: 1.837496\n",
            "{SqueezeNet} Train Epoch: 30 [34304/37814 (91%)]\tLoss: 1.847632\n",
            "{SqueezeNet} Train Epoch: 30 [34816/37814 (92%)]\tLoss: 1.854257\n",
            "{SqueezeNet} Train Epoch: 30 [35328/37814 (93%)]\tLoss: 1.728491\n",
            "{SqueezeNet} Train Epoch: 30 [35840/37814 (95%)]\tLoss: 1.798802\n",
            "{SqueezeNet} Train Epoch: 30 [36352/37814 (96%)]\tLoss: 1.821508\n",
            "{SqueezeNet} Train Epoch: 30 [36864/37814 (97%)]\tLoss: 1.792275\n",
            "{SqueezeNet} Train Epoch: 30 [31974/37814 (99%)]\tLoss: 1.838727\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8175, Accuracy: 1628/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.559271574020386 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"797be606-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"57dce9a0-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_ac84b6a717"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"797ce088-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_5e14f3dcfc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"797d1562-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_b45b92c9d7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"797d4c26-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"797d1562-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_79076a08ee"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"79a30e2a-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"797ce088-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_ada0366e57"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"79a401c2-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_93553c70cc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"79a43f70-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_f1c9426cff"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"79a474e0-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"79a43f70-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_7b6421754a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 31 [0/37814 (0%)]\tLoss: 2.002537\n",
            "{AlexNet} Train Epoch: 31 [512/37814 (1%)]\tLoss: 1.988719\n",
            "{AlexNet} Train Epoch: 31 [1024/37814 (3%)]\tLoss: 1.941494\n",
            "{AlexNet} Train Epoch: 31 [1536/37814 (4%)]\tLoss: 1.939093\n",
            "{AlexNet} Train Epoch: 31 [2048/37814 (5%)]\tLoss: 1.964442\n",
            "{AlexNet} Train Epoch: 31 [2560/37814 (7%)]\tLoss: 1.990754\n",
            "{AlexNet} Train Epoch: 31 [3072/37814 (8%)]\tLoss: 1.951500\n",
            "{AlexNet} Train Epoch: 31 [3584/37814 (9%)]\tLoss: 2.039101\n",
            "{AlexNet} Train Epoch: 31 [4096/37814 (11%)]\tLoss: 1.968633\n",
            "{AlexNet} Train Epoch: 31 [4608/37814 (12%)]\tLoss: 1.967773\n",
            "{AlexNet} Train Epoch: 31 [5120/37814 (14%)]\tLoss: 1.947644\n",
            "{AlexNet} Train Epoch: 31 [5632/37814 (15%)]\tLoss: 1.985694\n",
            "{AlexNet} Train Epoch: 31 [6144/37814 (16%)]\tLoss: 1.990627\n",
            "{AlexNet} Train Epoch: 31 [6656/37814 (18%)]\tLoss: 2.009121\n",
            "{AlexNet} Train Epoch: 31 [7168/37814 (19%)]\tLoss: 1.967347\n",
            "{AlexNet} Train Epoch: 31 [7680/37814 (20%)]\tLoss: 1.956876\n",
            "{AlexNet} Train Epoch: 31 [8192/37814 (22%)]\tLoss: 2.013704\n",
            "{AlexNet} Train Epoch: 31 [8704/37814 (23%)]\tLoss: 1.962483\n",
            "{AlexNet} Train Epoch: 31 [9216/37814 (24%)]\tLoss: 1.998357\n",
            "{AlexNet} Train Epoch: 31 [9728/37814 (26%)]\tLoss: 1.984741\n",
            "{AlexNet} Train Epoch: 31 [10240/37814 (27%)]\tLoss: 1.984195\n",
            "{AlexNet} Train Epoch: 31 [10752/37814 (28%)]\tLoss: 1.976202\n",
            "{AlexNet} Train Epoch: 31 [11264/37814 (30%)]\tLoss: 1.964993\n",
            "{AlexNet} Train Epoch: 31 [11776/37814 (31%)]\tLoss: 1.961321\n",
            "{AlexNet} Train Epoch: 31 [12288/37814 (32%)]\tLoss: 1.985138\n",
            "{AlexNet} Train Epoch: 31 [12800/37814 (34%)]\tLoss: 1.979084\n",
            "{AlexNet} Train Epoch: 31 [13312/37814 (35%)]\tLoss: 2.000559\n",
            "{AlexNet} Train Epoch: 31 [13824/37814 (36%)]\tLoss: 1.904015\n",
            "{AlexNet} Train Epoch: 31 [14336/37814 (38%)]\tLoss: 1.974701\n",
            "{AlexNet} Train Epoch: 31 [14848/37814 (39%)]\tLoss: 2.003897\n",
            "{AlexNet} Train Epoch: 31 [15360/37814 (41%)]\tLoss: 1.980137\n",
            "{AlexNet} Train Epoch: 31 [15872/37814 (42%)]\tLoss: 1.979183\n",
            "{AlexNet} Train Epoch: 31 [16384/37814 (43%)]\tLoss: 1.967107\n",
            "{AlexNet} Train Epoch: 31 [16896/37814 (45%)]\tLoss: 1.980950\n",
            "{AlexNet} Train Epoch: 31 [17408/37814 (46%)]\tLoss: 1.959124\n",
            "{AlexNet} Train Epoch: 31 [17920/37814 (47%)]\tLoss: 2.014560\n",
            "{AlexNet} Train Epoch: 31 [18432/37814 (49%)]\tLoss: 1.980816\n",
            "{AlexNet} Train Epoch: 31 [18944/37814 (50%)]\tLoss: 1.955002\n",
            "{AlexNet} Train Epoch: 31 [19456/37814 (51%)]\tLoss: 1.995356\n",
            "{AlexNet} Train Epoch: 31 [19968/37814 (53%)]\tLoss: 1.996993\n",
            "{AlexNet} Train Epoch: 31 [20480/37814 (54%)]\tLoss: 1.940512\n",
            "{AlexNet} Train Epoch: 31 [20992/37814 (55%)]\tLoss: 1.960023\n",
            "{AlexNet} Train Epoch: 31 [21504/37814 (57%)]\tLoss: 1.945500\n",
            "{AlexNet} Train Epoch: 31 [22016/37814 (58%)]\tLoss: 1.944764\n",
            "{AlexNet} Train Epoch: 31 [22528/37814 (59%)]\tLoss: 1.943387\n",
            "{AlexNet} Train Epoch: 31 [23040/37814 (61%)]\tLoss: 2.043836\n",
            "{AlexNet} Train Epoch: 31 [23552/37814 (62%)]\tLoss: 1.992973\n",
            "{AlexNet} Train Epoch: 31 [24064/37814 (64%)]\tLoss: 1.973514\n",
            "{AlexNet} Train Epoch: 31 [24576/37814 (65%)]\tLoss: 2.016111\n",
            "{AlexNet} Train Epoch: 31 [25088/37814 (66%)]\tLoss: 2.013019\n",
            "{AlexNet} Train Epoch: 31 [25600/37814 (68%)]\tLoss: 1.947343\n",
            "{AlexNet} Train Epoch: 31 [26112/37814 (69%)]\tLoss: 1.920875\n",
            "{AlexNet} Train Epoch: 31 [26624/37814 (70%)]\tLoss: 1.980080\n",
            "{AlexNet} Train Epoch: 31 [27136/37814 (72%)]\tLoss: 1.919806\n",
            "{AlexNet} Train Epoch: 31 [27648/37814 (73%)]\tLoss: 1.978145\n",
            "{AlexNet} Train Epoch: 31 [28160/37814 (74%)]\tLoss: 1.965848\n",
            "{AlexNet} Train Epoch: 31 [28672/37814 (76%)]\tLoss: 1.947564\n",
            "{AlexNet} Train Epoch: 31 [29184/37814 (77%)]\tLoss: 1.964771\n",
            "{AlexNet} Train Epoch: 31 [29696/37814 (78%)]\tLoss: 1.969919\n",
            "{AlexNet} Train Epoch: 31 [30208/37814 (80%)]\tLoss: 2.024863\n",
            "{AlexNet} Train Epoch: 31 [30720/37814 (81%)]\tLoss: 1.963170\n",
            "{AlexNet} Train Epoch: 31 [31232/37814 (82%)]\tLoss: 1.952818\n",
            "{AlexNet} Train Epoch: 31 [31744/37814 (84%)]\tLoss: 1.958670\n",
            "{AlexNet} Train Epoch: 31 [32256/37814 (85%)]\tLoss: 2.004119\n",
            "{AlexNet} Train Epoch: 31 [32768/37814 (86%)]\tLoss: 1.981859\n",
            "{AlexNet} Train Epoch: 31 [33280/37814 (88%)]\tLoss: 1.942621\n",
            "{AlexNet} Train Epoch: 31 [33792/37814 (89%)]\tLoss: 1.938001\n",
            "{AlexNet} Train Epoch: 31 [34304/37814 (91%)]\tLoss: 1.985666\n",
            "{AlexNet} Train Epoch: 31 [34816/37814 (92%)]\tLoss: 1.947732\n",
            "{AlexNet} Train Epoch: 31 [35328/37814 (93%)]\tLoss: 1.963136\n",
            "{AlexNet} Train Epoch: 31 [35840/37814 (95%)]\tLoss: 1.962465\n",
            "{AlexNet} Train Epoch: 31 [36352/37814 (96%)]\tLoss: 1.995456\n",
            "{AlexNet} Train Epoch: 31 [36864/37814 (97%)]\tLoss: 1.963736\n",
            "{AlexNet} Train Epoch: 31 [31974/37814 (99%)]\tLoss: 1.956689\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9792, Accuracy: 1035/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.817760229110718 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 31 [0/37814 (0%)]\tLoss: 1.796399\n",
            "{SqueezeNet} Train Epoch: 31 [512/37814 (1%)]\tLoss: 1.861565\n",
            "{SqueezeNet} Train Epoch: 31 [1024/37814 (3%)]\tLoss: 1.833035\n",
            "{SqueezeNet} Train Epoch: 31 [1536/37814 (4%)]\tLoss: 1.810626\n",
            "{SqueezeNet} Train Epoch: 31 [2048/37814 (5%)]\tLoss: 1.812806\n",
            "{SqueezeNet} Train Epoch: 31 [2560/37814 (7%)]\tLoss: 1.818194\n",
            "{SqueezeNet} Train Epoch: 31 [3072/37814 (8%)]\tLoss: 1.795857\n",
            "{SqueezeNet} Train Epoch: 31 [3584/37814 (9%)]\tLoss: 1.823360\n",
            "{SqueezeNet} Train Epoch: 31 [4096/37814 (11%)]\tLoss: 1.796687\n",
            "{SqueezeNet} Train Epoch: 31 [4608/37814 (12%)]\tLoss: 1.828947\n",
            "{SqueezeNet} Train Epoch: 31 [5120/37814 (14%)]\tLoss: 1.786669\n",
            "{SqueezeNet} Train Epoch: 31 [5632/37814 (15%)]\tLoss: 1.769485\n",
            "{SqueezeNet} Train Epoch: 31 [6144/37814 (16%)]\tLoss: 1.780617\n",
            "{SqueezeNet} Train Epoch: 31 [6656/37814 (18%)]\tLoss: 1.843418\n",
            "{SqueezeNet} Train Epoch: 31 [7168/37814 (19%)]\tLoss: 1.837173\n",
            "{SqueezeNet} Train Epoch: 31 [7680/37814 (20%)]\tLoss: 1.859092\n",
            "{SqueezeNet} Train Epoch: 31 [8192/37814 (22%)]\tLoss: 1.809844\n",
            "{SqueezeNet} Train Epoch: 31 [8704/37814 (23%)]\tLoss: 1.777343\n",
            "{SqueezeNet} Train Epoch: 31 [9216/37814 (24%)]\tLoss: 1.868471\n",
            "{SqueezeNet} Train Epoch: 31 [9728/37814 (26%)]\tLoss: 1.810854\n",
            "{SqueezeNet} Train Epoch: 31 [10240/37814 (27%)]\tLoss: 1.839071\n",
            "{SqueezeNet} Train Epoch: 31 [10752/37814 (28%)]\tLoss: 1.824713\n",
            "{SqueezeNet} Train Epoch: 31 [11264/37814 (30%)]\tLoss: 1.874259\n",
            "{SqueezeNet} Train Epoch: 31 [11776/37814 (31%)]\tLoss: 1.834112\n",
            "{SqueezeNet} Train Epoch: 31 [12288/37814 (32%)]\tLoss: 1.856198\n",
            "{SqueezeNet} Train Epoch: 31 [12800/37814 (34%)]\tLoss: 1.767709\n",
            "{SqueezeNet} Train Epoch: 31 [13312/37814 (35%)]\tLoss: 1.787768\n",
            "{SqueezeNet} Train Epoch: 31 [13824/37814 (36%)]\tLoss: 1.819585\n",
            "{SqueezeNet} Train Epoch: 31 [14336/37814 (38%)]\tLoss: 1.806776\n",
            "{SqueezeNet} Train Epoch: 31 [14848/37814 (39%)]\tLoss: 1.832296\n",
            "{SqueezeNet} Train Epoch: 31 [15360/37814 (41%)]\tLoss: 1.818884\n",
            "{SqueezeNet} Train Epoch: 31 [15872/37814 (42%)]\tLoss: 1.813345\n",
            "{SqueezeNet} Train Epoch: 31 [16384/37814 (43%)]\tLoss: 1.892300\n",
            "{SqueezeNet} Train Epoch: 31 [16896/37814 (45%)]\tLoss: 1.801691\n",
            "{SqueezeNet} Train Epoch: 31 [17408/37814 (46%)]\tLoss: 1.829381\n",
            "{SqueezeNet} Train Epoch: 31 [17920/37814 (47%)]\tLoss: 1.779672\n",
            "{SqueezeNet} Train Epoch: 31 [18432/37814 (49%)]\tLoss: 1.838395\n",
            "{SqueezeNet} Train Epoch: 31 [18944/37814 (50%)]\tLoss: 1.809628\n",
            "{SqueezeNet} Train Epoch: 31 [19456/37814 (51%)]\tLoss: 1.852242\n",
            "{SqueezeNet} Train Epoch: 31 [19968/37814 (53%)]\tLoss: 1.845481\n",
            "{SqueezeNet} Train Epoch: 31 [20480/37814 (54%)]\tLoss: 1.866863\n",
            "{SqueezeNet} Train Epoch: 31 [20992/37814 (55%)]\tLoss: 1.851878\n",
            "{SqueezeNet} Train Epoch: 31 [21504/37814 (57%)]\tLoss: 1.832464\n",
            "{SqueezeNet} Train Epoch: 31 [22016/37814 (58%)]\tLoss: 1.816807\n",
            "{SqueezeNet} Train Epoch: 31 [22528/37814 (59%)]\tLoss: 1.769247\n",
            "{SqueezeNet} Train Epoch: 31 [23040/37814 (61%)]\tLoss: 1.855988\n",
            "{SqueezeNet} Train Epoch: 31 [23552/37814 (62%)]\tLoss: 1.801962\n",
            "{SqueezeNet} Train Epoch: 31 [24064/37814 (64%)]\tLoss: 1.861719\n",
            "{SqueezeNet} Train Epoch: 31 [24576/37814 (65%)]\tLoss: 1.813174\n",
            "{SqueezeNet} Train Epoch: 31 [25088/37814 (66%)]\tLoss: 1.824808\n",
            "{SqueezeNet} Train Epoch: 31 [25600/37814 (68%)]\tLoss: 1.775571\n",
            "{SqueezeNet} Train Epoch: 31 [26112/37814 (69%)]\tLoss: 1.808428\n",
            "{SqueezeNet} Train Epoch: 31 [26624/37814 (70%)]\tLoss: 1.852598\n",
            "{SqueezeNet} Train Epoch: 31 [27136/37814 (72%)]\tLoss: 1.785931\n",
            "{SqueezeNet} Train Epoch: 31 [27648/37814 (73%)]\tLoss: 1.846725\n",
            "{SqueezeNet} Train Epoch: 31 [28160/37814 (74%)]\tLoss: 1.851695\n",
            "{SqueezeNet} Train Epoch: 31 [28672/37814 (76%)]\tLoss: 1.809538\n",
            "{SqueezeNet} Train Epoch: 31 [29184/37814 (77%)]\tLoss: 1.856407\n",
            "{SqueezeNet} Train Epoch: 31 [29696/37814 (78%)]\tLoss: 1.818404\n",
            "{SqueezeNet} Train Epoch: 31 [30208/37814 (80%)]\tLoss: 1.819801\n",
            "{SqueezeNet} Train Epoch: 31 [30720/37814 (81%)]\tLoss: 1.831102\n",
            "{SqueezeNet} Train Epoch: 31 [31232/37814 (82%)]\tLoss: 1.853303\n",
            "{SqueezeNet} Train Epoch: 31 [31744/37814 (84%)]\tLoss: 1.855999\n",
            "{SqueezeNet} Train Epoch: 31 [32256/37814 (85%)]\tLoss: 1.834797\n",
            "{SqueezeNet} Train Epoch: 31 [32768/37814 (86%)]\tLoss: 1.829983\n",
            "{SqueezeNet} Train Epoch: 31 [33280/37814 (88%)]\tLoss: 1.832011\n",
            "{SqueezeNet} Train Epoch: 31 [33792/37814 (89%)]\tLoss: 1.779441\n",
            "{SqueezeNet} Train Epoch: 31 [34304/37814 (91%)]\tLoss: 1.822897\n",
            "{SqueezeNet} Train Epoch: 31 [34816/37814 (92%)]\tLoss: 1.876708\n",
            "{SqueezeNet} Train Epoch: 31 [35328/37814 (93%)]\tLoss: 1.795693\n",
            "{SqueezeNet} Train Epoch: 31 [35840/37814 (95%)]\tLoss: 1.841254\n",
            "{SqueezeNet} Train Epoch: 31 [36352/37814 (96%)]\tLoss: 1.757146\n",
            "{SqueezeNet} Train Epoch: 31 [36864/37814 (97%)]\tLoss: 1.860161\n",
            "{SqueezeNet} Train Epoch: 31 [31974/37814 (99%)]\tLoss: 1.856516\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8098, Accuracy: 1589/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.861901998519897 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9ad60d5e-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"79a401c2-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8d506a2518"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9ad84aba-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_97856e6b35"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9ad89d8a-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_577341234f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9ad8eba0-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9ad89d8a-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_edc6ceab10"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9b071c82-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9ad84aba-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_db25de2f84"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9b09121c-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_330f335fb1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9b095b00-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_742db2b309"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9b099728-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9b095b00-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_85de621957"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 32 [0/37814 (0%)]\tLoss: 2.009925\n",
            "{AlexNet} Train Epoch: 32 [512/37814 (1%)]\tLoss: 1.992832\n",
            "{AlexNet} Train Epoch: 32 [1024/37814 (3%)]\tLoss: 1.993301\n",
            "{AlexNet} Train Epoch: 32 [1536/37814 (4%)]\tLoss: 1.967583\n",
            "{AlexNet} Train Epoch: 32 [2048/37814 (5%)]\tLoss: 1.971423\n",
            "{AlexNet} Train Epoch: 32 [2560/37814 (7%)]\tLoss: 1.966404\n",
            "{AlexNet} Train Epoch: 32 [3072/37814 (8%)]\tLoss: 2.020685\n",
            "{AlexNet} Train Epoch: 32 [3584/37814 (9%)]\tLoss: 1.956829\n",
            "{AlexNet} Train Epoch: 32 [4096/37814 (11%)]\tLoss: 1.977967\n",
            "{AlexNet} Train Epoch: 32 [4608/37814 (12%)]\tLoss: 2.008992\n",
            "{AlexNet} Train Epoch: 32 [5120/37814 (14%)]\tLoss: 1.972916\n",
            "{AlexNet} Train Epoch: 32 [5632/37814 (15%)]\tLoss: 1.944445\n",
            "{AlexNet} Train Epoch: 32 [6144/37814 (16%)]\tLoss: 1.973409\n",
            "{AlexNet} Train Epoch: 32 [6656/37814 (18%)]\tLoss: 1.966096\n",
            "{AlexNet} Train Epoch: 32 [7168/37814 (19%)]\tLoss: 1.967726\n",
            "{AlexNet} Train Epoch: 32 [7680/37814 (20%)]\tLoss: 1.967914\n",
            "{AlexNet} Train Epoch: 32 [8192/37814 (22%)]\tLoss: 1.965493\n",
            "{AlexNet} Train Epoch: 32 [8704/37814 (23%)]\tLoss: 1.950908\n",
            "{AlexNet} Train Epoch: 32 [9216/37814 (24%)]\tLoss: 1.937405\n",
            "{AlexNet} Train Epoch: 32 [9728/37814 (26%)]\tLoss: 1.987702\n",
            "{AlexNet} Train Epoch: 32 [10240/37814 (27%)]\tLoss: 1.970953\n",
            "{AlexNet} Train Epoch: 32 [10752/37814 (28%)]\tLoss: 1.942110\n",
            "{AlexNet} Train Epoch: 32 [11264/37814 (30%)]\tLoss: 2.008968\n",
            "{AlexNet} Train Epoch: 32 [11776/37814 (31%)]\tLoss: 1.961874\n",
            "{AlexNet} Train Epoch: 32 [12288/37814 (32%)]\tLoss: 1.972813\n",
            "{AlexNet} Train Epoch: 32 [12800/37814 (34%)]\tLoss: 1.935580\n",
            "{AlexNet} Train Epoch: 32 [13312/37814 (35%)]\tLoss: 2.005031\n",
            "{AlexNet} Train Epoch: 32 [13824/37814 (36%)]\tLoss: 1.998741\n",
            "{AlexNet} Train Epoch: 32 [14336/37814 (38%)]\tLoss: 1.957768\n",
            "{AlexNet} Train Epoch: 32 [14848/37814 (39%)]\tLoss: 2.001623\n",
            "{AlexNet} Train Epoch: 32 [15360/37814 (41%)]\tLoss: 1.980605\n",
            "{AlexNet} Train Epoch: 32 [15872/37814 (42%)]\tLoss: 1.963140\n",
            "{AlexNet} Train Epoch: 32 [16384/37814 (43%)]\tLoss: 1.928784\n",
            "{AlexNet} Train Epoch: 32 [16896/37814 (45%)]\tLoss: 2.000104\n",
            "{AlexNet} Train Epoch: 32 [17408/37814 (46%)]\tLoss: 1.959346\n",
            "{AlexNet} Train Epoch: 32 [17920/37814 (47%)]\tLoss: 1.956451\n",
            "{AlexNet} Train Epoch: 32 [18432/37814 (49%)]\tLoss: 2.020345\n",
            "{AlexNet} Train Epoch: 32 [18944/37814 (50%)]\tLoss: 2.006009\n",
            "{AlexNet} Train Epoch: 32 [19456/37814 (51%)]\tLoss: 1.926407\n",
            "{AlexNet} Train Epoch: 32 [19968/37814 (53%)]\tLoss: 1.935760\n",
            "{AlexNet} Train Epoch: 32 [20480/37814 (54%)]\tLoss: 1.960588\n",
            "{AlexNet} Train Epoch: 32 [20992/37814 (55%)]\tLoss: 1.986938\n",
            "{AlexNet} Train Epoch: 32 [21504/37814 (57%)]\tLoss: 1.980138\n",
            "{AlexNet} Train Epoch: 32 [22016/37814 (58%)]\tLoss: 1.998109\n",
            "{AlexNet} Train Epoch: 32 [22528/37814 (59%)]\tLoss: 1.993433\n",
            "{AlexNet} Train Epoch: 32 [23040/37814 (61%)]\tLoss: 1.945439\n",
            "{AlexNet} Train Epoch: 32 [23552/37814 (62%)]\tLoss: 1.988003\n",
            "{AlexNet} Train Epoch: 32 [24064/37814 (64%)]\tLoss: 1.974316\n",
            "{AlexNet} Train Epoch: 32 [24576/37814 (65%)]\tLoss: 1.975464\n",
            "{AlexNet} Train Epoch: 32 [25088/37814 (66%)]\tLoss: 1.972298\n",
            "{AlexNet} Train Epoch: 32 [25600/37814 (68%)]\tLoss: 1.931123\n",
            "{AlexNet} Train Epoch: 32 [26112/37814 (69%)]\tLoss: 1.960613\n",
            "{AlexNet} Train Epoch: 32 [26624/37814 (70%)]\tLoss: 2.001145\n",
            "{AlexNet} Train Epoch: 32 [27136/37814 (72%)]\tLoss: 1.960188\n",
            "{AlexNet} Train Epoch: 32 [27648/37814 (73%)]\tLoss: 2.005743\n",
            "{AlexNet} Train Epoch: 32 [28160/37814 (74%)]\tLoss: 1.995781\n",
            "{AlexNet} Train Epoch: 32 [28672/37814 (76%)]\tLoss: 1.983235\n",
            "{AlexNet} Train Epoch: 32 [29184/37814 (77%)]\tLoss: 2.004926\n",
            "{AlexNet} Train Epoch: 32 [29696/37814 (78%)]\tLoss: 2.011179\n",
            "{AlexNet} Train Epoch: 32 [30208/37814 (80%)]\tLoss: 1.949371\n",
            "{AlexNet} Train Epoch: 32 [30720/37814 (81%)]\tLoss: 1.958086\n",
            "{AlexNet} Train Epoch: 32 [31232/37814 (82%)]\tLoss: 1.986116\n",
            "{AlexNet} Train Epoch: 32 [31744/37814 (84%)]\tLoss: 1.981753\n",
            "{AlexNet} Train Epoch: 32 [32256/37814 (85%)]\tLoss: 2.015418\n",
            "{AlexNet} Train Epoch: 32 [32768/37814 (86%)]\tLoss: 1.995435\n",
            "{AlexNet} Train Epoch: 32 [33280/37814 (88%)]\tLoss: 1.967091\n",
            "{AlexNet} Train Epoch: 32 [33792/37814 (89%)]\tLoss: 1.989893\n",
            "{AlexNet} Train Epoch: 32 [34304/37814 (91%)]\tLoss: 2.001549\n",
            "{AlexNet} Train Epoch: 32 [34816/37814 (92%)]\tLoss: 1.954279\n",
            "{AlexNet} Train Epoch: 32 [35328/37814 (93%)]\tLoss: 2.012380\n",
            "{AlexNet} Train Epoch: 32 [35840/37814 (95%)]\tLoss: 1.992504\n",
            "{AlexNet} Train Epoch: 32 [36352/37814 (96%)]\tLoss: 1.931715\n",
            "{AlexNet} Train Epoch: 32 [36864/37814 (97%)]\tLoss: 1.944871\n",
            "{AlexNet} Train Epoch: 32 [31974/37814 (99%)]\tLoss: 1.998057\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9718, Accuracy: 1047/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.672993659973145 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 32 [0/37814 (0%)]\tLoss: 1.846688\n",
            "{SqueezeNet} Train Epoch: 32 [512/37814 (1%)]\tLoss: 1.885925\n",
            "{SqueezeNet} Train Epoch: 32 [1024/37814 (3%)]\tLoss: 1.801001\n",
            "{SqueezeNet} Train Epoch: 32 [1536/37814 (4%)]\tLoss: 1.873366\n",
            "{SqueezeNet} Train Epoch: 32 [2048/37814 (5%)]\tLoss: 1.816583\n",
            "{SqueezeNet} Train Epoch: 32 [2560/37814 (7%)]\tLoss: 1.810258\n",
            "{SqueezeNet} Train Epoch: 32 [3072/37814 (8%)]\tLoss: 1.907667\n",
            "{SqueezeNet} Train Epoch: 32 [3584/37814 (9%)]\tLoss: 1.827317\n",
            "{SqueezeNet} Train Epoch: 32 [4096/37814 (11%)]\tLoss: 1.807443\n",
            "{SqueezeNet} Train Epoch: 32 [4608/37814 (12%)]\tLoss: 1.815739\n",
            "{SqueezeNet} Train Epoch: 32 [5120/37814 (14%)]\tLoss: 1.807125\n",
            "{SqueezeNet} Train Epoch: 32 [5632/37814 (15%)]\tLoss: 1.854995\n",
            "{SqueezeNet} Train Epoch: 32 [6144/37814 (16%)]\tLoss: 1.849265\n",
            "{SqueezeNet} Train Epoch: 32 [6656/37814 (18%)]\tLoss: 1.835667\n",
            "{SqueezeNet} Train Epoch: 32 [7168/37814 (19%)]\tLoss: 1.840323\n",
            "{SqueezeNet} Train Epoch: 32 [7680/37814 (20%)]\tLoss: 1.809650\n",
            "{SqueezeNet} Train Epoch: 32 [8192/37814 (22%)]\tLoss: 1.842558\n",
            "{SqueezeNet} Train Epoch: 32 [8704/37814 (23%)]\tLoss: 1.837599\n",
            "{SqueezeNet} Train Epoch: 32 [9216/37814 (24%)]\tLoss: 1.781774\n",
            "{SqueezeNet} Train Epoch: 32 [9728/37814 (26%)]\tLoss: 1.806763\n",
            "{SqueezeNet} Train Epoch: 32 [10240/37814 (27%)]\tLoss: 1.832368\n",
            "{SqueezeNet} Train Epoch: 32 [10752/37814 (28%)]\tLoss: 1.821730\n",
            "{SqueezeNet} Train Epoch: 32 [11264/37814 (30%)]\tLoss: 1.892573\n",
            "{SqueezeNet} Train Epoch: 32 [11776/37814 (31%)]\tLoss: 1.800460\n",
            "{SqueezeNet} Train Epoch: 32 [12288/37814 (32%)]\tLoss: 1.799394\n",
            "{SqueezeNet} Train Epoch: 32 [12800/37814 (34%)]\tLoss: 1.863079\n",
            "{SqueezeNet} Train Epoch: 32 [13312/37814 (35%)]\tLoss: 1.828952\n",
            "{SqueezeNet} Train Epoch: 32 [13824/37814 (36%)]\tLoss: 1.827323\n",
            "{SqueezeNet} Train Epoch: 32 [14336/37814 (38%)]\tLoss: 1.777762\n",
            "{SqueezeNet} Train Epoch: 32 [14848/37814 (39%)]\tLoss: 1.844638\n",
            "{SqueezeNet} Train Epoch: 32 [15360/37814 (41%)]\tLoss: 1.883092\n",
            "{SqueezeNet} Train Epoch: 32 [15872/37814 (42%)]\tLoss: 1.804090\n",
            "{SqueezeNet} Train Epoch: 32 [16384/37814 (43%)]\tLoss: 1.813782\n",
            "{SqueezeNet} Train Epoch: 32 [16896/37814 (45%)]\tLoss: 1.840351\n",
            "{SqueezeNet} Train Epoch: 32 [17408/37814 (46%)]\tLoss: 1.839555\n",
            "{SqueezeNet} Train Epoch: 32 [17920/37814 (47%)]\tLoss: 1.831137\n",
            "{SqueezeNet} Train Epoch: 32 [18432/37814 (49%)]\tLoss: 1.858206\n",
            "{SqueezeNet} Train Epoch: 32 [18944/37814 (50%)]\tLoss: 1.879312\n",
            "{SqueezeNet} Train Epoch: 32 [19456/37814 (51%)]\tLoss: 1.758761\n",
            "{SqueezeNet} Train Epoch: 32 [19968/37814 (53%)]\tLoss: 1.840228\n",
            "{SqueezeNet} Train Epoch: 32 [20480/37814 (54%)]\tLoss: 1.817657\n",
            "{SqueezeNet} Train Epoch: 32 [20992/37814 (55%)]\tLoss: 1.801448\n",
            "{SqueezeNet} Train Epoch: 32 [21504/37814 (57%)]\tLoss: 1.801063\n",
            "{SqueezeNet} Train Epoch: 32 [22016/37814 (58%)]\tLoss: 1.883109\n",
            "{SqueezeNet} Train Epoch: 32 [22528/37814 (59%)]\tLoss: 1.780216\n",
            "{SqueezeNet} Train Epoch: 32 [23040/37814 (61%)]\tLoss: 1.842680\n",
            "{SqueezeNet} Train Epoch: 32 [23552/37814 (62%)]\tLoss: 1.866335\n",
            "{SqueezeNet} Train Epoch: 32 [24064/37814 (64%)]\tLoss: 1.814427\n",
            "{SqueezeNet} Train Epoch: 32 [24576/37814 (65%)]\tLoss: 1.765566\n",
            "{SqueezeNet} Train Epoch: 32 [25088/37814 (66%)]\tLoss: 1.818668\n",
            "{SqueezeNet} Train Epoch: 32 [25600/37814 (68%)]\tLoss: 1.838303\n",
            "{SqueezeNet} Train Epoch: 32 [26112/37814 (69%)]\tLoss: 1.867167\n",
            "{SqueezeNet} Train Epoch: 32 [26624/37814 (70%)]\tLoss: 1.885537\n",
            "{SqueezeNet} Train Epoch: 32 [27136/37814 (72%)]\tLoss: 1.819112\n",
            "{SqueezeNet} Train Epoch: 32 [27648/37814 (73%)]\tLoss: 1.846978\n",
            "{SqueezeNet} Train Epoch: 32 [28160/37814 (74%)]\tLoss: 1.840273\n",
            "{SqueezeNet} Train Epoch: 32 [28672/37814 (76%)]\tLoss: 1.805498\n",
            "{SqueezeNet} Train Epoch: 32 [29184/37814 (77%)]\tLoss: 1.813745\n",
            "{SqueezeNet} Train Epoch: 32 [29696/37814 (78%)]\tLoss: 1.828080\n",
            "{SqueezeNet} Train Epoch: 32 [30208/37814 (80%)]\tLoss: 1.823439\n",
            "{SqueezeNet} Train Epoch: 32 [30720/37814 (81%)]\tLoss: 1.837672\n",
            "{SqueezeNet} Train Epoch: 32 [31232/37814 (82%)]\tLoss: 1.822464\n",
            "{SqueezeNet} Train Epoch: 32 [31744/37814 (84%)]\tLoss: 1.846454\n",
            "{SqueezeNet} Train Epoch: 32 [32256/37814 (85%)]\tLoss: 1.823720\n",
            "{SqueezeNet} Train Epoch: 32 [32768/37814 (86%)]\tLoss: 1.824984\n",
            "{SqueezeNet} Train Epoch: 32 [33280/37814 (88%)]\tLoss: 1.803399\n",
            "{SqueezeNet} Train Epoch: 32 [33792/37814 (89%)]\tLoss: 1.837554\n",
            "{SqueezeNet} Train Epoch: 32 [34304/37814 (91%)]\tLoss: 1.767626\n",
            "{SqueezeNet} Train Epoch: 32 [34816/37814 (92%)]\tLoss: 1.817882\n",
            "{SqueezeNet} Train Epoch: 32 [35328/37814 (93%)]\tLoss: 1.874441\n",
            "{SqueezeNet} Train Epoch: 32 [35840/37814 (95%)]\tLoss: 1.775034\n",
            "{SqueezeNet} Train Epoch: 32 [36352/37814 (96%)]\tLoss: 1.808625\n",
            "{SqueezeNet} Train Epoch: 32 [36864/37814 (97%)]\tLoss: 1.822712\n",
            "{SqueezeNet} Train Epoch: 32 [31974/37814 (99%)]\tLoss: 1.879800\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8091, Accuracy: 1580/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.482243299484253 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bc83a254-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9b09121c-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_14329d167a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bc863870-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_8f5cd93583"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bc86b1ba-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_aacbd1bc91"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bc8731e4-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"bc86b1ba-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_cb349e9361"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bcac40f6-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"bc863870-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8ef23c3c1a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bcad374a-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_03e593355e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bcad721e-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_9be28cddcb"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"bcadac5c-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"bcad721e-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f91851f2b3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 33 [0/37814 (0%)]\tLoss: 1.979228\n",
            "{AlexNet} Train Epoch: 33 [512/37814 (1%)]\tLoss: 1.985308\n",
            "{AlexNet} Train Epoch: 33 [1024/37814 (3%)]\tLoss: 1.970648\n",
            "{AlexNet} Train Epoch: 33 [1536/37814 (4%)]\tLoss: 1.983084\n",
            "{AlexNet} Train Epoch: 33 [2048/37814 (5%)]\tLoss: 1.971899\n",
            "{AlexNet} Train Epoch: 33 [2560/37814 (7%)]\tLoss: 1.953098\n",
            "{AlexNet} Train Epoch: 33 [3072/37814 (8%)]\tLoss: 1.964242\n",
            "{AlexNet} Train Epoch: 33 [3584/37814 (9%)]\tLoss: 1.977097\n",
            "{AlexNet} Train Epoch: 33 [4096/37814 (11%)]\tLoss: 2.009847\n",
            "{AlexNet} Train Epoch: 33 [4608/37814 (12%)]\tLoss: 1.968560\n",
            "{AlexNet} Train Epoch: 33 [5120/37814 (14%)]\tLoss: 1.937090\n",
            "{AlexNet} Train Epoch: 33 [5632/37814 (15%)]\tLoss: 1.975800\n",
            "{AlexNet} Train Epoch: 33 [6144/37814 (16%)]\tLoss: 1.954497\n",
            "{AlexNet} Train Epoch: 33 [6656/37814 (18%)]\tLoss: 1.940107\n",
            "{AlexNet} Train Epoch: 33 [7168/37814 (19%)]\tLoss: 2.022698\n",
            "{AlexNet} Train Epoch: 33 [7680/37814 (20%)]\tLoss: 2.004535\n",
            "{AlexNet} Train Epoch: 33 [8192/37814 (22%)]\tLoss: 1.941586\n",
            "{AlexNet} Train Epoch: 33 [8704/37814 (23%)]\tLoss: 1.956661\n",
            "{AlexNet} Train Epoch: 33 [9216/37814 (24%)]\tLoss: 1.991284\n",
            "{AlexNet} Train Epoch: 33 [9728/37814 (26%)]\tLoss: 1.996017\n",
            "{AlexNet} Train Epoch: 33 [10240/37814 (27%)]\tLoss: 1.972125\n",
            "{AlexNet} Train Epoch: 33 [10752/37814 (28%)]\tLoss: 1.938174\n",
            "{AlexNet} Train Epoch: 33 [11264/37814 (30%)]\tLoss: 1.927357\n",
            "{AlexNet} Train Epoch: 33 [11776/37814 (31%)]\tLoss: 1.994230\n",
            "{AlexNet} Train Epoch: 33 [12288/37814 (32%)]\tLoss: 1.994994\n",
            "{AlexNet} Train Epoch: 33 [12800/37814 (34%)]\tLoss: 1.944389\n",
            "{AlexNet} Train Epoch: 33 [13312/37814 (35%)]\tLoss: 2.051169\n",
            "{AlexNet} Train Epoch: 33 [13824/37814 (36%)]\tLoss: 1.968840\n",
            "{AlexNet} Train Epoch: 33 [14336/37814 (38%)]\tLoss: 1.945174\n",
            "{AlexNet} Train Epoch: 33 [14848/37814 (39%)]\tLoss: 1.962233\n",
            "{AlexNet} Train Epoch: 33 [15360/37814 (41%)]\tLoss: 1.972829\n",
            "{AlexNet} Train Epoch: 33 [15872/37814 (42%)]\tLoss: 1.973010\n",
            "{AlexNet} Train Epoch: 33 [16384/37814 (43%)]\tLoss: 2.004903\n",
            "{AlexNet} Train Epoch: 33 [16896/37814 (45%)]\tLoss: 1.982459\n",
            "{AlexNet} Train Epoch: 33 [17408/37814 (46%)]\tLoss: 2.002320\n",
            "{AlexNet} Train Epoch: 33 [17920/37814 (47%)]\tLoss: 1.975421\n",
            "{AlexNet} Train Epoch: 33 [18432/37814 (49%)]\tLoss: 1.958698\n",
            "{AlexNet} Train Epoch: 33 [18944/37814 (50%)]\tLoss: 1.981722\n",
            "{AlexNet} Train Epoch: 33 [19456/37814 (51%)]\tLoss: 2.026232\n",
            "{AlexNet} Train Epoch: 33 [19968/37814 (53%)]\tLoss: 1.972713\n",
            "{AlexNet} Train Epoch: 33 [20480/37814 (54%)]\tLoss: 1.943793\n",
            "{AlexNet} Train Epoch: 33 [20992/37814 (55%)]\tLoss: 1.989870\n",
            "{AlexNet} Train Epoch: 33 [21504/37814 (57%)]\tLoss: 2.042395\n",
            "{AlexNet} Train Epoch: 33 [22016/37814 (58%)]\tLoss: 1.921332\n",
            "{AlexNet} Train Epoch: 33 [22528/37814 (59%)]\tLoss: 2.022236\n",
            "{AlexNet} Train Epoch: 33 [23040/37814 (61%)]\tLoss: 1.956120\n",
            "{AlexNet} Train Epoch: 33 [23552/37814 (62%)]\tLoss: 1.965688\n",
            "{AlexNet} Train Epoch: 33 [24064/37814 (64%)]\tLoss: 1.996379\n",
            "{AlexNet} Train Epoch: 33 [24576/37814 (65%)]\tLoss: 1.955075\n",
            "{AlexNet} Train Epoch: 33 [25088/37814 (66%)]\tLoss: 2.001995\n",
            "{AlexNet} Train Epoch: 33 [25600/37814 (68%)]\tLoss: 1.935654\n",
            "{AlexNet} Train Epoch: 33 [26112/37814 (69%)]\tLoss: 1.939322\n",
            "{AlexNet} Train Epoch: 33 [26624/37814 (70%)]\tLoss: 1.991379\n",
            "{AlexNet} Train Epoch: 33 [27136/37814 (72%)]\tLoss: 1.969515\n",
            "{AlexNet} Train Epoch: 33 [27648/37814 (73%)]\tLoss: 1.939832\n",
            "{AlexNet} Train Epoch: 33 [28160/37814 (74%)]\tLoss: 1.956159\n",
            "{AlexNet} Train Epoch: 33 [28672/37814 (76%)]\tLoss: 1.940660\n",
            "{AlexNet} Train Epoch: 33 [29184/37814 (77%)]\tLoss: 1.967847\n",
            "{AlexNet} Train Epoch: 33 [29696/37814 (78%)]\tLoss: 1.936545\n",
            "{AlexNet} Train Epoch: 33 [30208/37814 (80%)]\tLoss: 2.001530\n",
            "{AlexNet} Train Epoch: 33 [30720/37814 (81%)]\tLoss: 1.942676\n",
            "{AlexNet} Train Epoch: 33 [31232/37814 (82%)]\tLoss: 2.037195\n",
            "{AlexNet} Train Epoch: 33 [31744/37814 (84%)]\tLoss: 1.963457\n",
            "{AlexNet} Train Epoch: 33 [32256/37814 (85%)]\tLoss: 1.954275\n",
            "{AlexNet} Train Epoch: 33 [32768/37814 (86%)]\tLoss: 1.943545\n",
            "{AlexNet} Train Epoch: 33 [33280/37814 (88%)]\tLoss: 1.983220\n",
            "{AlexNet} Train Epoch: 33 [33792/37814 (89%)]\tLoss: 1.946434\n",
            "{AlexNet} Train Epoch: 33 [34304/37814 (91%)]\tLoss: 1.961448\n",
            "{AlexNet} Train Epoch: 33 [34816/37814 (92%)]\tLoss: 2.019527\n",
            "{AlexNet} Train Epoch: 33 [35328/37814 (93%)]\tLoss: 1.965626\n",
            "{AlexNet} Train Epoch: 33 [35840/37814 (95%)]\tLoss: 1.940008\n",
            "{AlexNet} Train Epoch: 33 [36352/37814 (96%)]\tLoss: 1.978439\n",
            "{AlexNet} Train Epoch: 33 [36864/37814 (97%)]\tLoss: 1.962240\n",
            "{AlexNet} Train Epoch: 33 [31974/37814 (99%)]\tLoss: 1.995215\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9703, Accuracy: 1029/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.47766613960266 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 33 [0/37814 (0%)]\tLoss: 1.820985\n",
            "{SqueezeNet} Train Epoch: 33 [512/37814 (1%)]\tLoss: 1.846645\n",
            "{SqueezeNet} Train Epoch: 33 [1024/37814 (3%)]\tLoss: 1.856645\n",
            "{SqueezeNet} Train Epoch: 33 [1536/37814 (4%)]\tLoss: 1.786083\n",
            "{SqueezeNet} Train Epoch: 33 [2048/37814 (5%)]\tLoss: 1.834191\n",
            "{SqueezeNet} Train Epoch: 33 [2560/37814 (7%)]\tLoss: 1.823574\n",
            "{SqueezeNet} Train Epoch: 33 [3072/37814 (8%)]\tLoss: 1.823753\n",
            "{SqueezeNet} Train Epoch: 33 [3584/37814 (9%)]\tLoss: 1.781945\n",
            "{SqueezeNet} Train Epoch: 33 [4096/37814 (11%)]\tLoss: 1.824079\n",
            "{SqueezeNet} Train Epoch: 33 [4608/37814 (12%)]\tLoss: 1.811785\n",
            "{SqueezeNet} Train Epoch: 33 [5120/37814 (14%)]\tLoss: 1.772193\n",
            "{SqueezeNet} Train Epoch: 33 [5632/37814 (15%)]\tLoss: 1.791190\n",
            "{SqueezeNet} Train Epoch: 33 [6144/37814 (16%)]\tLoss: 1.900765\n",
            "{SqueezeNet} Train Epoch: 33 [6656/37814 (18%)]\tLoss: 1.878875\n",
            "{SqueezeNet} Train Epoch: 33 [7168/37814 (19%)]\tLoss: 1.880257\n",
            "{SqueezeNet} Train Epoch: 33 [7680/37814 (20%)]\tLoss: 1.817377\n",
            "{SqueezeNet} Train Epoch: 33 [8192/37814 (22%)]\tLoss: 1.867186\n",
            "{SqueezeNet} Train Epoch: 33 [8704/37814 (23%)]\tLoss: 1.877311\n",
            "{SqueezeNet} Train Epoch: 33 [9216/37814 (24%)]\tLoss: 1.810229\n",
            "{SqueezeNet} Train Epoch: 33 [9728/37814 (26%)]\tLoss: 1.851254\n",
            "{SqueezeNet} Train Epoch: 33 [10240/37814 (27%)]\tLoss: 1.887865\n",
            "{SqueezeNet} Train Epoch: 33 [10752/37814 (28%)]\tLoss: 1.771879\n",
            "{SqueezeNet} Train Epoch: 33 [11264/37814 (30%)]\tLoss: 1.869493\n",
            "{SqueezeNet} Train Epoch: 33 [11776/37814 (31%)]\tLoss: 1.839663\n",
            "{SqueezeNet} Train Epoch: 33 [12288/37814 (32%)]\tLoss: 1.896486\n",
            "{SqueezeNet} Train Epoch: 33 [12800/37814 (34%)]\tLoss: 1.741574\n",
            "{SqueezeNet} Train Epoch: 33 [13312/37814 (35%)]\tLoss: 1.821194\n",
            "{SqueezeNet} Train Epoch: 33 [13824/37814 (36%)]\tLoss: 1.794629\n",
            "{SqueezeNet} Train Epoch: 33 [14336/37814 (38%)]\tLoss: 1.875074\n",
            "{SqueezeNet} Train Epoch: 33 [14848/37814 (39%)]\tLoss: 1.938493\n",
            "{SqueezeNet} Train Epoch: 33 [15360/37814 (41%)]\tLoss: 1.761577\n",
            "{SqueezeNet} Train Epoch: 33 [15872/37814 (42%)]\tLoss: 1.826415\n",
            "{SqueezeNet} Train Epoch: 33 [16384/37814 (43%)]\tLoss: 1.749715\n",
            "{SqueezeNet} Train Epoch: 33 [16896/37814 (45%)]\tLoss: 1.815234\n",
            "{SqueezeNet} Train Epoch: 33 [17408/37814 (46%)]\tLoss: 1.875823\n",
            "{SqueezeNet} Train Epoch: 33 [17920/37814 (47%)]\tLoss: 1.885861\n",
            "{SqueezeNet} Train Epoch: 33 [18432/37814 (49%)]\tLoss: 1.846556\n",
            "{SqueezeNet} Train Epoch: 33 [18944/37814 (50%)]\tLoss: 1.831312\n",
            "{SqueezeNet} Train Epoch: 33 [19456/37814 (51%)]\tLoss: 1.841427\n",
            "{SqueezeNet} Train Epoch: 33 [19968/37814 (53%)]\tLoss: 1.838608\n",
            "{SqueezeNet} Train Epoch: 33 [20480/37814 (54%)]\tLoss: 1.829238\n",
            "{SqueezeNet} Train Epoch: 33 [20992/37814 (55%)]\tLoss: 1.816640\n",
            "{SqueezeNet} Train Epoch: 33 [21504/37814 (57%)]\tLoss: 1.792271\n",
            "{SqueezeNet} Train Epoch: 33 [22016/37814 (58%)]\tLoss: 1.771999\n",
            "{SqueezeNet} Train Epoch: 33 [22528/37814 (59%)]\tLoss: 1.843432\n",
            "{SqueezeNet} Train Epoch: 33 [23040/37814 (61%)]\tLoss: 1.837481\n",
            "{SqueezeNet} Train Epoch: 33 [23552/37814 (62%)]\tLoss: 1.819535\n",
            "{SqueezeNet} Train Epoch: 33 [24064/37814 (64%)]\tLoss: 1.768581\n",
            "{SqueezeNet} Train Epoch: 33 [24576/37814 (65%)]\tLoss: 1.795021\n",
            "{SqueezeNet} Train Epoch: 33 [25088/37814 (66%)]\tLoss: 1.817219\n",
            "{SqueezeNet} Train Epoch: 33 [25600/37814 (68%)]\tLoss: 1.864429\n",
            "{SqueezeNet} Train Epoch: 33 [26112/37814 (69%)]\tLoss: 1.815518\n",
            "{SqueezeNet} Train Epoch: 33 [26624/37814 (70%)]\tLoss: 1.879136\n",
            "{SqueezeNet} Train Epoch: 33 [27136/37814 (72%)]\tLoss: 1.808773\n",
            "{SqueezeNet} Train Epoch: 33 [27648/37814 (73%)]\tLoss: 1.817981\n",
            "{SqueezeNet} Train Epoch: 33 [28160/37814 (74%)]\tLoss: 1.811496\n",
            "{SqueezeNet} Train Epoch: 33 [28672/37814 (76%)]\tLoss: 1.890239\n",
            "{SqueezeNet} Train Epoch: 33 [29184/37814 (77%)]\tLoss: 1.781140\n",
            "{SqueezeNet} Train Epoch: 33 [29696/37814 (78%)]\tLoss: 1.851807\n",
            "{SqueezeNet} Train Epoch: 33 [30208/37814 (80%)]\tLoss: 1.890067\n",
            "{SqueezeNet} Train Epoch: 33 [30720/37814 (81%)]\tLoss: 1.784096\n",
            "{SqueezeNet} Train Epoch: 33 [31232/37814 (82%)]\tLoss: 1.782562\n",
            "{SqueezeNet} Train Epoch: 33 [31744/37814 (84%)]\tLoss: 1.811064\n",
            "{SqueezeNet} Train Epoch: 33 [32256/37814 (85%)]\tLoss: 1.832073\n",
            "{SqueezeNet} Train Epoch: 33 [32768/37814 (86%)]\tLoss: 1.842130\n",
            "{SqueezeNet} Train Epoch: 33 [33280/37814 (88%)]\tLoss: 1.789894\n",
            "{SqueezeNet} Train Epoch: 33 [33792/37814 (89%)]\tLoss: 1.802431\n",
            "{SqueezeNet} Train Epoch: 33 [34304/37814 (91%)]\tLoss: 1.830987\n",
            "{SqueezeNet} Train Epoch: 33 [34816/37814 (92%)]\tLoss: 1.834968\n",
            "{SqueezeNet} Train Epoch: 33 [35328/37814 (93%)]\tLoss: 1.809362\n",
            "{SqueezeNet} Train Epoch: 33 [35840/37814 (95%)]\tLoss: 1.795657\n",
            "{SqueezeNet} Train Epoch: 33 [36352/37814 (96%)]\tLoss: 1.802400\n",
            "{SqueezeNet} Train Epoch: 33 [36864/37814 (97%)]\tLoss: 1.899431\n",
            "{SqueezeNet} Train Epoch: 33 [31974/37814 (99%)]\tLoss: 1.876101\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8080, Accuracy: 1632/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.737128734588623 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de304902-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"bcad374a-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_84b8a68b16"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de31f0f4-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_bd3b96447d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de3231c2-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_1d5980d6dc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de327fa6-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"de3231c2-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_be085bd0ba"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de5928ea-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"de31f0f4-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_af625781fa"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de5b71b8-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_69912bce5e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de5bd356-60e6-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_f541a6283e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"de5c2c5c-60e6-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"de5bd356-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_ddec9dc40e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 34 [0/37814 (0%)]\tLoss: 1.988073\n",
            "{AlexNet} Train Epoch: 34 [512/37814 (1%)]\tLoss: 1.971066\n",
            "{AlexNet} Train Epoch: 34 [1024/37814 (3%)]\tLoss: 1.986506\n",
            "{AlexNet} Train Epoch: 34 [1536/37814 (4%)]\tLoss: 1.944621\n",
            "{AlexNet} Train Epoch: 34 [2048/37814 (5%)]\tLoss: 1.984100\n",
            "{AlexNet} Train Epoch: 34 [2560/37814 (7%)]\tLoss: 2.009001\n",
            "{AlexNet} Train Epoch: 34 [3072/37814 (8%)]\tLoss: 1.990827\n",
            "{AlexNet} Train Epoch: 34 [3584/37814 (9%)]\tLoss: 2.038234\n",
            "{AlexNet} Train Epoch: 34 [4096/37814 (11%)]\tLoss: 2.001820\n",
            "{AlexNet} Train Epoch: 34 [4608/37814 (12%)]\tLoss: 1.938614\n",
            "{AlexNet} Train Epoch: 34 [5120/37814 (14%)]\tLoss: 1.987685\n",
            "{AlexNet} Train Epoch: 34 [5632/37814 (15%)]\tLoss: 1.951571\n",
            "{AlexNet} Train Epoch: 34 [6144/37814 (16%)]\tLoss: 1.988005\n",
            "{AlexNet} Train Epoch: 34 [6656/37814 (18%)]\tLoss: 1.978867\n",
            "{AlexNet} Train Epoch: 34 [7168/37814 (19%)]\tLoss: 1.958799\n",
            "{AlexNet} Train Epoch: 34 [7680/37814 (20%)]\tLoss: 1.984029\n",
            "{AlexNet} Train Epoch: 34 [8192/37814 (22%)]\tLoss: 1.984963\n",
            "{AlexNet} Train Epoch: 34 [8704/37814 (23%)]\tLoss: 1.970311\n",
            "{AlexNet} Train Epoch: 34 [9216/37814 (24%)]\tLoss: 1.973578\n",
            "{AlexNet} Train Epoch: 34 [9728/37814 (26%)]\tLoss: 1.957831\n",
            "{AlexNet} Train Epoch: 34 [10240/37814 (27%)]\tLoss: 1.984371\n",
            "{AlexNet} Train Epoch: 34 [10752/37814 (28%)]\tLoss: 1.956209\n",
            "{AlexNet} Train Epoch: 34 [11264/37814 (30%)]\tLoss: 2.012187\n",
            "{AlexNet} Train Epoch: 34 [11776/37814 (31%)]\tLoss: 1.951373\n",
            "{AlexNet} Train Epoch: 34 [12288/37814 (32%)]\tLoss: 1.973425\n",
            "{AlexNet} Train Epoch: 34 [12800/37814 (34%)]\tLoss: 2.004077\n",
            "{AlexNet} Train Epoch: 34 [13312/37814 (35%)]\tLoss: 1.970524\n",
            "{AlexNet} Train Epoch: 34 [13824/37814 (36%)]\tLoss: 1.988358\n",
            "{AlexNet} Train Epoch: 34 [14336/37814 (38%)]\tLoss: 1.997370\n",
            "{AlexNet} Train Epoch: 34 [14848/37814 (39%)]\tLoss: 1.931146\n",
            "{AlexNet} Train Epoch: 34 [15360/37814 (41%)]\tLoss: 1.942225\n",
            "{AlexNet} Train Epoch: 34 [15872/37814 (42%)]\tLoss: 1.995064\n",
            "{AlexNet} Train Epoch: 34 [16384/37814 (43%)]\tLoss: 1.956788\n",
            "{AlexNet} Train Epoch: 34 [16896/37814 (45%)]\tLoss: 1.979686\n",
            "{AlexNet} Train Epoch: 34 [17408/37814 (46%)]\tLoss: 2.015044\n",
            "{AlexNet} Train Epoch: 34 [17920/37814 (47%)]\tLoss: 1.935648\n",
            "{AlexNet} Train Epoch: 34 [18432/37814 (49%)]\tLoss: 1.977042\n",
            "{AlexNet} Train Epoch: 34 [18944/37814 (50%)]\tLoss: 1.954637\n",
            "{AlexNet} Train Epoch: 34 [19456/37814 (51%)]\tLoss: 1.990930\n",
            "{AlexNet} Train Epoch: 34 [19968/37814 (53%)]\tLoss: 1.954493\n",
            "{AlexNet} Train Epoch: 34 [20480/37814 (54%)]\tLoss: 1.900986\n",
            "{AlexNet} Train Epoch: 34 [20992/37814 (55%)]\tLoss: 1.976119\n",
            "{AlexNet} Train Epoch: 34 [21504/37814 (57%)]\tLoss: 1.951136\n",
            "{AlexNet} Train Epoch: 34 [22016/37814 (58%)]\tLoss: 2.001659\n",
            "{AlexNet} Train Epoch: 34 [22528/37814 (59%)]\tLoss: 2.026740\n",
            "{AlexNet} Train Epoch: 34 [23040/37814 (61%)]\tLoss: 2.011723\n",
            "{AlexNet} Train Epoch: 34 [23552/37814 (62%)]\tLoss: 1.951535\n",
            "{AlexNet} Train Epoch: 34 [24064/37814 (64%)]\tLoss: 1.962395\n",
            "{AlexNet} Train Epoch: 34 [24576/37814 (65%)]\tLoss: 1.929526\n",
            "{AlexNet} Train Epoch: 34 [25088/37814 (66%)]\tLoss: 1.961518\n",
            "{AlexNet} Train Epoch: 34 [25600/37814 (68%)]\tLoss: 1.995311\n",
            "{AlexNet} Train Epoch: 34 [26112/37814 (69%)]\tLoss: 1.943522\n",
            "{AlexNet} Train Epoch: 34 [26624/37814 (70%)]\tLoss: 1.985803\n",
            "{AlexNet} Train Epoch: 34 [27136/37814 (72%)]\tLoss: 2.026902\n",
            "{AlexNet} Train Epoch: 34 [27648/37814 (73%)]\tLoss: 1.947924\n",
            "{AlexNet} Train Epoch: 34 [28160/37814 (74%)]\tLoss: 1.979594\n",
            "{AlexNet} Train Epoch: 34 [28672/37814 (76%)]\tLoss: 2.000166\n",
            "{AlexNet} Train Epoch: 34 [29184/37814 (77%)]\tLoss: 1.968183\n",
            "{AlexNet} Train Epoch: 34 [29696/37814 (78%)]\tLoss: 1.971542\n",
            "{AlexNet} Train Epoch: 34 [30208/37814 (80%)]\tLoss: 1.965252\n",
            "{AlexNet} Train Epoch: 34 [30720/37814 (81%)]\tLoss: 1.959749\n",
            "{AlexNet} Train Epoch: 34 [31232/37814 (82%)]\tLoss: 1.973427\n",
            "{AlexNet} Train Epoch: 34 [31744/37814 (84%)]\tLoss: 2.006920\n",
            "{AlexNet} Train Epoch: 34 [32256/37814 (85%)]\tLoss: 1.968662\n",
            "{AlexNet} Train Epoch: 34 [32768/37814 (86%)]\tLoss: 1.972568\n",
            "{AlexNet} Train Epoch: 34 [33280/37814 (88%)]\tLoss: 2.001120\n",
            "{AlexNet} Train Epoch: 34 [33792/37814 (89%)]\tLoss: 1.994426\n",
            "{AlexNet} Train Epoch: 34 [34304/37814 (91%)]\tLoss: 2.024788\n",
            "{AlexNet} Train Epoch: 34 [34816/37814 (92%)]\tLoss: 2.010996\n",
            "{AlexNet} Train Epoch: 34 [35328/37814 (93%)]\tLoss: 1.968081\n",
            "{AlexNet} Train Epoch: 34 [35840/37814 (95%)]\tLoss: 1.953684\n",
            "{AlexNet} Train Epoch: 34 [36352/37814 (96%)]\tLoss: 1.946230\n",
            "{AlexNet} Train Epoch: 34 [36864/37814 (97%)]\tLoss: 1.969216\n",
            "{AlexNet} Train Epoch: 34 [31974/37814 (99%)]\tLoss: 1.937821\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9777, Accuracy: 1042/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.9417564868927 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 34 [0/37814 (0%)]\tLoss: 1.861242\n",
            "{SqueezeNet} Train Epoch: 34 [512/37814 (1%)]\tLoss: 1.869887\n",
            "{SqueezeNet} Train Epoch: 34 [1024/37814 (3%)]\tLoss: 1.850423\n",
            "{SqueezeNet} Train Epoch: 34 [1536/37814 (4%)]\tLoss: 1.798241\n",
            "{SqueezeNet} Train Epoch: 34 [2048/37814 (5%)]\tLoss: 1.813262\n",
            "{SqueezeNet} Train Epoch: 34 [2560/37814 (7%)]\tLoss: 1.850231\n",
            "{SqueezeNet} Train Epoch: 34 [3072/37814 (8%)]\tLoss: 1.805472\n",
            "{SqueezeNet} Train Epoch: 34 [3584/37814 (9%)]\tLoss: 1.774631\n",
            "{SqueezeNet} Train Epoch: 34 [4096/37814 (11%)]\tLoss: 1.883999\n",
            "{SqueezeNet} Train Epoch: 34 [4608/37814 (12%)]\tLoss: 1.787821\n",
            "{SqueezeNet} Train Epoch: 34 [5120/37814 (14%)]\tLoss: 1.779707\n",
            "{SqueezeNet} Train Epoch: 34 [5632/37814 (15%)]\tLoss: 1.827959\n",
            "{SqueezeNet} Train Epoch: 34 [6144/37814 (16%)]\tLoss: 1.853028\n",
            "{SqueezeNet} Train Epoch: 34 [6656/37814 (18%)]\tLoss: 1.817810\n",
            "{SqueezeNet} Train Epoch: 34 [7168/37814 (19%)]\tLoss: 1.828544\n",
            "{SqueezeNet} Train Epoch: 34 [7680/37814 (20%)]\tLoss: 1.850156\n",
            "{SqueezeNet} Train Epoch: 34 [8192/37814 (22%)]\tLoss: 1.785244\n",
            "{SqueezeNet} Train Epoch: 34 [8704/37814 (23%)]\tLoss: 1.812579\n",
            "{SqueezeNet} Train Epoch: 34 [9216/37814 (24%)]\tLoss: 1.883101\n",
            "{SqueezeNet} Train Epoch: 34 [9728/37814 (26%)]\tLoss: 1.800776\n",
            "{SqueezeNet} Train Epoch: 34 [10240/37814 (27%)]\tLoss: 1.814434\n",
            "{SqueezeNet} Train Epoch: 34 [10752/37814 (28%)]\tLoss: 1.801011\n",
            "{SqueezeNet} Train Epoch: 34 [11264/37814 (30%)]\tLoss: 1.867486\n",
            "{SqueezeNet} Train Epoch: 34 [11776/37814 (31%)]\tLoss: 1.834042\n",
            "{SqueezeNet} Train Epoch: 34 [12288/37814 (32%)]\tLoss: 1.792240\n",
            "{SqueezeNet} Train Epoch: 34 [12800/37814 (34%)]\tLoss: 1.872550\n",
            "{SqueezeNet} Train Epoch: 34 [13312/37814 (35%)]\tLoss: 1.857615\n",
            "{SqueezeNet} Train Epoch: 34 [13824/37814 (36%)]\tLoss: 1.828434\n",
            "{SqueezeNet} Train Epoch: 34 [14336/37814 (38%)]\tLoss: 1.813539\n",
            "{SqueezeNet} Train Epoch: 34 [14848/37814 (39%)]\tLoss: 1.802890\n",
            "{SqueezeNet} Train Epoch: 34 [15360/37814 (41%)]\tLoss: 1.835812\n",
            "{SqueezeNet} Train Epoch: 34 [15872/37814 (42%)]\tLoss: 1.807677\n",
            "{SqueezeNet} Train Epoch: 34 [16384/37814 (43%)]\tLoss: 1.830118\n",
            "{SqueezeNet} Train Epoch: 34 [16896/37814 (45%)]\tLoss: 1.858664\n",
            "{SqueezeNet} Train Epoch: 34 [17408/37814 (46%)]\tLoss: 1.813613\n",
            "{SqueezeNet} Train Epoch: 34 [17920/37814 (47%)]\tLoss: 1.821345\n",
            "{SqueezeNet} Train Epoch: 34 [18432/37814 (49%)]\tLoss: 1.797039\n",
            "{SqueezeNet} Train Epoch: 34 [18944/37814 (50%)]\tLoss: 1.850906\n",
            "{SqueezeNet} Train Epoch: 34 [19456/37814 (51%)]\tLoss: 1.779042\n",
            "{SqueezeNet} Train Epoch: 34 [19968/37814 (53%)]\tLoss: 1.830374\n",
            "{SqueezeNet} Train Epoch: 34 [20480/37814 (54%)]\tLoss: 1.807657\n",
            "{SqueezeNet} Train Epoch: 34 [20992/37814 (55%)]\tLoss: 1.868728\n",
            "{SqueezeNet} Train Epoch: 34 [21504/37814 (57%)]\tLoss: 1.791955\n",
            "{SqueezeNet} Train Epoch: 34 [22016/37814 (58%)]\tLoss: 1.810592\n",
            "{SqueezeNet} Train Epoch: 34 [22528/37814 (59%)]\tLoss: 1.827109\n",
            "{SqueezeNet} Train Epoch: 34 [23040/37814 (61%)]\tLoss: 1.730604\n",
            "{SqueezeNet} Train Epoch: 34 [23552/37814 (62%)]\tLoss: 1.769753\n",
            "{SqueezeNet} Train Epoch: 34 [24064/37814 (64%)]\tLoss: 1.849186\n",
            "{SqueezeNet} Train Epoch: 34 [24576/37814 (65%)]\tLoss: 1.778847\n",
            "{SqueezeNet} Train Epoch: 34 [25088/37814 (66%)]\tLoss: 1.778238\n",
            "{SqueezeNet} Train Epoch: 34 [25600/37814 (68%)]\tLoss: 1.836015\n",
            "{SqueezeNet} Train Epoch: 34 [26112/37814 (69%)]\tLoss: 1.783195\n",
            "{SqueezeNet} Train Epoch: 34 [26624/37814 (70%)]\tLoss: 1.791033\n",
            "{SqueezeNet} Train Epoch: 34 [27136/37814 (72%)]\tLoss: 1.876530\n",
            "{SqueezeNet} Train Epoch: 34 [27648/37814 (73%)]\tLoss: 1.842443\n",
            "{SqueezeNet} Train Epoch: 34 [28160/37814 (74%)]\tLoss: 1.842895\n",
            "{SqueezeNet} Train Epoch: 34 [28672/37814 (76%)]\tLoss: 1.861374\n",
            "{SqueezeNet} Train Epoch: 34 [29184/37814 (77%)]\tLoss: 1.874877\n",
            "{SqueezeNet} Train Epoch: 34 [29696/37814 (78%)]\tLoss: 1.815289\n",
            "{SqueezeNet} Train Epoch: 34 [30208/37814 (80%)]\tLoss: 1.852766\n",
            "{SqueezeNet} Train Epoch: 34 [30720/37814 (81%)]\tLoss: 1.846631\n",
            "{SqueezeNet} Train Epoch: 34 [31232/37814 (82%)]\tLoss: 1.874681\n",
            "{SqueezeNet} Train Epoch: 34 [31744/37814 (84%)]\tLoss: 1.835710\n",
            "{SqueezeNet} Train Epoch: 34 [32256/37814 (85%)]\tLoss: 1.840244\n",
            "{SqueezeNet} Train Epoch: 34 [32768/37814 (86%)]\tLoss: 1.802207\n",
            "{SqueezeNet} Train Epoch: 34 [33280/37814 (88%)]\tLoss: 1.811617\n",
            "{SqueezeNet} Train Epoch: 34 [33792/37814 (89%)]\tLoss: 1.805094\n",
            "{SqueezeNet} Train Epoch: 34 [34304/37814 (91%)]\tLoss: 1.816246\n",
            "{SqueezeNet} Train Epoch: 34 [34816/37814 (92%)]\tLoss: 1.856463\n",
            "{SqueezeNet} Train Epoch: 34 [35328/37814 (93%)]\tLoss: 1.872627\n",
            "{SqueezeNet} Train Epoch: 34 [35840/37814 (95%)]\tLoss: 1.811346\n",
            "{SqueezeNet} Train Epoch: 34 [36352/37814 (96%)]\tLoss: 1.800317\n",
            "{SqueezeNet} Train Epoch: 34 [36864/37814 (97%)]\tLoss: 1.768881\n",
            "{SqueezeNet} Train Epoch: 34 [31974/37814 (99%)]\tLoss: 1.782871\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8048, Accuracy: 1605/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.883986473083496 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"003c3e8e-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"de5b71b8-60e6-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_388eca1c1a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"003d4f2c-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_fd0dff523b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"003d83e8-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_c0b175ebb7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"003dbb10-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"003d83e8-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_d2dff990c1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0063f140-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"003d4f2c-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_11d5df6a26"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0064e726-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_e6ab159d08"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"00651aa2-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_e29f40b9b9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"00654e00-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"00651aa2-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_03df529566"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 35 [0/37814 (0%)]\tLoss: 2.024605\n",
            "{AlexNet} Train Epoch: 35 [512/37814 (1%)]\tLoss: 1.960734\n",
            "{AlexNet} Train Epoch: 35 [1024/37814 (3%)]\tLoss: 2.008188\n",
            "{AlexNet} Train Epoch: 35 [1536/37814 (4%)]\tLoss: 1.984948\n",
            "{AlexNet} Train Epoch: 35 [2048/37814 (5%)]\tLoss: 1.998721\n",
            "{AlexNet} Train Epoch: 35 [2560/37814 (7%)]\tLoss: 1.978889\n",
            "{AlexNet} Train Epoch: 35 [3072/37814 (8%)]\tLoss: 1.960750\n",
            "{AlexNet} Train Epoch: 35 [3584/37814 (9%)]\tLoss: 1.965147\n",
            "{AlexNet} Train Epoch: 35 [4096/37814 (11%)]\tLoss: 1.998442\n",
            "{AlexNet} Train Epoch: 35 [4608/37814 (12%)]\tLoss: 1.980931\n",
            "{AlexNet} Train Epoch: 35 [5120/37814 (14%)]\tLoss: 1.988857\n",
            "{AlexNet} Train Epoch: 35 [5632/37814 (15%)]\tLoss: 2.023060\n",
            "{AlexNet} Train Epoch: 35 [6144/37814 (16%)]\tLoss: 1.966507\n",
            "{AlexNet} Train Epoch: 35 [6656/37814 (18%)]\tLoss: 2.051798\n",
            "{AlexNet} Train Epoch: 35 [7168/37814 (19%)]\tLoss: 1.945687\n",
            "{AlexNet} Train Epoch: 35 [7680/37814 (20%)]\tLoss: 1.918270\n",
            "{AlexNet} Train Epoch: 35 [8192/37814 (22%)]\tLoss: 1.964098\n",
            "{AlexNet} Train Epoch: 35 [8704/37814 (23%)]\tLoss: 1.996297\n",
            "{AlexNet} Train Epoch: 35 [9216/37814 (24%)]\tLoss: 1.965361\n",
            "{AlexNet} Train Epoch: 35 [9728/37814 (26%)]\tLoss: 1.976271\n",
            "{AlexNet} Train Epoch: 35 [10240/37814 (27%)]\tLoss: 1.994835\n",
            "{AlexNet} Train Epoch: 35 [10752/37814 (28%)]\tLoss: 1.986621\n",
            "{AlexNet} Train Epoch: 35 [11264/37814 (30%)]\tLoss: 1.949850\n",
            "{AlexNet} Train Epoch: 35 [11776/37814 (31%)]\tLoss: 1.973132\n",
            "{AlexNet} Train Epoch: 35 [12288/37814 (32%)]\tLoss: 1.984201\n",
            "{AlexNet} Train Epoch: 35 [12800/37814 (34%)]\tLoss: 1.929706\n",
            "{AlexNet} Train Epoch: 35 [13312/37814 (35%)]\tLoss: 1.998722\n",
            "{AlexNet} Train Epoch: 35 [13824/37814 (36%)]\tLoss: 1.958773\n",
            "{AlexNet} Train Epoch: 35 [14336/37814 (38%)]\tLoss: 1.935539\n",
            "{AlexNet} Train Epoch: 35 [14848/37814 (39%)]\tLoss: 1.964144\n",
            "{AlexNet} Train Epoch: 35 [15360/37814 (41%)]\tLoss: 2.010150\n",
            "{AlexNet} Train Epoch: 35 [15872/37814 (42%)]\tLoss: 1.931785\n",
            "{AlexNet} Train Epoch: 35 [16384/37814 (43%)]\tLoss: 2.020661\n",
            "{AlexNet} Train Epoch: 35 [16896/37814 (45%)]\tLoss: 1.981273\n",
            "{AlexNet} Train Epoch: 35 [17408/37814 (46%)]\tLoss: 1.953642\n",
            "{AlexNet} Train Epoch: 35 [17920/37814 (47%)]\tLoss: 2.019051\n",
            "{AlexNet} Train Epoch: 35 [18432/37814 (49%)]\tLoss: 1.984733\n",
            "{AlexNet} Train Epoch: 35 [18944/37814 (50%)]\tLoss: 1.929789\n",
            "{AlexNet} Train Epoch: 35 [19456/37814 (51%)]\tLoss: 1.994452\n",
            "{AlexNet} Train Epoch: 35 [19968/37814 (53%)]\tLoss: 1.956242\n",
            "{AlexNet} Train Epoch: 35 [20480/37814 (54%)]\tLoss: 1.964825\n",
            "{AlexNet} Train Epoch: 35 [20992/37814 (55%)]\tLoss: 2.015537\n",
            "{AlexNet} Train Epoch: 35 [21504/37814 (57%)]\tLoss: 1.967664\n",
            "{AlexNet} Train Epoch: 35 [22016/37814 (58%)]\tLoss: 1.956453\n",
            "{AlexNet} Train Epoch: 35 [22528/37814 (59%)]\tLoss: 1.971488\n",
            "{AlexNet} Train Epoch: 35 [23040/37814 (61%)]\tLoss: 1.950722\n",
            "{AlexNet} Train Epoch: 35 [23552/37814 (62%)]\tLoss: 1.983215\n",
            "{AlexNet} Train Epoch: 35 [24064/37814 (64%)]\tLoss: 1.920891\n",
            "{AlexNet} Train Epoch: 35 [24576/37814 (65%)]\tLoss: 1.983861\n",
            "{AlexNet} Train Epoch: 35 [25088/37814 (66%)]\tLoss: 1.956823\n",
            "{AlexNet} Train Epoch: 35 [25600/37814 (68%)]\tLoss: 1.991315\n",
            "{AlexNet} Train Epoch: 35 [26112/37814 (69%)]\tLoss: 1.985893\n",
            "{AlexNet} Train Epoch: 35 [26624/37814 (70%)]\tLoss: 1.935276\n",
            "{AlexNet} Train Epoch: 35 [27136/37814 (72%)]\tLoss: 1.958529\n",
            "{AlexNet} Train Epoch: 35 [27648/37814 (73%)]\tLoss: 1.979564\n",
            "{AlexNet} Train Epoch: 35 [28160/37814 (74%)]\tLoss: 2.001302\n",
            "{AlexNet} Train Epoch: 35 [28672/37814 (76%)]\tLoss: 1.917574\n",
            "{AlexNet} Train Epoch: 35 [29184/37814 (77%)]\tLoss: 2.000894\n",
            "{AlexNet} Train Epoch: 35 [29696/37814 (78%)]\tLoss: 1.965923\n",
            "{AlexNet} Train Epoch: 35 [30208/37814 (80%)]\tLoss: 1.955729\n",
            "{AlexNet} Train Epoch: 35 [30720/37814 (81%)]\tLoss: 1.979418\n",
            "{AlexNet} Train Epoch: 35 [31232/37814 (82%)]\tLoss: 1.948750\n",
            "{AlexNet} Train Epoch: 35 [31744/37814 (84%)]\tLoss: 1.968348\n",
            "{AlexNet} Train Epoch: 35 [32256/37814 (85%)]\tLoss: 1.967132\n",
            "{AlexNet} Train Epoch: 35 [32768/37814 (86%)]\tLoss: 2.011099\n",
            "{AlexNet} Train Epoch: 35 [33280/37814 (88%)]\tLoss: 1.920273\n",
            "{AlexNet} Train Epoch: 35 [33792/37814 (89%)]\tLoss: 1.985655\n",
            "{AlexNet} Train Epoch: 35 [34304/37814 (91%)]\tLoss: 1.951775\n",
            "{AlexNet} Train Epoch: 35 [34816/37814 (92%)]\tLoss: 1.987905\n",
            "{AlexNet} Train Epoch: 35 [35328/37814 (93%)]\tLoss: 1.984971\n",
            "{AlexNet} Train Epoch: 35 [35840/37814 (95%)]\tLoss: 1.977340\n",
            "{AlexNet} Train Epoch: 35 [36352/37814 (96%)]\tLoss: 2.015860\n",
            "{AlexNet} Train Epoch: 35 [36864/37814 (97%)]\tLoss: 1.968151\n",
            "{AlexNet} Train Epoch: 35 [31974/37814 (99%)]\tLoss: 2.002503\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9611, Accuracy: 1051/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.756552934646606 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 35 [0/37814 (0%)]\tLoss: 1.824054\n",
            "{SqueezeNet} Train Epoch: 35 [512/37814 (1%)]\tLoss: 1.746646\n",
            "{SqueezeNet} Train Epoch: 35 [1024/37814 (3%)]\tLoss: 1.872432\n",
            "{SqueezeNet} Train Epoch: 35 [1536/37814 (4%)]\tLoss: 1.843651\n",
            "{SqueezeNet} Train Epoch: 35 [2048/37814 (5%)]\tLoss: 1.826644\n",
            "{SqueezeNet} Train Epoch: 35 [2560/37814 (7%)]\tLoss: 1.817799\n",
            "{SqueezeNet} Train Epoch: 35 [3072/37814 (8%)]\tLoss: 1.916110\n",
            "{SqueezeNet} Train Epoch: 35 [3584/37814 (9%)]\tLoss: 1.790627\n",
            "{SqueezeNet} Train Epoch: 35 [4096/37814 (11%)]\tLoss: 1.809131\n",
            "{SqueezeNet} Train Epoch: 35 [4608/37814 (12%)]\tLoss: 1.858601\n",
            "{SqueezeNet} Train Epoch: 35 [5120/37814 (14%)]\tLoss: 1.828267\n",
            "{SqueezeNet} Train Epoch: 35 [5632/37814 (15%)]\tLoss: 1.825328\n",
            "{SqueezeNet} Train Epoch: 35 [6144/37814 (16%)]\tLoss: 1.863438\n",
            "{SqueezeNet} Train Epoch: 35 [6656/37814 (18%)]\tLoss: 1.804087\n",
            "{SqueezeNet} Train Epoch: 35 [7168/37814 (19%)]\tLoss: 1.803912\n",
            "{SqueezeNet} Train Epoch: 35 [7680/37814 (20%)]\tLoss: 1.838247\n",
            "{SqueezeNet} Train Epoch: 35 [8192/37814 (22%)]\tLoss: 1.811796\n",
            "{SqueezeNet} Train Epoch: 35 [8704/37814 (23%)]\tLoss: 1.830796\n",
            "{SqueezeNet} Train Epoch: 35 [9216/37814 (24%)]\tLoss: 1.779116\n",
            "{SqueezeNet} Train Epoch: 35 [9728/37814 (26%)]\tLoss: 1.836629\n",
            "{SqueezeNet} Train Epoch: 35 [10240/37814 (27%)]\tLoss: 1.921879\n",
            "{SqueezeNet} Train Epoch: 35 [10752/37814 (28%)]\tLoss: 1.812703\n",
            "{SqueezeNet} Train Epoch: 35 [11264/37814 (30%)]\tLoss: 1.796369\n",
            "{SqueezeNet} Train Epoch: 35 [11776/37814 (31%)]\tLoss: 1.801327\n",
            "{SqueezeNet} Train Epoch: 35 [12288/37814 (32%)]\tLoss: 1.778102\n",
            "{SqueezeNet} Train Epoch: 35 [12800/37814 (34%)]\tLoss: 1.814973\n",
            "{SqueezeNet} Train Epoch: 35 [13312/37814 (35%)]\tLoss: 1.793833\n",
            "{SqueezeNet} Train Epoch: 35 [13824/37814 (36%)]\tLoss: 1.767591\n",
            "{SqueezeNet} Train Epoch: 35 [14336/37814 (38%)]\tLoss: 1.860036\n",
            "{SqueezeNet} Train Epoch: 35 [14848/37814 (39%)]\tLoss: 1.838931\n",
            "{SqueezeNet} Train Epoch: 35 [15360/37814 (41%)]\tLoss: 1.801885\n",
            "{SqueezeNet} Train Epoch: 35 [15872/37814 (42%)]\tLoss: 1.860570\n",
            "{SqueezeNet} Train Epoch: 35 [16384/37814 (43%)]\tLoss: 1.831926\n",
            "{SqueezeNet} Train Epoch: 35 [16896/37814 (45%)]\tLoss: 1.829809\n",
            "{SqueezeNet} Train Epoch: 35 [17408/37814 (46%)]\tLoss: 1.867632\n",
            "{SqueezeNet} Train Epoch: 35 [17920/37814 (47%)]\tLoss: 1.821517\n",
            "{SqueezeNet} Train Epoch: 35 [18432/37814 (49%)]\tLoss: 1.844236\n",
            "{SqueezeNet} Train Epoch: 35 [18944/37814 (50%)]\tLoss: 1.874271\n",
            "{SqueezeNet} Train Epoch: 35 [19456/37814 (51%)]\tLoss: 1.812465\n",
            "{SqueezeNet} Train Epoch: 35 [19968/37814 (53%)]\tLoss: 1.843163\n",
            "{SqueezeNet} Train Epoch: 35 [20480/37814 (54%)]\tLoss: 1.811850\n",
            "{SqueezeNet} Train Epoch: 35 [20992/37814 (55%)]\tLoss: 1.811039\n",
            "{SqueezeNet} Train Epoch: 35 [21504/37814 (57%)]\tLoss: 1.859086\n",
            "{SqueezeNet} Train Epoch: 35 [22016/37814 (58%)]\tLoss: 1.853649\n",
            "{SqueezeNet} Train Epoch: 35 [22528/37814 (59%)]\tLoss: 1.853822\n",
            "{SqueezeNet} Train Epoch: 35 [23040/37814 (61%)]\tLoss: 1.827373\n",
            "{SqueezeNet} Train Epoch: 35 [23552/37814 (62%)]\tLoss: 1.807421\n",
            "{SqueezeNet} Train Epoch: 35 [24064/37814 (64%)]\tLoss: 1.832943\n",
            "{SqueezeNet} Train Epoch: 35 [24576/37814 (65%)]\tLoss: 1.852041\n",
            "{SqueezeNet} Train Epoch: 35 [25088/37814 (66%)]\tLoss: 1.777989\n",
            "{SqueezeNet} Train Epoch: 35 [25600/37814 (68%)]\tLoss: 1.811785\n",
            "{SqueezeNet} Train Epoch: 35 [26112/37814 (69%)]\tLoss: 1.834688\n",
            "{SqueezeNet} Train Epoch: 35 [26624/37814 (70%)]\tLoss: 1.844467\n",
            "{SqueezeNet} Train Epoch: 35 [27136/37814 (72%)]\tLoss: 1.830576\n",
            "{SqueezeNet} Train Epoch: 35 [27648/37814 (73%)]\tLoss: 1.763973\n",
            "{SqueezeNet} Train Epoch: 35 [28160/37814 (74%)]\tLoss: 1.778842\n",
            "{SqueezeNet} Train Epoch: 35 [28672/37814 (76%)]\tLoss: 1.785093\n",
            "{SqueezeNet} Train Epoch: 35 [29184/37814 (77%)]\tLoss: 1.797056\n",
            "{SqueezeNet} Train Epoch: 35 [29696/37814 (78%)]\tLoss: 1.822415\n",
            "{SqueezeNet} Train Epoch: 35 [30208/37814 (80%)]\tLoss: 1.805401\n",
            "{SqueezeNet} Train Epoch: 35 [30720/37814 (81%)]\tLoss: 1.809742\n",
            "{SqueezeNet} Train Epoch: 35 [31232/37814 (82%)]\tLoss: 1.776810\n",
            "{SqueezeNet} Train Epoch: 35 [31744/37814 (84%)]\tLoss: 1.872182\n",
            "{SqueezeNet} Train Epoch: 35 [32256/37814 (85%)]\tLoss: 1.794309\n",
            "{SqueezeNet} Train Epoch: 35 [32768/37814 (86%)]\tLoss: 1.826063\n",
            "{SqueezeNet} Train Epoch: 35 [33280/37814 (88%)]\tLoss: 1.834950\n",
            "{SqueezeNet} Train Epoch: 35 [33792/37814 (89%)]\tLoss: 1.805922\n",
            "{SqueezeNet} Train Epoch: 35 [34304/37814 (91%)]\tLoss: 1.759537\n",
            "{SqueezeNet} Train Epoch: 35 [34816/37814 (92%)]\tLoss: 1.763671\n",
            "{SqueezeNet} Train Epoch: 35 [35328/37814 (93%)]\tLoss: 1.832379\n",
            "{SqueezeNet} Train Epoch: 35 [35840/37814 (95%)]\tLoss: 1.759335\n",
            "{SqueezeNet} Train Epoch: 35 [36352/37814 (96%)]\tLoss: 1.760342\n",
            "{SqueezeNet} Train Epoch: 35 [36864/37814 (97%)]\tLoss: 1.813800\n",
            "{SqueezeNet} Train Epoch: 35 [31974/37814 (99%)]\tLoss: 1.762777\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8111, Accuracy: 1600/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.41315507888794 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2148ca84-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0064e726-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2cb77a990c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"214ab9fc-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9c8b3bae9a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"214b0826-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_c8feab6098"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"214b7ae0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"214b0826-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_834a64de72"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2172ad2c-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"214ab9fc-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_09f6f5dd8f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2174510e-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_b720c509a1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"21749de4-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_e3d7e76695"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2174dc3c-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"21749de4-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_84ab6d02c1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 36 [0/37814 (0%)]\tLoss: 1.990831\n",
            "{AlexNet} Train Epoch: 36 [512/37814 (1%)]\tLoss: 1.936709\n",
            "{AlexNet} Train Epoch: 36 [1024/37814 (3%)]\tLoss: 1.971918\n",
            "{AlexNet} Train Epoch: 36 [1536/37814 (4%)]\tLoss: 1.975537\n",
            "{AlexNet} Train Epoch: 36 [2048/37814 (5%)]\tLoss: 1.949485\n",
            "{AlexNet} Train Epoch: 36 [2560/37814 (7%)]\tLoss: 1.977450\n",
            "{AlexNet} Train Epoch: 36 [3072/37814 (8%)]\tLoss: 1.910921\n",
            "{AlexNet} Train Epoch: 36 [3584/37814 (9%)]\tLoss: 1.980324\n",
            "{AlexNet} Train Epoch: 36 [4096/37814 (11%)]\tLoss: 1.964643\n",
            "{AlexNet} Train Epoch: 36 [4608/37814 (12%)]\tLoss: 1.996760\n",
            "{AlexNet} Train Epoch: 36 [5120/37814 (14%)]\tLoss: 2.004857\n",
            "{AlexNet} Train Epoch: 36 [5632/37814 (15%)]\tLoss: 1.958629\n",
            "{AlexNet} Train Epoch: 36 [6144/37814 (16%)]\tLoss: 1.943699\n",
            "{AlexNet} Train Epoch: 36 [6656/37814 (18%)]\tLoss: 1.980104\n",
            "{AlexNet} Train Epoch: 36 [7168/37814 (19%)]\tLoss: 1.983408\n",
            "{AlexNet} Train Epoch: 36 [7680/37814 (20%)]\tLoss: 1.955469\n",
            "{AlexNet} Train Epoch: 36 [8192/37814 (22%)]\tLoss: 1.947799\n",
            "{AlexNet} Train Epoch: 36 [8704/37814 (23%)]\tLoss: 1.998289\n",
            "{AlexNet} Train Epoch: 36 [9216/37814 (24%)]\tLoss: 1.996790\n",
            "{AlexNet} Train Epoch: 36 [9728/37814 (26%)]\tLoss: 2.000214\n",
            "{AlexNet} Train Epoch: 36 [10240/37814 (27%)]\tLoss: 2.001954\n",
            "{AlexNet} Train Epoch: 36 [10752/37814 (28%)]\tLoss: 1.974928\n",
            "{AlexNet} Train Epoch: 36 [11264/37814 (30%)]\tLoss: 1.964414\n",
            "{AlexNet} Train Epoch: 36 [11776/37814 (31%)]\tLoss: 1.950744\n",
            "{AlexNet} Train Epoch: 36 [12288/37814 (32%)]\tLoss: 2.021227\n",
            "{AlexNet} Train Epoch: 36 [12800/37814 (34%)]\tLoss: 2.004743\n",
            "{AlexNet} Train Epoch: 36 [13312/37814 (35%)]\tLoss: 1.963496\n",
            "{AlexNet} Train Epoch: 36 [13824/37814 (36%)]\tLoss: 1.953272\n",
            "{AlexNet} Train Epoch: 36 [14336/37814 (38%)]\tLoss: 1.972514\n",
            "{AlexNet} Train Epoch: 36 [14848/37814 (39%)]\tLoss: 1.942482\n",
            "{AlexNet} Train Epoch: 36 [15360/37814 (41%)]\tLoss: 1.991948\n",
            "{AlexNet} Train Epoch: 36 [15872/37814 (42%)]\tLoss: 2.005175\n",
            "{AlexNet} Train Epoch: 36 [16384/37814 (43%)]\tLoss: 1.958167\n",
            "{AlexNet} Train Epoch: 36 [16896/37814 (45%)]\tLoss: 1.929770\n",
            "{AlexNet} Train Epoch: 36 [17408/37814 (46%)]\tLoss: 1.940341\n",
            "{AlexNet} Train Epoch: 36 [17920/37814 (47%)]\tLoss: 1.974437\n",
            "{AlexNet} Train Epoch: 36 [18432/37814 (49%)]\tLoss: 2.002790\n",
            "{AlexNet} Train Epoch: 36 [18944/37814 (50%)]\tLoss: 1.954889\n",
            "{AlexNet} Train Epoch: 36 [19456/37814 (51%)]\tLoss: 1.984349\n",
            "{AlexNet} Train Epoch: 36 [19968/37814 (53%)]\tLoss: 2.005256\n",
            "{AlexNet} Train Epoch: 36 [20480/37814 (54%)]\tLoss: 1.932708\n",
            "{AlexNet} Train Epoch: 36 [20992/37814 (55%)]\tLoss: 1.988816\n",
            "{AlexNet} Train Epoch: 36 [21504/37814 (57%)]\tLoss: 1.965754\n",
            "{AlexNet} Train Epoch: 36 [22016/37814 (58%)]\tLoss: 1.961077\n",
            "{AlexNet} Train Epoch: 36 [22528/37814 (59%)]\tLoss: 1.985762\n",
            "{AlexNet} Train Epoch: 36 [23040/37814 (61%)]\tLoss: 1.969434\n",
            "{AlexNet} Train Epoch: 36 [23552/37814 (62%)]\tLoss: 1.984264\n",
            "{AlexNet} Train Epoch: 36 [24064/37814 (64%)]\tLoss: 1.954310\n",
            "{AlexNet} Train Epoch: 36 [24576/37814 (65%)]\tLoss: 1.967068\n",
            "{AlexNet} Train Epoch: 36 [25088/37814 (66%)]\tLoss: 1.906185\n",
            "{AlexNet} Train Epoch: 36 [25600/37814 (68%)]\tLoss: 1.984479\n",
            "{AlexNet} Train Epoch: 36 [26112/37814 (69%)]\tLoss: 1.995967\n",
            "{AlexNet} Train Epoch: 36 [26624/37814 (70%)]\tLoss: 2.018033\n",
            "{AlexNet} Train Epoch: 36 [27136/37814 (72%)]\tLoss: 1.983527\n",
            "{AlexNet} Train Epoch: 36 [27648/37814 (73%)]\tLoss: 1.939597\n",
            "{AlexNet} Train Epoch: 36 [28160/37814 (74%)]\tLoss: 1.962697\n",
            "{AlexNet} Train Epoch: 36 [28672/37814 (76%)]\tLoss: 1.943502\n",
            "{AlexNet} Train Epoch: 36 [29184/37814 (77%)]\tLoss: 1.925120\n",
            "{AlexNet} Train Epoch: 36 [29696/37814 (78%)]\tLoss: 1.988253\n",
            "{AlexNet} Train Epoch: 36 [30208/37814 (80%)]\tLoss: 2.016176\n",
            "{AlexNet} Train Epoch: 36 [30720/37814 (81%)]\tLoss: 2.003797\n",
            "{AlexNet} Train Epoch: 36 [31232/37814 (82%)]\tLoss: 1.905025\n",
            "{AlexNet} Train Epoch: 36 [31744/37814 (84%)]\tLoss: 1.984577\n",
            "{AlexNet} Train Epoch: 36 [32256/37814 (85%)]\tLoss: 1.947755\n",
            "{AlexNet} Train Epoch: 36 [32768/37814 (86%)]\tLoss: 1.974838\n",
            "{AlexNet} Train Epoch: 36 [33280/37814 (88%)]\tLoss: 1.976393\n",
            "{AlexNet} Train Epoch: 36 [33792/37814 (89%)]\tLoss: 2.017540\n",
            "{AlexNet} Train Epoch: 36 [34304/37814 (91%)]\tLoss: 1.947141\n",
            "{AlexNet} Train Epoch: 36 [34816/37814 (92%)]\tLoss: 1.951644\n",
            "{AlexNet} Train Epoch: 36 [35328/37814 (93%)]\tLoss: 1.970402\n",
            "{AlexNet} Train Epoch: 36 [35840/37814 (95%)]\tLoss: 2.080486\n",
            "{AlexNet} Train Epoch: 36 [36352/37814 (96%)]\tLoss: 2.003324\n",
            "{AlexNet} Train Epoch: 36 [36864/37814 (97%)]\tLoss: 1.946652\n",
            "{AlexNet} Train Epoch: 36 [31974/37814 (99%)]\tLoss: 1.988277\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9676, Accuracy: 1062/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 29.095004320144653 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 36 [0/37814 (0%)]\tLoss: 1.799355\n",
            "{SqueezeNet} Train Epoch: 36 [512/37814 (1%)]\tLoss: 1.791515\n",
            "{SqueezeNet} Train Epoch: 36 [1024/37814 (3%)]\tLoss: 1.824672\n",
            "{SqueezeNet} Train Epoch: 36 [1536/37814 (4%)]\tLoss: 1.824699\n",
            "{SqueezeNet} Train Epoch: 36 [2048/37814 (5%)]\tLoss: 1.830042\n",
            "{SqueezeNet} Train Epoch: 36 [2560/37814 (7%)]\tLoss: 1.847368\n",
            "{SqueezeNet} Train Epoch: 36 [3072/37814 (8%)]\tLoss: 1.856923\n",
            "{SqueezeNet} Train Epoch: 36 [3584/37814 (9%)]\tLoss: 1.825711\n",
            "{SqueezeNet} Train Epoch: 36 [4096/37814 (11%)]\tLoss: 1.915983\n",
            "{SqueezeNet} Train Epoch: 36 [4608/37814 (12%)]\tLoss: 1.804328\n",
            "{SqueezeNet} Train Epoch: 36 [5120/37814 (14%)]\tLoss: 1.840791\n",
            "{SqueezeNet} Train Epoch: 36 [5632/37814 (15%)]\tLoss: 1.802339\n",
            "{SqueezeNet} Train Epoch: 36 [6144/37814 (16%)]\tLoss: 1.794581\n",
            "{SqueezeNet} Train Epoch: 36 [6656/37814 (18%)]\tLoss: 1.811596\n",
            "{SqueezeNet} Train Epoch: 36 [7168/37814 (19%)]\tLoss: 1.782005\n",
            "{SqueezeNet} Train Epoch: 36 [7680/37814 (20%)]\tLoss: 1.813251\n",
            "{SqueezeNet} Train Epoch: 36 [8192/37814 (22%)]\tLoss: 1.812863\n",
            "{SqueezeNet} Train Epoch: 36 [8704/37814 (23%)]\tLoss: 1.846456\n",
            "{SqueezeNet} Train Epoch: 36 [9216/37814 (24%)]\tLoss: 1.810362\n",
            "{SqueezeNet} Train Epoch: 36 [9728/37814 (26%)]\tLoss: 1.800014\n",
            "{SqueezeNet} Train Epoch: 36 [10240/37814 (27%)]\tLoss: 1.835949\n",
            "{SqueezeNet} Train Epoch: 36 [10752/37814 (28%)]\tLoss: 1.801868\n",
            "{SqueezeNet} Train Epoch: 36 [11264/37814 (30%)]\tLoss: 1.822886\n",
            "{SqueezeNet} Train Epoch: 36 [11776/37814 (31%)]\tLoss: 1.817972\n",
            "{SqueezeNet} Train Epoch: 36 [12288/37814 (32%)]\tLoss: 1.860332\n",
            "{SqueezeNet} Train Epoch: 36 [12800/37814 (34%)]\tLoss: 1.845556\n",
            "{SqueezeNet} Train Epoch: 36 [13312/37814 (35%)]\tLoss: 1.828189\n",
            "{SqueezeNet} Train Epoch: 36 [13824/37814 (36%)]\tLoss: 1.833166\n",
            "{SqueezeNet} Train Epoch: 36 [14336/37814 (38%)]\tLoss: 1.852554\n",
            "{SqueezeNet} Train Epoch: 36 [14848/37814 (39%)]\tLoss: 1.842988\n",
            "{SqueezeNet} Train Epoch: 36 [15360/37814 (41%)]\tLoss: 1.852409\n",
            "{SqueezeNet} Train Epoch: 36 [15872/37814 (42%)]\tLoss: 1.843844\n",
            "{SqueezeNet} Train Epoch: 36 [16384/37814 (43%)]\tLoss: 1.834342\n",
            "{SqueezeNet} Train Epoch: 36 [16896/37814 (45%)]\tLoss: 1.833954\n",
            "{SqueezeNet} Train Epoch: 36 [17408/37814 (46%)]\tLoss: 1.824243\n",
            "{SqueezeNet} Train Epoch: 36 [17920/37814 (47%)]\tLoss: 1.841193\n",
            "{SqueezeNet} Train Epoch: 36 [18432/37814 (49%)]\tLoss: 1.771004\n",
            "{SqueezeNet} Train Epoch: 36 [18944/37814 (50%)]\tLoss: 1.857116\n",
            "{SqueezeNet} Train Epoch: 36 [19456/37814 (51%)]\tLoss: 1.791165\n",
            "{SqueezeNet} Train Epoch: 36 [19968/37814 (53%)]\tLoss: 1.817074\n",
            "{SqueezeNet} Train Epoch: 36 [20480/37814 (54%)]\tLoss: 1.850619\n",
            "{SqueezeNet} Train Epoch: 36 [20992/37814 (55%)]\tLoss: 1.772679\n",
            "{SqueezeNet} Train Epoch: 36 [21504/37814 (57%)]\tLoss: 1.830135\n",
            "{SqueezeNet} Train Epoch: 36 [22016/37814 (58%)]\tLoss: 1.910220\n",
            "{SqueezeNet} Train Epoch: 36 [22528/37814 (59%)]\tLoss: 1.823680\n",
            "{SqueezeNet} Train Epoch: 36 [23040/37814 (61%)]\tLoss: 1.851913\n",
            "{SqueezeNet} Train Epoch: 36 [23552/37814 (62%)]\tLoss: 1.815801\n",
            "{SqueezeNet} Train Epoch: 36 [24064/37814 (64%)]\tLoss: 1.863907\n",
            "{SqueezeNet} Train Epoch: 36 [24576/37814 (65%)]\tLoss: 1.870762\n",
            "{SqueezeNet} Train Epoch: 36 [25088/37814 (66%)]\tLoss: 1.814202\n",
            "{SqueezeNet} Train Epoch: 36 [25600/37814 (68%)]\tLoss: 1.813438\n",
            "{SqueezeNet} Train Epoch: 36 [26112/37814 (69%)]\tLoss: 1.789410\n",
            "{SqueezeNet} Train Epoch: 36 [26624/37814 (70%)]\tLoss: 1.804883\n",
            "{SqueezeNet} Train Epoch: 36 [27136/37814 (72%)]\tLoss: 1.832878\n",
            "{SqueezeNet} Train Epoch: 36 [27648/37814 (73%)]\tLoss: 1.841598\n",
            "{SqueezeNet} Train Epoch: 36 [28160/37814 (74%)]\tLoss: 1.817908\n",
            "{SqueezeNet} Train Epoch: 36 [28672/37814 (76%)]\tLoss: 1.800643\n",
            "{SqueezeNet} Train Epoch: 36 [29184/37814 (77%)]\tLoss: 1.808040\n",
            "{SqueezeNet} Train Epoch: 36 [29696/37814 (78%)]\tLoss: 1.844127\n",
            "{SqueezeNet} Train Epoch: 36 [30208/37814 (80%)]\tLoss: 1.821348\n",
            "{SqueezeNet} Train Epoch: 36 [30720/37814 (81%)]\tLoss: 1.783227\n",
            "{SqueezeNet} Train Epoch: 36 [31232/37814 (82%)]\tLoss: 1.866416\n",
            "{SqueezeNet} Train Epoch: 36 [31744/37814 (84%)]\tLoss: 1.885097\n",
            "{SqueezeNet} Train Epoch: 36 [32256/37814 (85%)]\tLoss: 1.783699\n",
            "{SqueezeNet} Train Epoch: 36 [32768/37814 (86%)]\tLoss: 1.866807\n",
            "{SqueezeNet} Train Epoch: 36 [33280/37814 (88%)]\tLoss: 1.804064\n",
            "{SqueezeNet} Train Epoch: 36 [33792/37814 (89%)]\tLoss: 1.835224\n",
            "{SqueezeNet} Train Epoch: 36 [34304/37814 (91%)]\tLoss: 1.873494\n",
            "{SqueezeNet} Train Epoch: 36 [34816/37814 (92%)]\tLoss: 1.859849\n",
            "{SqueezeNet} Train Epoch: 36 [35328/37814 (93%)]\tLoss: 1.803739\n",
            "{SqueezeNet} Train Epoch: 36 [35840/37814 (95%)]\tLoss: 1.796526\n",
            "{SqueezeNet} Train Epoch: 36 [36352/37814 (96%)]\tLoss: 1.811923\n",
            "{SqueezeNet} Train Epoch: 36 [36864/37814 (97%)]\tLoss: 1.821483\n",
            "{SqueezeNet} Train Epoch: 36 [31974/37814 (99%)]\tLoss: 1.845164\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8250, Accuracy: 1554/5000 (31%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.674902200698853 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"43e525f6-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2174510e-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_a4d70051f2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"43e6fca0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_9a8a14dab9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"43e73698-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_1176e09125"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"43e77ef0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"43e73698-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_48529b12d8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"440e08f4-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"43e6fca0-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8bb70136dc"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"441027ec-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_7791d863f6"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"44108e6c-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_583ff9004d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"4410cad0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"44108e6c-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_06196f1e46"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 37 [0/37814 (0%)]\tLoss: 1.925474\n",
            "{AlexNet} Train Epoch: 37 [512/37814 (1%)]\tLoss: 1.942297\n",
            "{AlexNet} Train Epoch: 37 [1024/37814 (3%)]\tLoss: 1.948116\n",
            "{AlexNet} Train Epoch: 37 [1536/37814 (4%)]\tLoss: 1.980583\n",
            "{AlexNet} Train Epoch: 37 [2048/37814 (5%)]\tLoss: 1.966328\n",
            "{AlexNet} Train Epoch: 37 [2560/37814 (7%)]\tLoss: 1.952914\n",
            "{AlexNet} Train Epoch: 37 [3072/37814 (8%)]\tLoss: 1.979693\n",
            "{AlexNet} Train Epoch: 37 [3584/37814 (9%)]\tLoss: 2.001678\n",
            "{AlexNet} Train Epoch: 37 [4096/37814 (11%)]\tLoss: 1.987319\n",
            "{AlexNet} Train Epoch: 37 [4608/37814 (12%)]\tLoss: 1.956998\n",
            "{AlexNet} Train Epoch: 37 [5120/37814 (14%)]\tLoss: 1.983597\n",
            "{AlexNet} Train Epoch: 37 [5632/37814 (15%)]\tLoss: 2.014433\n",
            "{AlexNet} Train Epoch: 37 [6144/37814 (16%)]\tLoss: 1.946147\n",
            "{AlexNet} Train Epoch: 37 [6656/37814 (18%)]\tLoss: 1.931815\n",
            "{AlexNet} Train Epoch: 37 [7168/37814 (19%)]\tLoss: 1.965341\n",
            "{AlexNet} Train Epoch: 37 [7680/37814 (20%)]\tLoss: 1.983815\n",
            "{AlexNet} Train Epoch: 37 [8192/37814 (22%)]\tLoss: 1.974801\n",
            "{AlexNet} Train Epoch: 37 [8704/37814 (23%)]\tLoss: 1.934166\n",
            "{AlexNet} Train Epoch: 37 [9216/37814 (24%)]\tLoss: 1.940710\n",
            "{AlexNet} Train Epoch: 37 [9728/37814 (26%)]\tLoss: 2.013989\n",
            "{AlexNet} Train Epoch: 37 [10240/37814 (27%)]\tLoss: 1.969134\n",
            "{AlexNet} Train Epoch: 37 [10752/37814 (28%)]\tLoss: 1.960363\n",
            "{AlexNet} Train Epoch: 37 [11264/37814 (30%)]\tLoss: 1.993984\n",
            "{AlexNet} Train Epoch: 37 [11776/37814 (31%)]\tLoss: 1.953793\n",
            "{AlexNet} Train Epoch: 37 [12288/37814 (32%)]\tLoss: 1.987669\n",
            "{AlexNet} Train Epoch: 37 [12800/37814 (34%)]\tLoss: 1.993346\n",
            "{AlexNet} Train Epoch: 37 [13312/37814 (35%)]\tLoss: 1.961338\n",
            "{AlexNet} Train Epoch: 37 [13824/37814 (36%)]\tLoss: 1.966011\n",
            "{AlexNet} Train Epoch: 37 [14336/37814 (38%)]\tLoss: 1.952784\n",
            "{AlexNet} Train Epoch: 37 [14848/37814 (39%)]\tLoss: 1.941023\n",
            "{AlexNet} Train Epoch: 37 [15360/37814 (41%)]\tLoss: 2.008977\n",
            "{AlexNet} Train Epoch: 37 [15872/37814 (42%)]\tLoss: 1.974988\n",
            "{AlexNet} Train Epoch: 37 [16384/37814 (43%)]\tLoss: 2.016922\n",
            "{AlexNet} Train Epoch: 37 [16896/37814 (45%)]\tLoss: 1.962434\n",
            "{AlexNet} Train Epoch: 37 [17408/37814 (46%)]\tLoss: 1.946401\n",
            "{AlexNet} Train Epoch: 37 [17920/37814 (47%)]\tLoss: 2.022061\n",
            "{AlexNet} Train Epoch: 37 [18432/37814 (49%)]\tLoss: 1.984959\n",
            "{AlexNet} Train Epoch: 37 [18944/37814 (50%)]\tLoss: 1.964342\n",
            "{AlexNet} Train Epoch: 37 [19456/37814 (51%)]\tLoss: 1.970966\n",
            "{AlexNet} Train Epoch: 37 [19968/37814 (53%)]\tLoss: 1.975453\n",
            "{AlexNet} Train Epoch: 37 [20480/37814 (54%)]\tLoss: 1.987886\n",
            "{AlexNet} Train Epoch: 37 [20992/37814 (55%)]\tLoss: 1.931358\n",
            "{AlexNet} Train Epoch: 37 [21504/37814 (57%)]\tLoss: 1.980344\n",
            "{AlexNet} Train Epoch: 37 [22016/37814 (58%)]\tLoss: 1.995573\n",
            "{AlexNet} Train Epoch: 37 [22528/37814 (59%)]\tLoss: 1.981779\n",
            "{AlexNet} Train Epoch: 37 [23040/37814 (61%)]\tLoss: 1.949930\n",
            "{AlexNet} Train Epoch: 37 [23552/37814 (62%)]\tLoss: 2.006661\n",
            "{AlexNet} Train Epoch: 37 [24064/37814 (64%)]\tLoss: 2.006736\n",
            "{AlexNet} Train Epoch: 37 [24576/37814 (65%)]\tLoss: 1.987980\n",
            "{AlexNet} Train Epoch: 37 [25088/37814 (66%)]\tLoss: 1.985247\n",
            "{AlexNet} Train Epoch: 37 [25600/37814 (68%)]\tLoss: 1.972191\n",
            "{AlexNet} Train Epoch: 37 [26112/37814 (69%)]\tLoss: 1.958222\n",
            "{AlexNet} Train Epoch: 37 [26624/37814 (70%)]\tLoss: 2.002242\n",
            "{AlexNet} Train Epoch: 37 [27136/37814 (72%)]\tLoss: 2.001308\n",
            "{AlexNet} Train Epoch: 37 [27648/37814 (73%)]\tLoss: 1.987922\n",
            "{AlexNet} Train Epoch: 37 [28160/37814 (74%)]\tLoss: 1.967748\n",
            "{AlexNet} Train Epoch: 37 [28672/37814 (76%)]\tLoss: 1.948713\n",
            "{AlexNet} Train Epoch: 37 [29184/37814 (77%)]\tLoss: 1.975733\n",
            "{AlexNet} Train Epoch: 37 [29696/37814 (78%)]\tLoss: 2.004647\n",
            "{AlexNet} Train Epoch: 37 [30208/37814 (80%)]\tLoss: 2.014034\n",
            "{AlexNet} Train Epoch: 37 [30720/37814 (81%)]\tLoss: 1.972569\n",
            "{AlexNet} Train Epoch: 37 [31232/37814 (82%)]\tLoss: 2.005558\n",
            "{AlexNet} Train Epoch: 37 [31744/37814 (84%)]\tLoss: 1.935148\n",
            "{AlexNet} Train Epoch: 37 [32256/37814 (85%)]\tLoss: 1.936037\n",
            "{AlexNet} Train Epoch: 37 [32768/37814 (86%)]\tLoss: 1.953712\n",
            "{AlexNet} Train Epoch: 37 [33280/37814 (88%)]\tLoss: 2.003654\n",
            "{AlexNet} Train Epoch: 37 [33792/37814 (89%)]\tLoss: 1.987145\n",
            "{AlexNet} Train Epoch: 37 [34304/37814 (91%)]\tLoss: 1.962931\n",
            "{AlexNet} Train Epoch: 37 [34816/37814 (92%)]\tLoss: 2.000908\n",
            "{AlexNet} Train Epoch: 37 [35328/37814 (93%)]\tLoss: 1.979799\n",
            "{AlexNet} Train Epoch: 37 [35840/37814 (95%)]\tLoss: 1.969683\n",
            "{AlexNet} Train Epoch: 37 [36352/37814 (96%)]\tLoss: 1.967839\n",
            "{AlexNet} Train Epoch: 37 [36864/37814 (97%)]\tLoss: 1.956740\n",
            "{AlexNet} Train Epoch: 37 [31974/37814 (99%)]\tLoss: 1.984985\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9726, Accuracy: 1031/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.85509753227234 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 37 [0/37814 (0%)]\tLoss: 1.775077\n",
            "{SqueezeNet} Train Epoch: 37 [512/37814 (1%)]\tLoss: 1.807929\n",
            "{SqueezeNet} Train Epoch: 37 [1024/37814 (3%)]\tLoss: 1.875407\n",
            "{SqueezeNet} Train Epoch: 37 [1536/37814 (4%)]\tLoss: 1.822702\n",
            "{SqueezeNet} Train Epoch: 37 [2048/37814 (5%)]\tLoss: 1.795950\n",
            "{SqueezeNet} Train Epoch: 37 [2560/37814 (7%)]\tLoss: 1.874284\n",
            "{SqueezeNet} Train Epoch: 37 [3072/37814 (8%)]\tLoss: 1.831143\n",
            "{SqueezeNet} Train Epoch: 37 [3584/37814 (9%)]\tLoss: 1.748536\n",
            "{SqueezeNet} Train Epoch: 37 [4096/37814 (11%)]\tLoss: 1.771182\n",
            "{SqueezeNet} Train Epoch: 37 [4608/37814 (12%)]\tLoss: 1.811675\n",
            "{SqueezeNet} Train Epoch: 37 [5120/37814 (14%)]\tLoss: 1.846158\n",
            "{SqueezeNet} Train Epoch: 37 [5632/37814 (15%)]\tLoss: 1.803149\n",
            "{SqueezeNet} Train Epoch: 37 [6144/37814 (16%)]\tLoss: 1.845952\n",
            "{SqueezeNet} Train Epoch: 37 [6656/37814 (18%)]\tLoss: 1.846338\n",
            "{SqueezeNet} Train Epoch: 37 [7168/37814 (19%)]\tLoss: 1.804997\n",
            "{SqueezeNet} Train Epoch: 37 [7680/37814 (20%)]\tLoss: 1.904807\n",
            "{SqueezeNet} Train Epoch: 37 [8192/37814 (22%)]\tLoss: 1.769969\n",
            "{SqueezeNet} Train Epoch: 37 [8704/37814 (23%)]\tLoss: 1.834716\n",
            "{SqueezeNet} Train Epoch: 37 [9216/37814 (24%)]\tLoss: 1.887047\n",
            "{SqueezeNet} Train Epoch: 37 [9728/37814 (26%)]\tLoss: 1.806329\n",
            "{SqueezeNet} Train Epoch: 37 [10240/37814 (27%)]\tLoss: 1.910668\n",
            "{SqueezeNet} Train Epoch: 37 [10752/37814 (28%)]\tLoss: 1.807726\n",
            "{SqueezeNet} Train Epoch: 37 [11264/37814 (30%)]\tLoss: 1.791112\n",
            "{SqueezeNet} Train Epoch: 37 [11776/37814 (31%)]\tLoss: 1.896209\n",
            "{SqueezeNet} Train Epoch: 37 [12288/37814 (32%)]\tLoss: 1.808135\n",
            "{SqueezeNet} Train Epoch: 37 [12800/37814 (34%)]\tLoss: 1.898084\n",
            "{SqueezeNet} Train Epoch: 37 [13312/37814 (35%)]\tLoss: 1.887043\n",
            "{SqueezeNet} Train Epoch: 37 [13824/37814 (36%)]\tLoss: 1.834811\n",
            "{SqueezeNet} Train Epoch: 37 [14336/37814 (38%)]\tLoss: 1.838230\n",
            "{SqueezeNet} Train Epoch: 37 [14848/37814 (39%)]\tLoss: 1.828488\n",
            "{SqueezeNet} Train Epoch: 37 [15360/37814 (41%)]\tLoss: 1.867837\n",
            "{SqueezeNet} Train Epoch: 37 [15872/37814 (42%)]\tLoss: 1.947077\n",
            "{SqueezeNet} Train Epoch: 37 [16384/37814 (43%)]\tLoss: 1.787078\n",
            "{SqueezeNet} Train Epoch: 37 [16896/37814 (45%)]\tLoss: 1.779295\n",
            "{SqueezeNet} Train Epoch: 37 [17408/37814 (46%)]\tLoss: 1.790959\n",
            "{SqueezeNet} Train Epoch: 37 [17920/37814 (47%)]\tLoss: 1.837697\n",
            "{SqueezeNet} Train Epoch: 37 [18432/37814 (49%)]\tLoss: 1.872375\n",
            "{SqueezeNet} Train Epoch: 37 [18944/37814 (50%)]\tLoss: 1.767747\n",
            "{SqueezeNet} Train Epoch: 37 [19456/37814 (51%)]\tLoss: 1.831855\n",
            "{SqueezeNet} Train Epoch: 37 [19968/37814 (53%)]\tLoss: 1.821686\n",
            "{SqueezeNet} Train Epoch: 37 [20480/37814 (54%)]\tLoss: 1.809473\n",
            "{SqueezeNet} Train Epoch: 37 [20992/37814 (55%)]\tLoss: 1.821328\n",
            "{SqueezeNet} Train Epoch: 37 [21504/37814 (57%)]\tLoss: 1.840775\n",
            "{SqueezeNet} Train Epoch: 37 [22016/37814 (58%)]\tLoss: 1.847610\n",
            "{SqueezeNet} Train Epoch: 37 [22528/37814 (59%)]\tLoss: 1.825978\n",
            "{SqueezeNet} Train Epoch: 37 [23040/37814 (61%)]\tLoss: 1.829406\n",
            "{SqueezeNet} Train Epoch: 37 [23552/37814 (62%)]\tLoss: 1.752410\n",
            "{SqueezeNet} Train Epoch: 37 [24064/37814 (64%)]\tLoss: 1.815746\n",
            "{SqueezeNet} Train Epoch: 37 [24576/37814 (65%)]\tLoss: 1.831711\n",
            "{SqueezeNet} Train Epoch: 37 [25088/37814 (66%)]\tLoss: 1.812373\n",
            "{SqueezeNet} Train Epoch: 37 [25600/37814 (68%)]\tLoss: 1.822737\n",
            "{SqueezeNet} Train Epoch: 37 [26112/37814 (69%)]\tLoss: 1.827665\n",
            "{SqueezeNet} Train Epoch: 37 [26624/37814 (70%)]\tLoss: 1.772591\n",
            "{SqueezeNet} Train Epoch: 37 [27136/37814 (72%)]\tLoss: 1.803922\n",
            "{SqueezeNet} Train Epoch: 37 [27648/37814 (73%)]\tLoss: 1.826626\n",
            "{SqueezeNet} Train Epoch: 37 [28160/37814 (74%)]\tLoss: 1.812460\n",
            "{SqueezeNet} Train Epoch: 37 [28672/37814 (76%)]\tLoss: 1.825343\n",
            "{SqueezeNet} Train Epoch: 37 [29184/37814 (77%)]\tLoss: 1.809082\n",
            "{SqueezeNet} Train Epoch: 37 [29696/37814 (78%)]\tLoss: 1.810425\n",
            "{SqueezeNet} Train Epoch: 37 [30208/37814 (80%)]\tLoss: 1.845064\n",
            "{SqueezeNet} Train Epoch: 37 [30720/37814 (81%)]\tLoss: 1.802543\n",
            "{SqueezeNet} Train Epoch: 37 [31232/37814 (82%)]\tLoss: 1.851160\n",
            "{SqueezeNet} Train Epoch: 37 [31744/37814 (84%)]\tLoss: 1.819482\n",
            "{SqueezeNet} Train Epoch: 37 [32256/37814 (85%)]\tLoss: 1.880161\n",
            "{SqueezeNet} Train Epoch: 37 [32768/37814 (86%)]\tLoss: 1.787565\n",
            "{SqueezeNet} Train Epoch: 37 [33280/37814 (88%)]\tLoss: 1.778551\n",
            "{SqueezeNet} Train Epoch: 37 [33792/37814 (89%)]\tLoss: 1.824214\n",
            "{SqueezeNet} Train Epoch: 37 [34304/37814 (91%)]\tLoss: 1.822428\n",
            "{SqueezeNet} Train Epoch: 37 [34816/37814 (92%)]\tLoss: 1.864427\n",
            "{SqueezeNet} Train Epoch: 37 [35328/37814 (93%)]\tLoss: 1.816442\n",
            "{SqueezeNet} Train Epoch: 37 [35840/37814 (95%)]\tLoss: 1.772712\n",
            "{SqueezeNet} Train Epoch: 37 [36352/37814 (96%)]\tLoss: 1.777476\n",
            "{SqueezeNet} Train Epoch: 37 [36864/37814 (97%)]\tLoss: 1.822417\n",
            "{SqueezeNet} Train Epoch: 37 [31974/37814 (99%)]\tLoss: 1.825618\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8062, Accuracy: 1623/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.30032706260681 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"658adeee-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"441027ec-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0f63e188bb"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"658ca396-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_7cfe61f77d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"658cf396-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_a1b530e20d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"658d3a22-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"658cf396-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_a8d39ca4da"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"65b2cb52-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"658ca396-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_32e28998c9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"65b435aa-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_6820dfd2eb"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"65b4786c-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_664e42d5e8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"65b4bae8-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"65b4786c-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_933a833158"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 38 [0/37814 (0%)]\tLoss: 1.964238\n",
            "{AlexNet} Train Epoch: 38 [512/37814 (1%)]\tLoss: 2.021712\n",
            "{AlexNet} Train Epoch: 38 [1024/37814 (3%)]\tLoss: 2.020014\n",
            "{AlexNet} Train Epoch: 38 [1536/37814 (4%)]\tLoss: 2.007809\n",
            "{AlexNet} Train Epoch: 38 [2048/37814 (5%)]\tLoss: 1.966831\n",
            "{AlexNet} Train Epoch: 38 [2560/37814 (7%)]\tLoss: 2.002470\n",
            "{AlexNet} Train Epoch: 38 [3072/37814 (8%)]\tLoss: 1.972142\n",
            "{AlexNet} Train Epoch: 38 [3584/37814 (9%)]\tLoss: 1.967025\n",
            "{AlexNet} Train Epoch: 38 [4096/37814 (11%)]\tLoss: 1.945951\n",
            "{AlexNet} Train Epoch: 38 [4608/37814 (12%)]\tLoss: 1.945124\n",
            "{AlexNet} Train Epoch: 38 [5120/37814 (14%)]\tLoss: 1.962816\n",
            "{AlexNet} Train Epoch: 38 [5632/37814 (15%)]\tLoss: 2.005781\n",
            "{AlexNet} Train Epoch: 38 [6144/37814 (16%)]\tLoss: 1.954654\n",
            "{AlexNet} Train Epoch: 38 [6656/37814 (18%)]\tLoss: 1.959083\n",
            "{AlexNet} Train Epoch: 38 [7168/37814 (19%)]\tLoss: 1.941822\n",
            "{AlexNet} Train Epoch: 38 [7680/37814 (20%)]\tLoss: 1.987172\n",
            "{AlexNet} Train Epoch: 38 [8192/37814 (22%)]\tLoss: 1.987960\n",
            "{AlexNet} Train Epoch: 38 [8704/37814 (23%)]\tLoss: 1.972459\n",
            "{AlexNet} Train Epoch: 38 [9216/37814 (24%)]\tLoss: 1.997641\n",
            "{AlexNet} Train Epoch: 38 [9728/37814 (26%)]\tLoss: 2.006245\n",
            "{AlexNet} Train Epoch: 38 [10240/37814 (27%)]\tLoss: 1.966269\n",
            "{AlexNet} Train Epoch: 38 [10752/37814 (28%)]\tLoss: 1.953340\n",
            "{AlexNet} Train Epoch: 38 [11264/37814 (30%)]\tLoss: 1.941977\n",
            "{AlexNet} Train Epoch: 38 [11776/37814 (31%)]\tLoss: 1.923958\n",
            "{AlexNet} Train Epoch: 38 [12288/37814 (32%)]\tLoss: 1.944470\n",
            "{AlexNet} Train Epoch: 38 [12800/37814 (34%)]\tLoss: 1.992050\n",
            "{AlexNet} Train Epoch: 38 [13312/37814 (35%)]\tLoss: 1.966022\n",
            "{AlexNet} Train Epoch: 38 [13824/37814 (36%)]\tLoss: 1.969515\n",
            "{AlexNet} Train Epoch: 38 [14336/37814 (38%)]\tLoss: 1.999712\n",
            "{AlexNet} Train Epoch: 38 [14848/37814 (39%)]\tLoss: 1.945891\n",
            "{AlexNet} Train Epoch: 38 [15360/37814 (41%)]\tLoss: 1.979937\n",
            "{AlexNet} Train Epoch: 38 [15872/37814 (42%)]\tLoss: 1.967371\n",
            "{AlexNet} Train Epoch: 38 [16384/37814 (43%)]\tLoss: 1.973619\n",
            "{AlexNet} Train Epoch: 38 [16896/37814 (45%)]\tLoss: 1.998696\n",
            "{AlexNet} Train Epoch: 38 [17408/37814 (46%)]\tLoss: 1.974421\n",
            "{AlexNet} Train Epoch: 38 [17920/37814 (47%)]\tLoss: 1.966432\n",
            "{AlexNet} Train Epoch: 38 [18432/37814 (49%)]\tLoss: 1.967804\n",
            "{AlexNet} Train Epoch: 38 [18944/37814 (50%)]\tLoss: 2.055076\n",
            "{AlexNet} Train Epoch: 38 [19456/37814 (51%)]\tLoss: 1.969126\n",
            "{AlexNet} Train Epoch: 38 [19968/37814 (53%)]\tLoss: 2.025386\n",
            "{AlexNet} Train Epoch: 38 [20480/37814 (54%)]\tLoss: 1.956138\n",
            "{AlexNet} Train Epoch: 38 [20992/37814 (55%)]\tLoss: 1.938272\n",
            "{AlexNet} Train Epoch: 38 [21504/37814 (57%)]\tLoss: 1.954813\n",
            "{AlexNet} Train Epoch: 38 [22016/37814 (58%)]\tLoss: 1.966031\n",
            "{AlexNet} Train Epoch: 38 [22528/37814 (59%)]\tLoss: 1.989518\n",
            "{AlexNet} Train Epoch: 38 [23040/37814 (61%)]\tLoss: 1.966587\n",
            "{AlexNet} Train Epoch: 38 [23552/37814 (62%)]\tLoss: 1.896824\n",
            "{AlexNet} Train Epoch: 38 [24064/37814 (64%)]\tLoss: 1.975251\n",
            "{AlexNet} Train Epoch: 38 [24576/37814 (65%)]\tLoss: 1.985487\n",
            "{AlexNet} Train Epoch: 38 [25088/37814 (66%)]\tLoss: 1.983904\n",
            "{AlexNet} Train Epoch: 38 [25600/37814 (68%)]\tLoss: 1.982222\n",
            "{AlexNet} Train Epoch: 38 [26112/37814 (69%)]\tLoss: 1.953925\n",
            "{AlexNet} Train Epoch: 38 [26624/37814 (70%)]\tLoss: 1.950829\n",
            "{AlexNet} Train Epoch: 38 [27136/37814 (72%)]\tLoss: 1.961765\n",
            "{AlexNet} Train Epoch: 38 [27648/37814 (73%)]\tLoss: 1.971956\n",
            "{AlexNet} Train Epoch: 38 [28160/37814 (74%)]\tLoss: 1.997930\n",
            "{AlexNet} Train Epoch: 38 [28672/37814 (76%)]\tLoss: 1.933132\n",
            "{AlexNet} Train Epoch: 38 [29184/37814 (77%)]\tLoss: 2.009288\n",
            "{AlexNet} Train Epoch: 38 [29696/37814 (78%)]\tLoss: 1.989032\n",
            "{AlexNet} Train Epoch: 38 [30208/37814 (80%)]\tLoss: 1.993220\n",
            "{AlexNet} Train Epoch: 38 [30720/37814 (81%)]\tLoss: 1.974189\n",
            "{AlexNet} Train Epoch: 38 [31232/37814 (82%)]\tLoss: 1.938920\n",
            "{AlexNet} Train Epoch: 38 [31744/37814 (84%)]\tLoss: 1.954124\n",
            "{AlexNet} Train Epoch: 38 [32256/37814 (85%)]\tLoss: 1.993209\n",
            "{AlexNet} Train Epoch: 38 [32768/37814 (86%)]\tLoss: 1.984747\n",
            "{AlexNet} Train Epoch: 38 [33280/37814 (88%)]\tLoss: 1.951532\n",
            "{AlexNet} Train Epoch: 38 [33792/37814 (89%)]\tLoss: 1.955596\n",
            "{AlexNet} Train Epoch: 38 [34304/37814 (91%)]\tLoss: 1.942950\n",
            "{AlexNet} Train Epoch: 38 [34816/37814 (92%)]\tLoss: 1.998253\n",
            "{AlexNet} Train Epoch: 38 [35328/37814 (93%)]\tLoss: 2.040627\n",
            "{AlexNet} Train Epoch: 38 [35840/37814 (95%)]\tLoss: 2.019737\n",
            "{AlexNet} Train Epoch: 38 [36352/37814 (96%)]\tLoss: 1.957646\n",
            "{AlexNet} Train Epoch: 38 [36864/37814 (97%)]\tLoss: 1.923277\n",
            "{AlexNet} Train Epoch: 38 [31974/37814 (99%)]\tLoss: 2.022972\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9787, Accuracy: 1022/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.738446950912476 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 38 [0/37814 (0%)]\tLoss: 1.854248\n",
            "{SqueezeNet} Train Epoch: 38 [512/37814 (1%)]\tLoss: 1.815080\n",
            "{SqueezeNet} Train Epoch: 38 [1024/37814 (3%)]\tLoss: 1.905391\n",
            "{SqueezeNet} Train Epoch: 38 [1536/37814 (4%)]\tLoss: 1.834615\n",
            "{SqueezeNet} Train Epoch: 38 [2048/37814 (5%)]\tLoss: 1.785592\n",
            "{SqueezeNet} Train Epoch: 38 [2560/37814 (7%)]\tLoss: 1.792973\n",
            "{SqueezeNet} Train Epoch: 38 [3072/37814 (8%)]\tLoss: 1.808733\n",
            "{SqueezeNet} Train Epoch: 38 [3584/37814 (9%)]\tLoss: 1.820376\n",
            "{SqueezeNet} Train Epoch: 38 [4096/37814 (11%)]\tLoss: 1.783520\n",
            "{SqueezeNet} Train Epoch: 38 [4608/37814 (12%)]\tLoss: 1.840851\n",
            "{SqueezeNet} Train Epoch: 38 [5120/37814 (14%)]\tLoss: 1.860142\n",
            "{SqueezeNet} Train Epoch: 38 [5632/37814 (15%)]\tLoss: 1.841760\n",
            "{SqueezeNet} Train Epoch: 38 [6144/37814 (16%)]\tLoss: 1.806079\n",
            "{SqueezeNet} Train Epoch: 38 [6656/37814 (18%)]\tLoss: 1.797174\n",
            "{SqueezeNet} Train Epoch: 38 [7168/37814 (19%)]\tLoss: 1.805723\n",
            "{SqueezeNet} Train Epoch: 38 [7680/37814 (20%)]\tLoss: 1.788540\n",
            "{SqueezeNet} Train Epoch: 38 [8192/37814 (22%)]\tLoss: 1.862704\n",
            "{SqueezeNet} Train Epoch: 38 [8704/37814 (23%)]\tLoss: 1.850456\n",
            "{SqueezeNet} Train Epoch: 38 [9216/37814 (24%)]\tLoss: 1.752900\n",
            "{SqueezeNet} Train Epoch: 38 [9728/37814 (26%)]\tLoss: 1.878992\n",
            "{SqueezeNet} Train Epoch: 38 [10240/37814 (27%)]\tLoss: 1.830871\n",
            "{SqueezeNet} Train Epoch: 38 [10752/37814 (28%)]\tLoss: 1.844556\n",
            "{SqueezeNet} Train Epoch: 38 [11264/37814 (30%)]\tLoss: 1.753673\n",
            "{SqueezeNet} Train Epoch: 38 [11776/37814 (31%)]\tLoss: 1.827821\n",
            "{SqueezeNet} Train Epoch: 38 [12288/37814 (32%)]\tLoss: 1.795155\n",
            "{SqueezeNet} Train Epoch: 38 [12800/37814 (34%)]\tLoss: 1.842846\n",
            "{SqueezeNet} Train Epoch: 38 [13312/37814 (35%)]\tLoss: 1.774934\n",
            "{SqueezeNet} Train Epoch: 38 [13824/37814 (36%)]\tLoss: 1.795099\n",
            "{SqueezeNet} Train Epoch: 38 [14336/37814 (38%)]\tLoss: 1.852332\n",
            "{SqueezeNet} Train Epoch: 38 [14848/37814 (39%)]\tLoss: 1.747853\n",
            "{SqueezeNet} Train Epoch: 38 [15360/37814 (41%)]\tLoss: 1.865253\n",
            "{SqueezeNet} Train Epoch: 38 [15872/37814 (42%)]\tLoss: 1.825467\n",
            "{SqueezeNet} Train Epoch: 38 [16384/37814 (43%)]\tLoss: 1.832119\n",
            "{SqueezeNet} Train Epoch: 38 [16896/37814 (45%)]\tLoss: 1.801208\n",
            "{SqueezeNet} Train Epoch: 38 [17408/37814 (46%)]\tLoss: 1.843299\n",
            "{SqueezeNet} Train Epoch: 38 [17920/37814 (47%)]\tLoss: 1.822225\n",
            "{SqueezeNet} Train Epoch: 38 [18432/37814 (49%)]\tLoss: 1.844231\n",
            "{SqueezeNet} Train Epoch: 38 [18944/37814 (50%)]\tLoss: 1.886162\n",
            "{SqueezeNet} Train Epoch: 38 [19456/37814 (51%)]\tLoss: 1.833652\n",
            "{SqueezeNet} Train Epoch: 38 [19968/37814 (53%)]\tLoss: 1.776780\n",
            "{SqueezeNet} Train Epoch: 38 [20480/37814 (54%)]\tLoss: 1.769347\n",
            "{SqueezeNet} Train Epoch: 38 [20992/37814 (55%)]\tLoss: 1.870382\n",
            "{SqueezeNet} Train Epoch: 38 [21504/37814 (57%)]\tLoss: 1.847340\n",
            "{SqueezeNet} Train Epoch: 38 [22016/37814 (58%)]\tLoss: 1.791809\n",
            "{SqueezeNet} Train Epoch: 38 [22528/37814 (59%)]\tLoss: 1.731228\n",
            "{SqueezeNet} Train Epoch: 38 [23040/37814 (61%)]\tLoss: 1.848024\n",
            "{SqueezeNet} Train Epoch: 38 [23552/37814 (62%)]\tLoss: 1.830117\n",
            "{SqueezeNet} Train Epoch: 38 [24064/37814 (64%)]\tLoss: 1.866815\n",
            "{SqueezeNet} Train Epoch: 38 [24576/37814 (65%)]\tLoss: 1.828834\n",
            "{SqueezeNet} Train Epoch: 38 [25088/37814 (66%)]\tLoss: 1.862463\n",
            "{SqueezeNet} Train Epoch: 38 [25600/37814 (68%)]\tLoss: 1.851244\n",
            "{SqueezeNet} Train Epoch: 38 [26112/37814 (69%)]\tLoss: 1.803624\n",
            "{SqueezeNet} Train Epoch: 38 [26624/37814 (70%)]\tLoss: 1.792657\n",
            "{SqueezeNet} Train Epoch: 38 [27136/37814 (72%)]\tLoss: 1.810616\n",
            "{SqueezeNet} Train Epoch: 38 [27648/37814 (73%)]\tLoss: 1.845701\n",
            "{SqueezeNet} Train Epoch: 38 [28160/37814 (74%)]\tLoss: 1.809559\n",
            "{SqueezeNet} Train Epoch: 38 [28672/37814 (76%)]\tLoss: 1.844227\n",
            "{SqueezeNet} Train Epoch: 38 [29184/37814 (77%)]\tLoss: 1.834368\n",
            "{SqueezeNet} Train Epoch: 38 [29696/37814 (78%)]\tLoss: 1.830994\n",
            "{SqueezeNet} Train Epoch: 38 [30208/37814 (80%)]\tLoss: 1.821869\n",
            "{SqueezeNet} Train Epoch: 38 [30720/37814 (81%)]\tLoss: 1.874331\n",
            "{SqueezeNet} Train Epoch: 38 [31232/37814 (82%)]\tLoss: 1.772342\n",
            "{SqueezeNet} Train Epoch: 38 [31744/37814 (84%)]\tLoss: 1.793239\n",
            "{SqueezeNet} Train Epoch: 38 [32256/37814 (85%)]\tLoss: 1.814243\n",
            "{SqueezeNet} Train Epoch: 38 [32768/37814 (86%)]\tLoss: 1.783652\n",
            "{SqueezeNet} Train Epoch: 38 [33280/37814 (88%)]\tLoss: 1.839186\n",
            "{SqueezeNet} Train Epoch: 38 [33792/37814 (89%)]\tLoss: 1.835980\n",
            "{SqueezeNet} Train Epoch: 38 [34304/37814 (91%)]\tLoss: 1.870954\n",
            "{SqueezeNet} Train Epoch: 38 [34816/37814 (92%)]\tLoss: 1.881089\n",
            "{SqueezeNet} Train Epoch: 38 [35328/37814 (93%)]\tLoss: 1.777097\n",
            "{SqueezeNet} Train Epoch: 38 [35840/37814 (95%)]\tLoss: 1.799841\n",
            "{SqueezeNet} Train Epoch: 38 [36352/37814 (96%)]\tLoss: 1.796436\n",
            "{SqueezeNet} Train Epoch: 38 [36864/37814 (97%)]\tLoss: 1.820618\n",
            "{SqueezeNet} Train Epoch: 38 [31974/37814 (99%)]\tLoss: 1.819129\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8168, Accuracy: 1634/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.36720323562622 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"868e24f2-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"65b435aa-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_54bb6f8f12"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"868fab92-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_97a40876c2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"868feb5c-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_270ca817b6"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"869020ae-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"868feb5c-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_338f510d64"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"86b7b9ac-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"868fab92-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_695dc6ee0f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"86b950a0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_e2e7629b04"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"86b990ec-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_67ddd4e2fb"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"86b9dbce-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"86b990ec-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_3a84945137"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 39 [0/37814 (0%)]\tLoss: 2.000658\n",
            "{AlexNet} Train Epoch: 39 [512/37814 (1%)]\tLoss: 2.000211\n",
            "{AlexNet} Train Epoch: 39 [1024/37814 (3%)]\tLoss: 1.980609\n",
            "{AlexNet} Train Epoch: 39 [1536/37814 (4%)]\tLoss: 2.002306\n",
            "{AlexNet} Train Epoch: 39 [2048/37814 (5%)]\tLoss: 2.009607\n",
            "{AlexNet} Train Epoch: 39 [2560/37814 (7%)]\tLoss: 1.999350\n",
            "{AlexNet} Train Epoch: 39 [3072/37814 (8%)]\tLoss: 1.959397\n",
            "{AlexNet} Train Epoch: 39 [3584/37814 (9%)]\tLoss: 2.016741\n",
            "{AlexNet} Train Epoch: 39 [4096/37814 (11%)]\tLoss: 1.952103\n",
            "{AlexNet} Train Epoch: 39 [4608/37814 (12%)]\tLoss: 1.933045\n",
            "{AlexNet} Train Epoch: 39 [5120/37814 (14%)]\tLoss: 1.999782\n",
            "{AlexNet} Train Epoch: 39 [5632/37814 (15%)]\tLoss: 1.924605\n",
            "{AlexNet} Train Epoch: 39 [6144/37814 (16%)]\tLoss: 2.011599\n",
            "{AlexNet} Train Epoch: 39 [6656/37814 (18%)]\tLoss: 1.994712\n",
            "{AlexNet} Train Epoch: 39 [7168/37814 (19%)]\tLoss: 2.008494\n",
            "{AlexNet} Train Epoch: 39 [7680/37814 (20%)]\tLoss: 1.990733\n",
            "{AlexNet} Train Epoch: 39 [8192/37814 (22%)]\tLoss: 1.994999\n",
            "{AlexNet} Train Epoch: 39 [8704/37814 (23%)]\tLoss: 1.962878\n",
            "{AlexNet} Train Epoch: 39 [9216/37814 (24%)]\tLoss: 1.967532\n",
            "{AlexNet} Train Epoch: 39 [9728/37814 (26%)]\tLoss: 1.899403\n",
            "{AlexNet} Train Epoch: 39 [10240/37814 (27%)]\tLoss: 1.978246\n",
            "{AlexNet} Train Epoch: 39 [10752/37814 (28%)]\tLoss: 1.931465\n",
            "{AlexNet} Train Epoch: 39 [11264/37814 (30%)]\tLoss: 2.017233\n",
            "{AlexNet} Train Epoch: 39 [11776/37814 (31%)]\tLoss: 1.982731\n",
            "{AlexNet} Train Epoch: 39 [12288/37814 (32%)]\tLoss: 1.960628\n",
            "{AlexNet} Train Epoch: 39 [12800/37814 (34%)]\tLoss: 1.991457\n",
            "{AlexNet} Train Epoch: 39 [13312/37814 (35%)]\tLoss: 1.984419\n",
            "{AlexNet} Train Epoch: 39 [13824/37814 (36%)]\tLoss: 2.013285\n",
            "{AlexNet} Train Epoch: 39 [14336/37814 (38%)]\tLoss: 1.942260\n",
            "{AlexNet} Train Epoch: 39 [14848/37814 (39%)]\tLoss: 1.948973\n",
            "{AlexNet} Train Epoch: 39 [15360/37814 (41%)]\tLoss: 1.968999\n",
            "{AlexNet} Train Epoch: 39 [15872/37814 (42%)]\tLoss: 1.999178\n",
            "{AlexNet} Train Epoch: 39 [16384/37814 (43%)]\tLoss: 2.008759\n",
            "{AlexNet} Train Epoch: 39 [16896/37814 (45%)]\tLoss: 1.996099\n",
            "{AlexNet} Train Epoch: 39 [17408/37814 (46%)]\tLoss: 1.999612\n",
            "{AlexNet} Train Epoch: 39 [17920/37814 (47%)]\tLoss: 2.013664\n",
            "{AlexNet} Train Epoch: 39 [18432/37814 (49%)]\tLoss: 1.962569\n",
            "{AlexNet} Train Epoch: 39 [18944/37814 (50%)]\tLoss: 1.960177\n",
            "{AlexNet} Train Epoch: 39 [19456/37814 (51%)]\tLoss: 1.959527\n",
            "{AlexNet} Train Epoch: 39 [19968/37814 (53%)]\tLoss: 2.023802\n",
            "{AlexNet} Train Epoch: 39 [20480/37814 (54%)]\tLoss: 1.931166\n",
            "{AlexNet} Train Epoch: 39 [20992/37814 (55%)]\tLoss: 1.998555\n",
            "{AlexNet} Train Epoch: 39 [21504/37814 (57%)]\tLoss: 1.935339\n",
            "{AlexNet} Train Epoch: 39 [22016/37814 (58%)]\tLoss: 1.939745\n",
            "{AlexNet} Train Epoch: 39 [22528/37814 (59%)]\tLoss: 1.979047\n",
            "{AlexNet} Train Epoch: 39 [23040/37814 (61%)]\tLoss: 1.996916\n",
            "{AlexNet} Train Epoch: 39 [23552/37814 (62%)]\tLoss: 1.957270\n",
            "{AlexNet} Train Epoch: 39 [24064/37814 (64%)]\tLoss: 1.982856\n",
            "{AlexNet} Train Epoch: 39 [24576/37814 (65%)]\tLoss: 1.979806\n",
            "{AlexNet} Train Epoch: 39 [25088/37814 (66%)]\tLoss: 1.986051\n",
            "{AlexNet} Train Epoch: 39 [25600/37814 (68%)]\tLoss: 2.031623\n",
            "{AlexNet} Train Epoch: 39 [26112/37814 (69%)]\tLoss: 1.956522\n",
            "{AlexNet} Train Epoch: 39 [26624/37814 (70%)]\tLoss: 1.951207\n",
            "{AlexNet} Train Epoch: 39 [27136/37814 (72%)]\tLoss: 1.937562\n",
            "{AlexNet} Train Epoch: 39 [27648/37814 (73%)]\tLoss: 1.991253\n",
            "{AlexNet} Train Epoch: 39 [28160/37814 (74%)]\tLoss: 1.973781\n",
            "{AlexNet} Train Epoch: 39 [28672/37814 (76%)]\tLoss: 1.958956\n",
            "{AlexNet} Train Epoch: 39 [29184/37814 (77%)]\tLoss: 1.951749\n",
            "{AlexNet} Train Epoch: 39 [29696/37814 (78%)]\tLoss: 1.973709\n",
            "{AlexNet} Train Epoch: 39 [30208/37814 (80%)]\tLoss: 1.913387\n",
            "{AlexNet} Train Epoch: 39 [30720/37814 (81%)]\tLoss: 2.007837\n",
            "{AlexNet} Train Epoch: 39 [31232/37814 (82%)]\tLoss: 1.944847\n",
            "{AlexNet} Train Epoch: 39 [31744/37814 (84%)]\tLoss: 1.994242\n",
            "{AlexNet} Train Epoch: 39 [32256/37814 (85%)]\tLoss: 1.972211\n",
            "{AlexNet} Train Epoch: 39 [32768/37814 (86%)]\tLoss: 1.980351\n",
            "{AlexNet} Train Epoch: 39 [33280/37814 (88%)]\tLoss: 1.959294\n",
            "{AlexNet} Train Epoch: 39 [33792/37814 (89%)]\tLoss: 1.982249\n",
            "{AlexNet} Train Epoch: 39 [34304/37814 (91%)]\tLoss: 1.950498\n",
            "{AlexNet} Train Epoch: 39 [34816/37814 (92%)]\tLoss: 1.956127\n",
            "{AlexNet} Train Epoch: 39 [35328/37814 (93%)]\tLoss: 1.968284\n",
            "{AlexNet} Train Epoch: 39 [35840/37814 (95%)]\tLoss: 1.978587\n",
            "{AlexNet} Train Epoch: 39 [36352/37814 (96%)]\tLoss: 1.954602\n",
            "{AlexNet} Train Epoch: 39 [36864/37814 (97%)]\tLoss: 1.995857\n",
            "{AlexNet} Train Epoch: 39 [31974/37814 (99%)]\tLoss: 1.975496\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9753, Accuracy: 1039/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.818434476852417 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 39 [0/37814 (0%)]\tLoss: 1.823305\n",
            "{SqueezeNet} Train Epoch: 39 [512/37814 (1%)]\tLoss: 1.853092\n",
            "{SqueezeNet} Train Epoch: 39 [1024/37814 (3%)]\tLoss: 1.825093\n",
            "{SqueezeNet} Train Epoch: 39 [1536/37814 (4%)]\tLoss: 1.850979\n",
            "{SqueezeNet} Train Epoch: 39 [2048/37814 (5%)]\tLoss: 1.863811\n",
            "{SqueezeNet} Train Epoch: 39 [2560/37814 (7%)]\tLoss: 1.872198\n",
            "{SqueezeNet} Train Epoch: 39 [3072/37814 (8%)]\tLoss: 1.873553\n",
            "{SqueezeNet} Train Epoch: 39 [3584/37814 (9%)]\tLoss: 1.870984\n",
            "{SqueezeNet} Train Epoch: 39 [4096/37814 (11%)]\tLoss: 1.835390\n",
            "{SqueezeNet} Train Epoch: 39 [4608/37814 (12%)]\tLoss: 1.807231\n",
            "{SqueezeNet} Train Epoch: 39 [5120/37814 (14%)]\tLoss: 1.797454\n",
            "{SqueezeNet} Train Epoch: 39 [5632/37814 (15%)]\tLoss: 1.777747\n",
            "{SqueezeNet} Train Epoch: 39 [6144/37814 (16%)]\tLoss: 1.838891\n",
            "{SqueezeNet} Train Epoch: 39 [6656/37814 (18%)]\tLoss: 1.832768\n",
            "{SqueezeNet} Train Epoch: 39 [7168/37814 (19%)]\tLoss: 1.825732\n",
            "{SqueezeNet} Train Epoch: 39 [7680/37814 (20%)]\tLoss: 1.849931\n",
            "{SqueezeNet} Train Epoch: 39 [8192/37814 (22%)]\tLoss: 1.790983\n",
            "{SqueezeNet} Train Epoch: 39 [8704/37814 (23%)]\tLoss: 1.784621\n",
            "{SqueezeNet} Train Epoch: 39 [9216/37814 (24%)]\tLoss: 1.780926\n",
            "{SqueezeNet} Train Epoch: 39 [9728/37814 (26%)]\tLoss: 1.819901\n",
            "{SqueezeNet} Train Epoch: 39 [10240/37814 (27%)]\tLoss: 1.844023\n",
            "{SqueezeNet} Train Epoch: 39 [10752/37814 (28%)]\tLoss: 1.801803\n",
            "{SqueezeNet} Train Epoch: 39 [11264/37814 (30%)]\tLoss: 1.794280\n",
            "{SqueezeNet} Train Epoch: 39 [11776/37814 (31%)]\tLoss: 1.854112\n",
            "{SqueezeNet} Train Epoch: 39 [12288/37814 (32%)]\tLoss: 1.836517\n",
            "{SqueezeNet} Train Epoch: 39 [12800/37814 (34%)]\tLoss: 1.778301\n",
            "{SqueezeNet} Train Epoch: 39 [13312/37814 (35%)]\tLoss: 1.794293\n",
            "{SqueezeNet} Train Epoch: 39 [13824/37814 (36%)]\tLoss: 1.796133\n",
            "{SqueezeNet} Train Epoch: 39 [14336/37814 (38%)]\tLoss: 1.817128\n",
            "{SqueezeNet} Train Epoch: 39 [14848/37814 (39%)]\tLoss: 1.846159\n",
            "{SqueezeNet} Train Epoch: 39 [15360/37814 (41%)]\tLoss: 1.811622\n",
            "{SqueezeNet} Train Epoch: 39 [15872/37814 (42%)]\tLoss: 1.821834\n",
            "{SqueezeNet} Train Epoch: 39 [16384/37814 (43%)]\tLoss: 1.826765\n",
            "{SqueezeNet} Train Epoch: 39 [16896/37814 (45%)]\tLoss: 1.804143\n",
            "{SqueezeNet} Train Epoch: 39 [17408/37814 (46%)]\tLoss: 1.810029\n",
            "{SqueezeNet} Train Epoch: 39 [17920/37814 (47%)]\tLoss: 1.805222\n",
            "{SqueezeNet} Train Epoch: 39 [18432/37814 (49%)]\tLoss: 1.806029\n",
            "{SqueezeNet} Train Epoch: 39 [18944/37814 (50%)]\tLoss: 1.859507\n",
            "{SqueezeNet} Train Epoch: 39 [19456/37814 (51%)]\tLoss: 1.783787\n",
            "{SqueezeNet} Train Epoch: 39 [19968/37814 (53%)]\tLoss: 1.818363\n",
            "{SqueezeNet} Train Epoch: 39 [20480/37814 (54%)]\tLoss: 1.875280\n",
            "{SqueezeNet} Train Epoch: 39 [20992/37814 (55%)]\tLoss: 1.835448\n",
            "{SqueezeNet} Train Epoch: 39 [21504/37814 (57%)]\tLoss: 1.850502\n",
            "{SqueezeNet} Train Epoch: 39 [22016/37814 (58%)]\tLoss: 1.838260\n",
            "{SqueezeNet} Train Epoch: 39 [22528/37814 (59%)]\tLoss: 1.824823\n",
            "{SqueezeNet} Train Epoch: 39 [23040/37814 (61%)]\tLoss: 1.836137\n",
            "{SqueezeNet} Train Epoch: 39 [23552/37814 (62%)]\tLoss: 1.744666\n",
            "{SqueezeNet} Train Epoch: 39 [24064/37814 (64%)]\tLoss: 1.791791\n",
            "{SqueezeNet} Train Epoch: 39 [24576/37814 (65%)]\tLoss: 1.790652\n",
            "{SqueezeNet} Train Epoch: 39 [25088/37814 (66%)]\tLoss: 1.840427\n",
            "{SqueezeNet} Train Epoch: 39 [25600/37814 (68%)]\tLoss: 1.858640\n",
            "{SqueezeNet} Train Epoch: 39 [26112/37814 (69%)]\tLoss: 1.812196\n",
            "{SqueezeNet} Train Epoch: 39 [26624/37814 (70%)]\tLoss: 1.844538\n",
            "{SqueezeNet} Train Epoch: 39 [27136/37814 (72%)]\tLoss: 1.813244\n",
            "{SqueezeNet} Train Epoch: 39 [27648/37814 (73%)]\tLoss: 1.754859\n",
            "{SqueezeNet} Train Epoch: 39 [28160/37814 (74%)]\tLoss: 1.817572\n",
            "{SqueezeNet} Train Epoch: 39 [28672/37814 (76%)]\tLoss: 1.808875\n",
            "{SqueezeNet} Train Epoch: 39 [29184/37814 (77%)]\tLoss: 1.904194\n",
            "{SqueezeNet} Train Epoch: 39 [29696/37814 (78%)]\tLoss: 1.844882\n",
            "{SqueezeNet} Train Epoch: 39 [30208/37814 (80%)]\tLoss: 1.803687\n",
            "{SqueezeNet} Train Epoch: 39 [30720/37814 (81%)]\tLoss: 1.837323\n",
            "{SqueezeNet} Train Epoch: 39 [31232/37814 (82%)]\tLoss: 1.839023\n",
            "{SqueezeNet} Train Epoch: 39 [31744/37814 (84%)]\tLoss: 1.797879\n",
            "{SqueezeNet} Train Epoch: 39 [32256/37814 (85%)]\tLoss: 1.758582\n",
            "{SqueezeNet} Train Epoch: 39 [32768/37814 (86%)]\tLoss: 1.888775\n",
            "{SqueezeNet} Train Epoch: 39 [33280/37814 (88%)]\tLoss: 1.888639\n",
            "{SqueezeNet} Train Epoch: 39 [33792/37814 (89%)]\tLoss: 1.838891\n",
            "{SqueezeNet} Train Epoch: 39 [34304/37814 (91%)]\tLoss: 1.866015\n",
            "{SqueezeNet} Train Epoch: 39 [34816/37814 (92%)]\tLoss: 1.875201\n",
            "{SqueezeNet} Train Epoch: 39 [35328/37814 (93%)]\tLoss: 1.849025\n",
            "{SqueezeNet} Train Epoch: 39 [35840/37814 (95%)]\tLoss: 1.847716\n",
            "{SqueezeNet} Train Epoch: 39 [36352/37814 (96%)]\tLoss: 1.832426\n",
            "{SqueezeNet} Train Epoch: 39 [36864/37814 (97%)]\tLoss: 1.819099\n",
            "{SqueezeNet} Train Epoch: 39 [31974/37814 (99%)]\tLoss: 1.917520\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8124, Accuracy: 1642/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.854063510894775 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a882a970-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"86b950a0-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_fa5ab9fde3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a884da92-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_8617f46e4f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8853848-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_8c9bd95007"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8858fa0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a8853848-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b927c8e9ce"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8b23aaa-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a884da92-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9376377db7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8b363d0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_dd73614329"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8b3a1c4-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_05a81b24c2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"a8b3dfcc-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a8b3a1c4-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f28f71b9e0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 40 [0/37814 (0%)]\tLoss: 1.993224\n",
            "{AlexNet} Train Epoch: 40 [512/37814 (1%)]\tLoss: 1.961676\n",
            "{AlexNet} Train Epoch: 40 [1024/37814 (3%)]\tLoss: 1.980081\n",
            "{AlexNet} Train Epoch: 40 [1536/37814 (4%)]\tLoss: 1.962756\n",
            "{AlexNet} Train Epoch: 40 [2048/37814 (5%)]\tLoss: 1.952972\n",
            "{AlexNet} Train Epoch: 40 [2560/37814 (7%)]\tLoss: 1.973274\n",
            "{AlexNet} Train Epoch: 40 [3072/37814 (8%)]\tLoss: 1.956106\n",
            "{AlexNet} Train Epoch: 40 [3584/37814 (9%)]\tLoss: 1.978757\n",
            "{AlexNet} Train Epoch: 40 [4096/37814 (11%)]\tLoss: 1.973793\n",
            "{AlexNet} Train Epoch: 40 [4608/37814 (12%)]\tLoss: 1.987104\n",
            "{AlexNet} Train Epoch: 40 [5120/37814 (14%)]\tLoss: 2.007684\n",
            "{AlexNet} Train Epoch: 40 [5632/37814 (15%)]\tLoss: 1.925226\n",
            "{AlexNet} Train Epoch: 40 [6144/37814 (16%)]\tLoss: 1.962222\n",
            "{AlexNet} Train Epoch: 40 [6656/37814 (18%)]\tLoss: 1.977410\n",
            "{AlexNet} Train Epoch: 40 [7168/37814 (19%)]\tLoss: 2.003037\n",
            "{AlexNet} Train Epoch: 40 [7680/37814 (20%)]\tLoss: 1.954986\n",
            "{AlexNet} Train Epoch: 40 [8192/37814 (22%)]\tLoss: 1.952655\n",
            "{AlexNet} Train Epoch: 40 [8704/37814 (23%)]\tLoss: 1.978284\n",
            "{AlexNet} Train Epoch: 40 [9216/37814 (24%)]\tLoss: 1.952563\n",
            "{AlexNet} Train Epoch: 40 [9728/37814 (26%)]\tLoss: 1.969250\n",
            "{AlexNet} Train Epoch: 40 [10240/37814 (27%)]\tLoss: 1.937845\n",
            "{AlexNet} Train Epoch: 40 [10752/37814 (28%)]\tLoss: 1.945301\n",
            "{AlexNet} Train Epoch: 40 [11264/37814 (30%)]\tLoss: 1.969368\n",
            "{AlexNet} Train Epoch: 40 [11776/37814 (31%)]\tLoss: 1.957732\n",
            "{AlexNet} Train Epoch: 40 [12288/37814 (32%)]\tLoss: 1.955611\n",
            "{AlexNet} Train Epoch: 40 [12800/37814 (34%)]\tLoss: 2.011697\n",
            "{AlexNet} Train Epoch: 40 [13312/37814 (35%)]\tLoss: 1.973601\n",
            "{AlexNet} Train Epoch: 40 [13824/37814 (36%)]\tLoss: 2.045255\n",
            "{AlexNet} Train Epoch: 40 [14336/37814 (38%)]\tLoss: 2.029905\n",
            "{AlexNet} Train Epoch: 40 [14848/37814 (39%)]\tLoss: 1.914703\n",
            "{AlexNet} Train Epoch: 40 [15360/37814 (41%)]\tLoss: 1.977374\n",
            "{AlexNet} Train Epoch: 40 [15872/37814 (42%)]\tLoss: 1.937157\n",
            "{AlexNet} Train Epoch: 40 [16384/37814 (43%)]\tLoss: 1.954053\n",
            "{AlexNet} Train Epoch: 40 [16896/37814 (45%)]\tLoss: 1.971531\n",
            "{AlexNet} Train Epoch: 40 [17408/37814 (46%)]\tLoss: 1.985080\n",
            "{AlexNet} Train Epoch: 40 [17920/37814 (47%)]\tLoss: 2.033844\n",
            "{AlexNet} Train Epoch: 40 [18432/37814 (49%)]\tLoss: 1.979232\n",
            "{AlexNet} Train Epoch: 40 [18944/37814 (50%)]\tLoss: 1.953455\n",
            "{AlexNet} Train Epoch: 40 [19456/37814 (51%)]\tLoss: 1.968767\n",
            "{AlexNet} Train Epoch: 40 [19968/37814 (53%)]\tLoss: 2.000484\n",
            "{AlexNet} Train Epoch: 40 [20480/37814 (54%)]\tLoss: 1.997330\n",
            "{AlexNet} Train Epoch: 40 [20992/37814 (55%)]\tLoss: 1.999267\n",
            "{AlexNet} Train Epoch: 40 [21504/37814 (57%)]\tLoss: 1.982230\n",
            "{AlexNet} Train Epoch: 40 [22016/37814 (58%)]\tLoss: 1.980530\n",
            "{AlexNet} Train Epoch: 40 [22528/37814 (59%)]\tLoss: 1.973566\n",
            "{AlexNet} Train Epoch: 40 [23040/37814 (61%)]\tLoss: 2.000453\n",
            "{AlexNet} Train Epoch: 40 [23552/37814 (62%)]\tLoss: 1.946211\n",
            "{AlexNet} Train Epoch: 40 [24064/37814 (64%)]\tLoss: 1.986111\n",
            "{AlexNet} Train Epoch: 40 [24576/37814 (65%)]\tLoss: 1.956712\n",
            "{AlexNet} Train Epoch: 40 [25088/37814 (66%)]\tLoss: 1.942798\n",
            "{AlexNet} Train Epoch: 40 [25600/37814 (68%)]\tLoss: 2.003596\n",
            "{AlexNet} Train Epoch: 40 [26112/37814 (69%)]\tLoss: 1.989444\n",
            "{AlexNet} Train Epoch: 40 [26624/37814 (70%)]\tLoss: 1.961354\n",
            "{AlexNet} Train Epoch: 40 [27136/37814 (72%)]\tLoss: 1.976474\n",
            "{AlexNet} Train Epoch: 40 [27648/37814 (73%)]\tLoss: 1.967655\n",
            "{AlexNet} Train Epoch: 40 [28160/37814 (74%)]\tLoss: 1.993904\n",
            "{AlexNet} Train Epoch: 40 [28672/37814 (76%)]\tLoss: 1.997418\n",
            "{AlexNet} Train Epoch: 40 [29184/37814 (77%)]\tLoss: 1.949173\n",
            "{AlexNet} Train Epoch: 40 [29696/37814 (78%)]\tLoss: 1.986150\n",
            "{AlexNet} Train Epoch: 40 [30208/37814 (80%)]\tLoss: 1.948829\n",
            "{AlexNet} Train Epoch: 40 [30720/37814 (81%)]\tLoss: 2.009950\n",
            "{AlexNet} Train Epoch: 40 [31232/37814 (82%)]\tLoss: 2.011073\n",
            "{AlexNet} Train Epoch: 40 [31744/37814 (84%)]\tLoss: 1.933282\n",
            "{AlexNet} Train Epoch: 40 [32256/37814 (85%)]\tLoss: 1.958090\n",
            "{AlexNet} Train Epoch: 40 [32768/37814 (86%)]\tLoss: 1.964155\n",
            "{AlexNet} Train Epoch: 40 [33280/37814 (88%)]\tLoss: 1.983983\n",
            "{AlexNet} Train Epoch: 40 [33792/37814 (89%)]\tLoss: 2.030972\n",
            "{AlexNet} Train Epoch: 40 [34304/37814 (91%)]\tLoss: 1.946057\n",
            "{AlexNet} Train Epoch: 40 [34816/37814 (92%)]\tLoss: 1.998336\n",
            "{AlexNet} Train Epoch: 40 [35328/37814 (93%)]\tLoss: 1.990425\n",
            "{AlexNet} Train Epoch: 40 [35840/37814 (95%)]\tLoss: 1.990637\n",
            "{AlexNet} Train Epoch: 40 [36352/37814 (96%)]\tLoss: 1.966432\n",
            "{AlexNet} Train Epoch: 40 [36864/37814 (97%)]\tLoss: 1.977750\n",
            "{AlexNet} Train Epoch: 40 [31974/37814 (99%)]\tLoss: 1.930374\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9723, Accuracy: 1076/5000 (22%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.826913833618164 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 40 [0/37814 (0%)]\tLoss: 1.803220\n",
            "{SqueezeNet} Train Epoch: 40 [512/37814 (1%)]\tLoss: 1.830717\n",
            "{SqueezeNet} Train Epoch: 40 [1024/37814 (3%)]\tLoss: 1.868230\n",
            "{SqueezeNet} Train Epoch: 40 [1536/37814 (4%)]\tLoss: 1.843498\n",
            "{SqueezeNet} Train Epoch: 40 [2048/37814 (5%)]\tLoss: 1.874020\n",
            "{SqueezeNet} Train Epoch: 40 [2560/37814 (7%)]\tLoss: 1.858881\n",
            "{SqueezeNet} Train Epoch: 40 [3072/37814 (8%)]\tLoss: 1.858353\n",
            "{SqueezeNet} Train Epoch: 40 [3584/37814 (9%)]\tLoss: 1.816161\n",
            "{SqueezeNet} Train Epoch: 40 [4096/37814 (11%)]\tLoss: 1.831842\n",
            "{SqueezeNet} Train Epoch: 40 [4608/37814 (12%)]\tLoss: 1.795695\n",
            "{SqueezeNet} Train Epoch: 40 [5120/37814 (14%)]\tLoss: 1.842030\n",
            "{SqueezeNet} Train Epoch: 40 [5632/37814 (15%)]\tLoss: 1.826799\n",
            "{SqueezeNet} Train Epoch: 40 [6144/37814 (16%)]\tLoss: 1.797747\n",
            "{SqueezeNet} Train Epoch: 40 [6656/37814 (18%)]\tLoss: 1.795931\n",
            "{SqueezeNet} Train Epoch: 40 [7168/37814 (19%)]\tLoss: 1.814015\n",
            "{SqueezeNet} Train Epoch: 40 [7680/37814 (20%)]\tLoss: 1.816623\n",
            "{SqueezeNet} Train Epoch: 40 [8192/37814 (22%)]\tLoss: 1.880697\n",
            "{SqueezeNet} Train Epoch: 40 [8704/37814 (23%)]\tLoss: 1.833447\n",
            "{SqueezeNet} Train Epoch: 40 [9216/37814 (24%)]\tLoss: 1.815445\n",
            "{SqueezeNet} Train Epoch: 40 [9728/37814 (26%)]\tLoss: 1.808509\n",
            "{SqueezeNet} Train Epoch: 40 [10240/37814 (27%)]\tLoss: 1.800635\n",
            "{SqueezeNet} Train Epoch: 40 [10752/37814 (28%)]\tLoss: 1.721335\n",
            "{SqueezeNet} Train Epoch: 40 [11264/37814 (30%)]\tLoss: 1.815625\n",
            "{SqueezeNet} Train Epoch: 40 [11776/37814 (31%)]\tLoss: 1.866872\n",
            "{SqueezeNet} Train Epoch: 40 [12288/37814 (32%)]\tLoss: 1.834194\n",
            "{SqueezeNet} Train Epoch: 40 [12800/37814 (34%)]\tLoss: 1.773978\n",
            "{SqueezeNet} Train Epoch: 40 [13312/37814 (35%)]\tLoss: 1.800291\n",
            "{SqueezeNet} Train Epoch: 40 [13824/37814 (36%)]\tLoss: 1.788089\n",
            "{SqueezeNet} Train Epoch: 40 [14336/37814 (38%)]\tLoss: 1.843108\n",
            "{SqueezeNet} Train Epoch: 40 [14848/37814 (39%)]\tLoss: 1.871053\n",
            "{SqueezeNet} Train Epoch: 40 [15360/37814 (41%)]\tLoss: 1.853960\n",
            "{SqueezeNet} Train Epoch: 40 [15872/37814 (42%)]\tLoss: 1.874754\n",
            "{SqueezeNet} Train Epoch: 40 [16384/37814 (43%)]\tLoss: 1.848548\n",
            "{SqueezeNet} Train Epoch: 40 [16896/37814 (45%)]\tLoss: 1.845207\n",
            "{SqueezeNet} Train Epoch: 40 [17408/37814 (46%)]\tLoss: 1.807855\n",
            "{SqueezeNet} Train Epoch: 40 [17920/37814 (47%)]\tLoss: 1.797416\n",
            "{SqueezeNet} Train Epoch: 40 [18432/37814 (49%)]\tLoss: 1.844978\n",
            "{SqueezeNet} Train Epoch: 40 [18944/37814 (50%)]\tLoss: 1.786476\n",
            "{SqueezeNet} Train Epoch: 40 [19456/37814 (51%)]\tLoss: 1.865685\n",
            "{SqueezeNet} Train Epoch: 40 [19968/37814 (53%)]\tLoss: 1.823111\n",
            "{SqueezeNet} Train Epoch: 40 [20480/37814 (54%)]\tLoss: 1.816933\n",
            "{SqueezeNet} Train Epoch: 40 [20992/37814 (55%)]\tLoss: 1.821368\n",
            "{SqueezeNet} Train Epoch: 40 [21504/37814 (57%)]\tLoss: 1.790028\n",
            "{SqueezeNet} Train Epoch: 40 [22016/37814 (58%)]\tLoss: 1.831623\n",
            "{SqueezeNet} Train Epoch: 40 [22528/37814 (59%)]\tLoss: 1.795926\n",
            "{SqueezeNet} Train Epoch: 40 [23040/37814 (61%)]\tLoss: 1.801277\n",
            "{SqueezeNet} Train Epoch: 40 [23552/37814 (62%)]\tLoss: 1.843200\n",
            "{SqueezeNet} Train Epoch: 40 [24064/37814 (64%)]\tLoss: 1.845129\n",
            "{SqueezeNet} Train Epoch: 40 [24576/37814 (65%)]\tLoss: 1.805160\n",
            "{SqueezeNet} Train Epoch: 40 [25088/37814 (66%)]\tLoss: 1.785830\n",
            "{SqueezeNet} Train Epoch: 40 [25600/37814 (68%)]\tLoss: 1.790631\n",
            "{SqueezeNet} Train Epoch: 40 [26112/37814 (69%)]\tLoss: 1.849842\n",
            "{SqueezeNet} Train Epoch: 40 [26624/37814 (70%)]\tLoss: 1.808015\n",
            "{SqueezeNet} Train Epoch: 40 [27136/37814 (72%)]\tLoss: 1.891130\n",
            "{SqueezeNet} Train Epoch: 40 [27648/37814 (73%)]\tLoss: 1.753457\n",
            "{SqueezeNet} Train Epoch: 40 [28160/37814 (74%)]\tLoss: 1.818486\n",
            "{SqueezeNet} Train Epoch: 40 [28672/37814 (76%)]\tLoss: 1.812378\n",
            "{SqueezeNet} Train Epoch: 40 [29184/37814 (77%)]\tLoss: 1.837469\n",
            "{SqueezeNet} Train Epoch: 40 [29696/37814 (78%)]\tLoss: 1.805304\n",
            "{SqueezeNet} Train Epoch: 40 [30208/37814 (80%)]\tLoss: 1.771854\n",
            "{SqueezeNet} Train Epoch: 40 [30720/37814 (81%)]\tLoss: 1.815480\n",
            "{SqueezeNet} Train Epoch: 40 [31232/37814 (82%)]\tLoss: 1.898305\n",
            "{SqueezeNet} Train Epoch: 40 [31744/37814 (84%)]\tLoss: 1.842533\n",
            "{SqueezeNet} Train Epoch: 40 [32256/37814 (85%)]\tLoss: 1.809322\n",
            "{SqueezeNet} Train Epoch: 40 [32768/37814 (86%)]\tLoss: 1.802102\n",
            "{SqueezeNet} Train Epoch: 40 [33280/37814 (88%)]\tLoss: 1.869015\n",
            "{SqueezeNet} Train Epoch: 40 [33792/37814 (89%)]\tLoss: 1.839191\n",
            "{SqueezeNet} Train Epoch: 40 [34304/37814 (91%)]\tLoss: 1.831875\n",
            "{SqueezeNet} Train Epoch: 40 [34816/37814 (92%)]\tLoss: 1.819520\n",
            "{SqueezeNet} Train Epoch: 40 [35328/37814 (93%)]\tLoss: 1.829682\n",
            "{SqueezeNet} Train Epoch: 40 [35840/37814 (95%)]\tLoss: 1.778637\n",
            "{SqueezeNet} Train Epoch: 40 [36352/37814 (96%)]\tLoss: 1.778249\n",
            "{SqueezeNet} Train Epoch: 40 [36864/37814 (97%)]\tLoss: 1.796669\n",
            "{SqueezeNet} Train Epoch: 40 [31974/37814 (99%)]\tLoss: 1.799561\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8161, Accuracy: 1553/5000 (31%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.31506371498108 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca2bf158-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"a8b363d0-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6663d738d9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca2d5156-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_f92cdb20d5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca2da494-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_99fc5a20b2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca2de710-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca2da494-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_75db624ac4"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca542ba0-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca2d5156-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e0048b6612"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca55f35e-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_2b6ea3ae04"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca5645ca-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_ed33bff01a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ca568ae4-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca5645ca-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_abc527194a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 41 [0/37814 (0%)]\tLoss: 1.919917\n",
            "{AlexNet} Train Epoch: 41 [512/37814 (1%)]\tLoss: 1.995627\n",
            "{AlexNet} Train Epoch: 41 [1024/37814 (3%)]\tLoss: 2.030900\n",
            "{AlexNet} Train Epoch: 41 [1536/37814 (4%)]\tLoss: 1.984142\n",
            "{AlexNet} Train Epoch: 41 [2048/37814 (5%)]\tLoss: 1.985259\n",
            "{AlexNet} Train Epoch: 41 [2560/37814 (7%)]\tLoss: 1.953408\n",
            "{AlexNet} Train Epoch: 41 [3072/37814 (8%)]\tLoss: 1.915751\n",
            "{AlexNet} Train Epoch: 41 [3584/37814 (9%)]\tLoss: 1.971023\n",
            "{AlexNet} Train Epoch: 41 [4096/37814 (11%)]\tLoss: 1.925310\n",
            "{AlexNet} Train Epoch: 41 [4608/37814 (12%)]\tLoss: 2.006144\n",
            "{AlexNet} Train Epoch: 41 [5120/37814 (14%)]\tLoss: 1.994486\n",
            "{AlexNet} Train Epoch: 41 [5632/37814 (15%)]\tLoss: 2.008022\n",
            "{AlexNet} Train Epoch: 41 [6144/37814 (16%)]\tLoss: 1.935273\n",
            "{AlexNet} Train Epoch: 41 [6656/37814 (18%)]\tLoss: 1.962377\n",
            "{AlexNet} Train Epoch: 41 [7168/37814 (19%)]\tLoss: 2.021303\n",
            "{AlexNet} Train Epoch: 41 [7680/37814 (20%)]\tLoss: 2.008400\n",
            "{AlexNet} Train Epoch: 41 [8192/37814 (22%)]\tLoss: 1.988415\n",
            "{AlexNet} Train Epoch: 41 [8704/37814 (23%)]\tLoss: 2.008240\n",
            "{AlexNet} Train Epoch: 41 [9216/37814 (24%)]\tLoss: 1.931418\n",
            "{AlexNet} Train Epoch: 41 [9728/37814 (26%)]\tLoss: 1.962697\n",
            "{AlexNet} Train Epoch: 41 [10240/37814 (27%)]\tLoss: 2.010900\n",
            "{AlexNet} Train Epoch: 41 [10752/37814 (28%)]\tLoss: 1.992208\n",
            "{AlexNet} Train Epoch: 41 [11264/37814 (30%)]\tLoss: 1.976572\n",
            "{AlexNet} Train Epoch: 41 [11776/37814 (31%)]\tLoss: 1.949754\n",
            "{AlexNet} Train Epoch: 41 [12288/37814 (32%)]\tLoss: 1.972260\n",
            "{AlexNet} Train Epoch: 41 [12800/37814 (34%)]\tLoss: 1.958302\n",
            "{AlexNet} Train Epoch: 41 [13312/37814 (35%)]\tLoss: 2.048228\n",
            "{AlexNet} Train Epoch: 41 [13824/37814 (36%)]\tLoss: 1.956275\n",
            "{AlexNet} Train Epoch: 41 [14336/37814 (38%)]\tLoss: 1.941751\n",
            "{AlexNet} Train Epoch: 41 [14848/37814 (39%)]\tLoss: 2.019741\n",
            "{AlexNet} Train Epoch: 41 [15360/37814 (41%)]\tLoss: 1.933918\n",
            "{AlexNet} Train Epoch: 41 [15872/37814 (42%)]\tLoss: 1.994819\n",
            "{AlexNet} Train Epoch: 41 [16384/37814 (43%)]\tLoss: 1.941567\n",
            "{AlexNet} Train Epoch: 41 [16896/37814 (45%)]\tLoss: 1.990617\n",
            "{AlexNet} Train Epoch: 41 [17408/37814 (46%)]\tLoss: 1.960576\n",
            "{AlexNet} Train Epoch: 41 [17920/37814 (47%)]\tLoss: 1.927722\n",
            "{AlexNet} Train Epoch: 41 [18432/37814 (49%)]\tLoss: 1.980337\n",
            "{AlexNet} Train Epoch: 41 [18944/37814 (50%)]\tLoss: 2.013461\n",
            "{AlexNet} Train Epoch: 41 [19456/37814 (51%)]\tLoss: 1.943825\n",
            "{AlexNet} Train Epoch: 41 [19968/37814 (53%)]\tLoss: 2.005643\n",
            "{AlexNet} Train Epoch: 41 [20480/37814 (54%)]\tLoss: 2.003474\n",
            "{AlexNet} Train Epoch: 41 [20992/37814 (55%)]\tLoss: 1.994995\n",
            "{AlexNet} Train Epoch: 41 [21504/37814 (57%)]\tLoss: 1.966269\n",
            "{AlexNet} Train Epoch: 41 [22016/37814 (58%)]\tLoss: 1.962266\n",
            "{AlexNet} Train Epoch: 41 [22528/37814 (59%)]\tLoss: 1.946557\n",
            "{AlexNet} Train Epoch: 41 [23040/37814 (61%)]\tLoss: 1.970354\n",
            "{AlexNet} Train Epoch: 41 [23552/37814 (62%)]\tLoss: 1.941347\n",
            "{AlexNet} Train Epoch: 41 [24064/37814 (64%)]\tLoss: 1.995784\n",
            "{AlexNet} Train Epoch: 41 [24576/37814 (65%)]\tLoss: 1.978590\n",
            "{AlexNet} Train Epoch: 41 [25088/37814 (66%)]\tLoss: 1.981866\n",
            "{AlexNet} Train Epoch: 41 [25600/37814 (68%)]\tLoss: 1.982808\n",
            "{AlexNet} Train Epoch: 41 [26112/37814 (69%)]\tLoss: 1.956259\n",
            "{AlexNet} Train Epoch: 41 [26624/37814 (70%)]\tLoss: 1.995064\n",
            "{AlexNet} Train Epoch: 41 [27136/37814 (72%)]\tLoss: 1.962965\n",
            "{AlexNet} Train Epoch: 41 [27648/37814 (73%)]\tLoss: 1.953358\n",
            "{AlexNet} Train Epoch: 41 [28160/37814 (74%)]\tLoss: 1.978488\n",
            "{AlexNet} Train Epoch: 41 [28672/37814 (76%)]\tLoss: 1.947246\n",
            "{AlexNet} Train Epoch: 41 [29184/37814 (77%)]\tLoss: 1.979344\n",
            "{AlexNet} Train Epoch: 41 [29696/37814 (78%)]\tLoss: 1.938482\n",
            "{AlexNet} Train Epoch: 41 [30208/37814 (80%)]\tLoss: 1.982446\n",
            "{AlexNet} Train Epoch: 41 [30720/37814 (81%)]\tLoss: 1.989999\n",
            "{AlexNet} Train Epoch: 41 [31232/37814 (82%)]\tLoss: 1.984586\n",
            "{AlexNet} Train Epoch: 41 [31744/37814 (84%)]\tLoss: 1.963830\n",
            "{AlexNet} Train Epoch: 41 [32256/37814 (85%)]\tLoss: 1.955083\n",
            "{AlexNet} Train Epoch: 41 [32768/37814 (86%)]\tLoss: 1.947786\n",
            "{AlexNet} Train Epoch: 41 [33280/37814 (88%)]\tLoss: 1.996676\n",
            "{AlexNet} Train Epoch: 41 [33792/37814 (89%)]\tLoss: 1.976695\n",
            "{AlexNet} Train Epoch: 41 [34304/37814 (91%)]\tLoss: 1.969450\n",
            "{AlexNet} Train Epoch: 41 [34816/37814 (92%)]\tLoss: 2.057779\n",
            "{AlexNet} Train Epoch: 41 [35328/37814 (93%)]\tLoss: 1.916105\n",
            "{AlexNet} Train Epoch: 41 [35840/37814 (95%)]\tLoss: 2.002857\n",
            "{AlexNet} Train Epoch: 41 [36352/37814 (96%)]\tLoss: 1.953605\n",
            "{AlexNet} Train Epoch: 41 [36864/37814 (97%)]\tLoss: 1.991407\n",
            "{AlexNet} Train Epoch: 41 [31974/37814 (99%)]\tLoss: 1.912504\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9785, Accuracy: 1024/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.722820281982422 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 41 [0/37814 (0%)]\tLoss: 1.804157\n",
            "{SqueezeNet} Train Epoch: 41 [512/37814 (1%)]\tLoss: 1.876124\n",
            "{SqueezeNet} Train Epoch: 41 [1024/37814 (3%)]\tLoss: 1.839178\n",
            "{SqueezeNet} Train Epoch: 41 [1536/37814 (4%)]\tLoss: 1.913039\n",
            "{SqueezeNet} Train Epoch: 41 [2048/37814 (5%)]\tLoss: 1.862837\n",
            "{SqueezeNet} Train Epoch: 41 [2560/37814 (7%)]\tLoss: 1.860506\n",
            "{SqueezeNet} Train Epoch: 41 [3072/37814 (8%)]\tLoss: 1.812156\n",
            "{SqueezeNet} Train Epoch: 41 [3584/37814 (9%)]\tLoss: 1.784881\n",
            "{SqueezeNet} Train Epoch: 41 [4096/37814 (11%)]\tLoss: 1.852745\n",
            "{SqueezeNet} Train Epoch: 41 [4608/37814 (12%)]\tLoss: 1.817991\n",
            "{SqueezeNet} Train Epoch: 41 [5120/37814 (14%)]\tLoss: 1.835812\n",
            "{SqueezeNet} Train Epoch: 41 [5632/37814 (15%)]\tLoss: 1.785120\n",
            "{SqueezeNet} Train Epoch: 41 [6144/37814 (16%)]\tLoss: 1.843539\n",
            "{SqueezeNet} Train Epoch: 41 [6656/37814 (18%)]\tLoss: 1.805389\n",
            "{SqueezeNet} Train Epoch: 41 [7168/37814 (19%)]\tLoss: 1.855711\n",
            "{SqueezeNet} Train Epoch: 41 [7680/37814 (20%)]\tLoss: 1.823247\n",
            "{SqueezeNet} Train Epoch: 41 [8192/37814 (22%)]\tLoss: 1.763209\n",
            "{SqueezeNet} Train Epoch: 41 [8704/37814 (23%)]\tLoss: 1.842475\n",
            "{SqueezeNet} Train Epoch: 41 [9216/37814 (24%)]\tLoss: 1.810665\n",
            "{SqueezeNet} Train Epoch: 41 [9728/37814 (26%)]\tLoss: 1.830017\n",
            "{SqueezeNet} Train Epoch: 41 [10240/37814 (27%)]\tLoss: 1.797161\n",
            "{SqueezeNet} Train Epoch: 41 [10752/37814 (28%)]\tLoss: 1.797531\n",
            "{SqueezeNet} Train Epoch: 41 [11264/37814 (30%)]\tLoss: 1.879506\n",
            "{SqueezeNet} Train Epoch: 41 [11776/37814 (31%)]\tLoss: 1.852263\n",
            "{SqueezeNet} Train Epoch: 41 [12288/37814 (32%)]\tLoss: 1.852341\n",
            "{SqueezeNet} Train Epoch: 41 [12800/37814 (34%)]\tLoss: 1.854176\n",
            "{SqueezeNet} Train Epoch: 41 [13312/37814 (35%)]\tLoss: 1.828340\n",
            "{SqueezeNet} Train Epoch: 41 [13824/37814 (36%)]\tLoss: 1.769384\n",
            "{SqueezeNet} Train Epoch: 41 [14336/37814 (38%)]\tLoss: 1.796719\n",
            "{SqueezeNet} Train Epoch: 41 [14848/37814 (39%)]\tLoss: 1.825603\n",
            "{SqueezeNet} Train Epoch: 41 [15360/37814 (41%)]\tLoss: 1.869915\n",
            "{SqueezeNet} Train Epoch: 41 [15872/37814 (42%)]\tLoss: 1.865957\n",
            "{SqueezeNet} Train Epoch: 41 [16384/37814 (43%)]\tLoss: 1.838801\n",
            "{SqueezeNet} Train Epoch: 41 [16896/37814 (45%)]\tLoss: 1.831380\n",
            "{SqueezeNet} Train Epoch: 41 [17408/37814 (46%)]\tLoss: 1.879434\n",
            "{SqueezeNet} Train Epoch: 41 [17920/37814 (47%)]\tLoss: 1.839244\n",
            "{SqueezeNet} Train Epoch: 41 [18432/37814 (49%)]\tLoss: 1.830793\n",
            "{SqueezeNet} Train Epoch: 41 [18944/37814 (50%)]\tLoss: 1.810583\n",
            "{SqueezeNet} Train Epoch: 41 [19456/37814 (51%)]\tLoss: 1.786829\n",
            "{SqueezeNet} Train Epoch: 41 [19968/37814 (53%)]\tLoss: 1.851667\n",
            "{SqueezeNet} Train Epoch: 41 [20480/37814 (54%)]\tLoss: 1.827991\n",
            "{SqueezeNet} Train Epoch: 41 [20992/37814 (55%)]\tLoss: 1.811542\n",
            "{SqueezeNet} Train Epoch: 41 [21504/37814 (57%)]\tLoss: 1.830576\n",
            "{SqueezeNet} Train Epoch: 41 [22016/37814 (58%)]\tLoss: 1.739560\n",
            "{SqueezeNet} Train Epoch: 41 [22528/37814 (59%)]\tLoss: 1.737771\n",
            "{SqueezeNet} Train Epoch: 41 [23040/37814 (61%)]\tLoss: 1.837315\n",
            "{SqueezeNet} Train Epoch: 41 [23552/37814 (62%)]\tLoss: 1.788069\n",
            "{SqueezeNet} Train Epoch: 41 [24064/37814 (64%)]\tLoss: 1.860956\n",
            "{SqueezeNet} Train Epoch: 41 [24576/37814 (65%)]\tLoss: 1.768062\n",
            "{SqueezeNet} Train Epoch: 41 [25088/37814 (66%)]\tLoss: 1.792166\n",
            "{SqueezeNet} Train Epoch: 41 [25600/37814 (68%)]\tLoss: 1.838642\n",
            "{SqueezeNet} Train Epoch: 41 [26112/37814 (69%)]\tLoss: 1.829752\n",
            "{SqueezeNet} Train Epoch: 41 [26624/37814 (70%)]\tLoss: 1.837426\n",
            "{SqueezeNet} Train Epoch: 41 [27136/37814 (72%)]\tLoss: 1.863153\n",
            "{SqueezeNet} Train Epoch: 41 [27648/37814 (73%)]\tLoss: 1.817055\n",
            "{SqueezeNet} Train Epoch: 41 [28160/37814 (74%)]\tLoss: 1.790971\n",
            "{SqueezeNet} Train Epoch: 41 [28672/37814 (76%)]\tLoss: 1.803247\n",
            "{SqueezeNet} Train Epoch: 41 [29184/37814 (77%)]\tLoss: 1.758657\n",
            "{SqueezeNet} Train Epoch: 41 [29696/37814 (78%)]\tLoss: 1.863580\n",
            "{SqueezeNet} Train Epoch: 41 [30208/37814 (80%)]\tLoss: 1.797897\n",
            "{SqueezeNet} Train Epoch: 41 [30720/37814 (81%)]\tLoss: 1.817083\n",
            "{SqueezeNet} Train Epoch: 41 [31232/37814 (82%)]\tLoss: 1.906440\n",
            "{SqueezeNet} Train Epoch: 41 [31744/37814 (84%)]\tLoss: 1.861086\n",
            "{SqueezeNet} Train Epoch: 41 [32256/37814 (85%)]\tLoss: 1.808064\n",
            "{SqueezeNet} Train Epoch: 41 [32768/37814 (86%)]\tLoss: 1.824929\n",
            "{SqueezeNet} Train Epoch: 41 [33280/37814 (88%)]\tLoss: 1.821182\n",
            "{SqueezeNet} Train Epoch: 41 [33792/37814 (89%)]\tLoss: 1.762469\n",
            "{SqueezeNet} Train Epoch: 41 [34304/37814 (91%)]\tLoss: 1.813990\n",
            "{SqueezeNet} Train Epoch: 41 [34816/37814 (92%)]\tLoss: 1.832141\n",
            "{SqueezeNet} Train Epoch: 41 [35328/37814 (93%)]\tLoss: 1.814082\n",
            "{SqueezeNet} Train Epoch: 41 [35840/37814 (95%)]\tLoss: 1.806726\n",
            "{SqueezeNet} Train Epoch: 41 [36352/37814 (96%)]\tLoss: 1.780724\n",
            "{SqueezeNet} Train Epoch: 41 [36864/37814 (97%)]\tLoss: 1.797024\n",
            "{SqueezeNet} Train Epoch: 41 [31974/37814 (99%)]\tLoss: 1.827983\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8132, Accuracy: 1612/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 30.059179067611694 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec306cd4-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ca55f35e-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f1988c11a1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec3318da-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_12d0625851"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec334e36-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_248b7088a5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec33b0f6-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ec334e36-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_fbbf60d55c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec6391fe-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ec3318da-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_67b5d37eb5"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec65520a-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_3e4b07229f"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec658f68-60e7-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_aaded303f2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"ec65c96a-60e7-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ec658f68-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c6ff05562e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 42 [0/37814 (0%)]\tLoss: 1.957486\n",
            "{AlexNet} Train Epoch: 42 [512/37814 (1%)]\tLoss: 1.930573\n",
            "{AlexNet} Train Epoch: 42 [1024/37814 (3%)]\tLoss: 1.955262\n",
            "{AlexNet} Train Epoch: 42 [1536/37814 (4%)]\tLoss: 1.980121\n",
            "{AlexNet} Train Epoch: 42 [2048/37814 (5%)]\tLoss: 1.981261\n",
            "{AlexNet} Train Epoch: 42 [2560/37814 (7%)]\tLoss: 2.001094\n",
            "{AlexNet} Train Epoch: 42 [3072/37814 (8%)]\tLoss: 2.006196\n",
            "{AlexNet} Train Epoch: 42 [3584/37814 (9%)]\tLoss: 1.997173\n",
            "{AlexNet} Train Epoch: 42 [4096/37814 (11%)]\tLoss: 1.966401\n",
            "{AlexNet} Train Epoch: 42 [4608/37814 (12%)]\tLoss: 1.928062\n",
            "{AlexNet} Train Epoch: 42 [5120/37814 (14%)]\tLoss: 1.962324\n",
            "{AlexNet} Train Epoch: 42 [5632/37814 (15%)]\tLoss: 1.940447\n",
            "{AlexNet} Train Epoch: 42 [6144/37814 (16%)]\tLoss: 1.930727\n",
            "{AlexNet} Train Epoch: 42 [6656/37814 (18%)]\tLoss: 1.970221\n",
            "{AlexNet} Train Epoch: 42 [7168/37814 (19%)]\tLoss: 1.973979\n",
            "{AlexNet} Train Epoch: 42 [7680/37814 (20%)]\tLoss: 1.969236\n",
            "{AlexNet} Train Epoch: 42 [8192/37814 (22%)]\tLoss: 1.961039\n",
            "{AlexNet} Train Epoch: 42 [8704/37814 (23%)]\tLoss: 1.940725\n",
            "{AlexNet} Train Epoch: 42 [9216/37814 (24%)]\tLoss: 1.961105\n",
            "{AlexNet} Train Epoch: 42 [9728/37814 (26%)]\tLoss: 2.001979\n",
            "{AlexNet} Train Epoch: 42 [10240/37814 (27%)]\tLoss: 1.973629\n",
            "{AlexNet} Train Epoch: 42 [10752/37814 (28%)]\tLoss: 1.983701\n",
            "{AlexNet} Train Epoch: 42 [11264/37814 (30%)]\tLoss: 1.930651\n",
            "{AlexNet} Train Epoch: 42 [11776/37814 (31%)]\tLoss: 1.939798\n",
            "{AlexNet} Train Epoch: 42 [12288/37814 (32%)]\tLoss: 1.970244\n",
            "{AlexNet} Train Epoch: 42 [12800/37814 (34%)]\tLoss: 1.967925\n",
            "{AlexNet} Train Epoch: 42 [13312/37814 (35%)]\tLoss: 1.950990\n",
            "{AlexNet} Train Epoch: 42 [13824/37814 (36%)]\tLoss: 1.955032\n",
            "{AlexNet} Train Epoch: 42 [14336/37814 (38%)]\tLoss: 1.965879\n",
            "{AlexNet} Train Epoch: 42 [14848/37814 (39%)]\tLoss: 1.957631\n",
            "{AlexNet} Train Epoch: 42 [15360/37814 (41%)]\tLoss: 1.969673\n",
            "{AlexNet} Train Epoch: 42 [15872/37814 (42%)]\tLoss: 1.977093\n",
            "{AlexNet} Train Epoch: 42 [16384/37814 (43%)]\tLoss: 1.970741\n",
            "{AlexNet} Train Epoch: 42 [16896/37814 (45%)]\tLoss: 2.030953\n",
            "{AlexNet} Train Epoch: 42 [17408/37814 (46%)]\tLoss: 2.007110\n",
            "{AlexNet} Train Epoch: 42 [17920/37814 (47%)]\tLoss: 2.015192\n",
            "{AlexNet} Train Epoch: 42 [18432/37814 (49%)]\tLoss: 1.931742\n",
            "{AlexNet} Train Epoch: 42 [18944/37814 (50%)]\tLoss: 1.995479\n",
            "{AlexNet} Train Epoch: 42 [19456/37814 (51%)]\tLoss: 1.952497\n",
            "{AlexNet} Train Epoch: 42 [19968/37814 (53%)]\tLoss: 1.961810\n",
            "{AlexNet} Train Epoch: 42 [20480/37814 (54%)]\tLoss: 2.024628\n",
            "{AlexNet} Train Epoch: 42 [20992/37814 (55%)]\tLoss: 2.004905\n",
            "{AlexNet} Train Epoch: 42 [21504/37814 (57%)]\tLoss: 1.939265\n",
            "{AlexNet} Train Epoch: 42 [22016/37814 (58%)]\tLoss: 1.990261\n",
            "{AlexNet} Train Epoch: 42 [22528/37814 (59%)]\tLoss: 1.937108\n",
            "{AlexNet} Train Epoch: 42 [23040/37814 (61%)]\tLoss: 1.963737\n",
            "{AlexNet} Train Epoch: 42 [23552/37814 (62%)]\tLoss: 1.983147\n",
            "{AlexNet} Train Epoch: 42 [24064/37814 (64%)]\tLoss: 1.958524\n",
            "{AlexNet} Train Epoch: 42 [24576/37814 (65%)]\tLoss: 1.941297\n",
            "{AlexNet} Train Epoch: 42 [25088/37814 (66%)]\tLoss: 1.986261\n",
            "{AlexNet} Train Epoch: 42 [25600/37814 (68%)]\tLoss: 2.006242\n",
            "{AlexNet} Train Epoch: 42 [26112/37814 (69%)]\tLoss: 1.974458\n",
            "{AlexNet} Train Epoch: 42 [26624/37814 (70%)]\tLoss: 1.973760\n",
            "{AlexNet} Train Epoch: 42 [27136/37814 (72%)]\tLoss: 2.010249\n",
            "{AlexNet} Train Epoch: 42 [27648/37814 (73%)]\tLoss: 1.983733\n",
            "{AlexNet} Train Epoch: 42 [28160/37814 (74%)]\tLoss: 1.937248\n",
            "{AlexNet} Train Epoch: 42 [28672/37814 (76%)]\tLoss: 1.978721\n",
            "{AlexNet} Train Epoch: 42 [29184/37814 (77%)]\tLoss: 1.986583\n",
            "{AlexNet} Train Epoch: 42 [29696/37814 (78%)]\tLoss: 1.997535\n",
            "{AlexNet} Train Epoch: 42 [30208/37814 (80%)]\tLoss: 1.916019\n",
            "{AlexNet} Train Epoch: 42 [30720/37814 (81%)]\tLoss: 1.966631\n",
            "{AlexNet} Train Epoch: 42 [31232/37814 (82%)]\tLoss: 1.957111\n",
            "{AlexNet} Train Epoch: 42 [31744/37814 (84%)]\tLoss: 1.976489\n",
            "{AlexNet} Train Epoch: 42 [32256/37814 (85%)]\tLoss: 1.969034\n",
            "{AlexNet} Train Epoch: 42 [32768/37814 (86%)]\tLoss: 2.015957\n",
            "{AlexNet} Train Epoch: 42 [33280/37814 (88%)]\tLoss: 1.998595\n",
            "{AlexNet} Train Epoch: 42 [33792/37814 (89%)]\tLoss: 1.998110\n",
            "{AlexNet} Train Epoch: 42 [34304/37814 (91%)]\tLoss: 1.951073\n",
            "{AlexNet} Train Epoch: 42 [34816/37814 (92%)]\tLoss: 1.950896\n",
            "{AlexNet} Train Epoch: 42 [35328/37814 (93%)]\tLoss: 2.001189\n",
            "{AlexNet} Train Epoch: 42 [35840/37814 (95%)]\tLoss: 1.949015\n",
            "{AlexNet} Train Epoch: 42 [36352/37814 (96%)]\tLoss: 1.964233\n",
            "{AlexNet} Train Epoch: 42 [36864/37814 (97%)]\tLoss: 1.978839\n",
            "{AlexNet} Train Epoch: 42 [31974/37814 (99%)]\tLoss: 1.950394\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9670, Accuracy: 1075/5000 (22%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.24321961402893 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 42 [0/37814 (0%)]\tLoss: 1.835601\n",
            "{SqueezeNet} Train Epoch: 42 [512/37814 (1%)]\tLoss: 1.888976\n",
            "{SqueezeNet} Train Epoch: 42 [1024/37814 (3%)]\tLoss: 1.868201\n",
            "{SqueezeNet} Train Epoch: 42 [1536/37814 (4%)]\tLoss: 1.791728\n",
            "{SqueezeNet} Train Epoch: 42 [2048/37814 (5%)]\tLoss: 1.875463\n",
            "{SqueezeNet} Train Epoch: 42 [2560/37814 (7%)]\tLoss: 1.773025\n",
            "{SqueezeNet} Train Epoch: 42 [3072/37814 (8%)]\tLoss: 1.830681\n",
            "{SqueezeNet} Train Epoch: 42 [3584/37814 (9%)]\tLoss: 1.858197\n",
            "{SqueezeNet} Train Epoch: 42 [4096/37814 (11%)]\tLoss: 1.765439\n",
            "{SqueezeNet} Train Epoch: 42 [4608/37814 (12%)]\tLoss: 1.853847\n",
            "{SqueezeNet} Train Epoch: 42 [5120/37814 (14%)]\tLoss: 1.818671\n",
            "{SqueezeNet} Train Epoch: 42 [5632/37814 (15%)]\tLoss: 1.770179\n",
            "{SqueezeNet} Train Epoch: 42 [6144/37814 (16%)]\tLoss: 1.860966\n",
            "{SqueezeNet} Train Epoch: 42 [6656/37814 (18%)]\tLoss: 1.829586\n",
            "{SqueezeNet} Train Epoch: 42 [7168/37814 (19%)]\tLoss: 1.893493\n",
            "{SqueezeNet} Train Epoch: 42 [7680/37814 (20%)]\tLoss: 1.832573\n",
            "{SqueezeNet} Train Epoch: 42 [8192/37814 (22%)]\tLoss: 1.786923\n",
            "{SqueezeNet} Train Epoch: 42 [8704/37814 (23%)]\tLoss: 1.883207\n",
            "{SqueezeNet} Train Epoch: 42 [9216/37814 (24%)]\tLoss: 1.785937\n",
            "{SqueezeNet} Train Epoch: 42 [9728/37814 (26%)]\tLoss: 1.811733\n",
            "{SqueezeNet} Train Epoch: 42 [10240/37814 (27%)]\tLoss: 1.825326\n",
            "{SqueezeNet} Train Epoch: 42 [10752/37814 (28%)]\tLoss: 1.816085\n",
            "{SqueezeNet} Train Epoch: 42 [11264/37814 (30%)]\tLoss: 1.834689\n",
            "{SqueezeNet} Train Epoch: 42 [11776/37814 (31%)]\tLoss: 1.824451\n",
            "{SqueezeNet} Train Epoch: 42 [12288/37814 (32%)]\tLoss: 1.778163\n",
            "{SqueezeNet} Train Epoch: 42 [12800/37814 (34%)]\tLoss: 1.783213\n",
            "{SqueezeNet} Train Epoch: 42 [13312/37814 (35%)]\tLoss: 1.872374\n",
            "{SqueezeNet} Train Epoch: 42 [13824/37814 (36%)]\tLoss: 1.807994\n",
            "{SqueezeNet} Train Epoch: 42 [14336/37814 (38%)]\tLoss: 1.797655\n",
            "{SqueezeNet} Train Epoch: 42 [14848/37814 (39%)]\tLoss: 1.827347\n",
            "{SqueezeNet} Train Epoch: 42 [15360/37814 (41%)]\tLoss: 1.815265\n",
            "{SqueezeNet} Train Epoch: 42 [15872/37814 (42%)]\tLoss: 1.811844\n",
            "{SqueezeNet} Train Epoch: 42 [16384/37814 (43%)]\tLoss: 1.824147\n",
            "{SqueezeNet} Train Epoch: 42 [16896/37814 (45%)]\tLoss: 1.844344\n",
            "{SqueezeNet} Train Epoch: 42 [17408/37814 (46%)]\tLoss: 1.816128\n",
            "{SqueezeNet} Train Epoch: 42 [17920/37814 (47%)]\tLoss: 1.850346\n",
            "{SqueezeNet} Train Epoch: 42 [18432/37814 (49%)]\tLoss: 1.861657\n",
            "{SqueezeNet} Train Epoch: 42 [18944/37814 (50%)]\tLoss: 1.904092\n",
            "{SqueezeNet} Train Epoch: 42 [19456/37814 (51%)]\tLoss: 1.869393\n",
            "{SqueezeNet} Train Epoch: 42 [19968/37814 (53%)]\tLoss: 1.788698\n",
            "{SqueezeNet} Train Epoch: 42 [20480/37814 (54%)]\tLoss: 1.804209\n",
            "{SqueezeNet} Train Epoch: 42 [20992/37814 (55%)]\tLoss: 1.822262\n",
            "{SqueezeNet} Train Epoch: 42 [21504/37814 (57%)]\tLoss: 1.776693\n",
            "{SqueezeNet} Train Epoch: 42 [22016/37814 (58%)]\tLoss: 1.814389\n",
            "{SqueezeNet} Train Epoch: 42 [22528/37814 (59%)]\tLoss: 1.790410\n",
            "{SqueezeNet} Train Epoch: 42 [23040/37814 (61%)]\tLoss: 1.834357\n",
            "{SqueezeNet} Train Epoch: 42 [23552/37814 (62%)]\tLoss: 1.807594\n",
            "{SqueezeNet} Train Epoch: 42 [24064/37814 (64%)]\tLoss: 1.836414\n",
            "{SqueezeNet} Train Epoch: 42 [24576/37814 (65%)]\tLoss: 1.850456\n",
            "{SqueezeNet} Train Epoch: 42 [25088/37814 (66%)]\tLoss: 1.830206\n",
            "{SqueezeNet} Train Epoch: 42 [25600/37814 (68%)]\tLoss: 1.809996\n",
            "{SqueezeNet} Train Epoch: 42 [26112/37814 (69%)]\tLoss: 1.790988\n",
            "{SqueezeNet} Train Epoch: 42 [26624/37814 (70%)]\tLoss: 1.824210\n",
            "{SqueezeNet} Train Epoch: 42 [27136/37814 (72%)]\tLoss: 1.794283\n",
            "{SqueezeNet} Train Epoch: 42 [27648/37814 (73%)]\tLoss: 1.798536\n",
            "{SqueezeNet} Train Epoch: 42 [28160/37814 (74%)]\tLoss: 1.790830\n",
            "{SqueezeNet} Train Epoch: 42 [28672/37814 (76%)]\tLoss: 1.782171\n",
            "{SqueezeNet} Train Epoch: 42 [29184/37814 (77%)]\tLoss: 1.774828\n",
            "{SqueezeNet} Train Epoch: 42 [29696/37814 (78%)]\tLoss: 1.822822\n",
            "{SqueezeNet} Train Epoch: 42 [30208/37814 (80%)]\tLoss: 1.784603\n",
            "{SqueezeNet} Train Epoch: 42 [30720/37814 (81%)]\tLoss: 1.873031\n",
            "{SqueezeNet} Train Epoch: 42 [31232/37814 (82%)]\tLoss: 1.844832\n",
            "{SqueezeNet} Train Epoch: 42 [31744/37814 (84%)]\tLoss: 1.909154\n",
            "{SqueezeNet} Train Epoch: 42 [32256/37814 (85%)]\tLoss: 1.805391\n",
            "{SqueezeNet} Train Epoch: 42 [32768/37814 (86%)]\tLoss: 1.878211\n",
            "{SqueezeNet} Train Epoch: 42 [33280/37814 (88%)]\tLoss: 1.753008\n",
            "{SqueezeNet} Train Epoch: 42 [33792/37814 (89%)]\tLoss: 1.808987\n",
            "{SqueezeNet} Train Epoch: 42 [34304/37814 (91%)]\tLoss: 1.801044\n",
            "{SqueezeNet} Train Epoch: 42 [34816/37814 (92%)]\tLoss: 1.854248\n",
            "{SqueezeNet} Train Epoch: 42 [35328/37814 (93%)]\tLoss: 1.794472\n",
            "{SqueezeNet} Train Epoch: 42 [35840/37814 (95%)]\tLoss: 1.826077\n",
            "{SqueezeNet} Train Epoch: 42 [36352/37814 (96%)]\tLoss: 1.827818\n",
            "{SqueezeNet} Train Epoch: 42 [36864/37814 (97%)]\tLoss: 1.864596\n",
            "{SqueezeNet} Train Epoch: 42 [31974/37814 (99%)]\tLoss: 1.930340\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8035, Accuracy: 1602/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.4017653465271 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0d91e196-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"ec65520a-60e7-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_82e9828885"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0d934be4-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_482fa51d9d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0d939c16-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_acbd8575bf"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0d93e69e-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0d939c16-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_812ae3398c"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0dbea19a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0d934be4-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b511b9f316"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0dc1020a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_44c7869105"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0dc15a48-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_9e902f0e84"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"0dc1bc9a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0dc15a48-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8c643c0ee8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 43 [0/37814 (0%)]\tLoss: 1.960842\n",
            "{AlexNet} Train Epoch: 43 [512/37814 (1%)]\tLoss: 1.970756\n",
            "{AlexNet} Train Epoch: 43 [1024/37814 (3%)]\tLoss: 1.953605\n",
            "{AlexNet} Train Epoch: 43 [1536/37814 (4%)]\tLoss: 1.987946\n",
            "{AlexNet} Train Epoch: 43 [2048/37814 (5%)]\tLoss: 1.997800\n",
            "{AlexNet} Train Epoch: 43 [2560/37814 (7%)]\tLoss: 1.954756\n",
            "{AlexNet} Train Epoch: 43 [3072/37814 (8%)]\tLoss: 1.993239\n",
            "{AlexNet} Train Epoch: 43 [3584/37814 (9%)]\tLoss: 1.951815\n",
            "{AlexNet} Train Epoch: 43 [4096/37814 (11%)]\tLoss: 1.965040\n",
            "{AlexNet} Train Epoch: 43 [4608/37814 (12%)]\tLoss: 1.936365\n",
            "{AlexNet} Train Epoch: 43 [5120/37814 (14%)]\tLoss: 1.982348\n",
            "{AlexNet} Train Epoch: 43 [5632/37814 (15%)]\tLoss: 1.985963\n",
            "{AlexNet} Train Epoch: 43 [6144/37814 (16%)]\tLoss: 1.939482\n",
            "{AlexNet} Train Epoch: 43 [6656/37814 (18%)]\tLoss: 1.938929\n",
            "{AlexNet} Train Epoch: 43 [7168/37814 (19%)]\tLoss: 1.976602\n",
            "{AlexNet} Train Epoch: 43 [7680/37814 (20%)]\tLoss: 1.988096\n",
            "{AlexNet} Train Epoch: 43 [8192/37814 (22%)]\tLoss: 1.950272\n",
            "{AlexNet} Train Epoch: 43 [8704/37814 (23%)]\tLoss: 1.971630\n",
            "{AlexNet} Train Epoch: 43 [9216/37814 (24%)]\tLoss: 1.973932\n",
            "{AlexNet} Train Epoch: 43 [9728/37814 (26%)]\tLoss: 1.967698\n",
            "{AlexNet} Train Epoch: 43 [10240/37814 (27%)]\tLoss: 1.946083\n",
            "{AlexNet} Train Epoch: 43 [10752/37814 (28%)]\tLoss: 2.005416\n",
            "{AlexNet} Train Epoch: 43 [11264/37814 (30%)]\tLoss: 1.968185\n",
            "{AlexNet} Train Epoch: 43 [11776/37814 (31%)]\tLoss: 1.991168\n",
            "{AlexNet} Train Epoch: 43 [12288/37814 (32%)]\tLoss: 1.988978\n",
            "{AlexNet} Train Epoch: 43 [12800/37814 (34%)]\tLoss: 1.987073\n",
            "{AlexNet} Train Epoch: 43 [13312/37814 (35%)]\tLoss: 1.956301\n",
            "{AlexNet} Train Epoch: 43 [13824/37814 (36%)]\tLoss: 1.994881\n",
            "{AlexNet} Train Epoch: 43 [14336/37814 (38%)]\tLoss: 1.931073\n",
            "{AlexNet} Train Epoch: 43 [14848/37814 (39%)]\tLoss: 1.959984\n",
            "{AlexNet} Train Epoch: 43 [15360/37814 (41%)]\tLoss: 1.987921\n",
            "{AlexNet} Train Epoch: 43 [15872/37814 (42%)]\tLoss: 2.012044\n",
            "{AlexNet} Train Epoch: 43 [16384/37814 (43%)]\tLoss: 1.950517\n",
            "{AlexNet} Train Epoch: 43 [16896/37814 (45%)]\tLoss: 1.955744\n",
            "{AlexNet} Train Epoch: 43 [17408/37814 (46%)]\tLoss: 1.941364\n",
            "{AlexNet} Train Epoch: 43 [17920/37814 (47%)]\tLoss: 1.963775\n",
            "{AlexNet} Train Epoch: 43 [18432/37814 (49%)]\tLoss: 2.026667\n",
            "{AlexNet} Train Epoch: 43 [18944/37814 (50%)]\tLoss: 1.959294\n",
            "{AlexNet} Train Epoch: 43 [19456/37814 (51%)]\tLoss: 2.050410\n",
            "{AlexNet} Train Epoch: 43 [19968/37814 (53%)]\tLoss: 1.969061\n",
            "{AlexNet} Train Epoch: 43 [20480/37814 (54%)]\tLoss: 1.971886\n",
            "{AlexNet} Train Epoch: 43 [20992/37814 (55%)]\tLoss: 1.980410\n",
            "{AlexNet} Train Epoch: 43 [21504/37814 (57%)]\tLoss: 1.998241\n",
            "{AlexNet} Train Epoch: 43 [22016/37814 (58%)]\tLoss: 1.978943\n",
            "{AlexNet} Train Epoch: 43 [22528/37814 (59%)]\tLoss: 1.989996\n",
            "{AlexNet} Train Epoch: 43 [23040/37814 (61%)]\tLoss: 1.985397\n",
            "{AlexNet} Train Epoch: 43 [23552/37814 (62%)]\tLoss: 1.983577\n",
            "{AlexNet} Train Epoch: 43 [24064/37814 (64%)]\tLoss: 1.920573\n",
            "{AlexNet} Train Epoch: 43 [24576/37814 (65%)]\tLoss: 2.000987\n",
            "{AlexNet} Train Epoch: 43 [25088/37814 (66%)]\tLoss: 1.967069\n",
            "{AlexNet} Train Epoch: 43 [25600/37814 (68%)]\tLoss: 1.977226\n",
            "{AlexNet} Train Epoch: 43 [26112/37814 (69%)]\tLoss: 1.960814\n",
            "{AlexNet} Train Epoch: 43 [26624/37814 (70%)]\tLoss: 1.982690\n",
            "{AlexNet} Train Epoch: 43 [27136/37814 (72%)]\tLoss: 1.997980\n",
            "{AlexNet} Train Epoch: 43 [27648/37814 (73%)]\tLoss: 1.971599\n",
            "{AlexNet} Train Epoch: 43 [28160/37814 (74%)]\tLoss: 1.965143\n",
            "{AlexNet} Train Epoch: 43 [28672/37814 (76%)]\tLoss: 1.986326\n",
            "{AlexNet} Train Epoch: 43 [29184/37814 (77%)]\tLoss: 1.971971\n",
            "{AlexNet} Train Epoch: 43 [29696/37814 (78%)]\tLoss: 1.949079\n",
            "{AlexNet} Train Epoch: 43 [30208/37814 (80%)]\tLoss: 2.027228\n",
            "{AlexNet} Train Epoch: 43 [30720/37814 (81%)]\tLoss: 1.950058\n",
            "{AlexNet} Train Epoch: 43 [31232/37814 (82%)]\tLoss: 1.968302\n",
            "{AlexNet} Train Epoch: 43 [31744/37814 (84%)]\tLoss: 1.991812\n",
            "{AlexNet} Train Epoch: 43 [32256/37814 (85%)]\tLoss: 2.025929\n",
            "{AlexNet} Train Epoch: 43 [32768/37814 (86%)]\tLoss: 1.988533\n",
            "{AlexNet} Train Epoch: 43 [33280/37814 (88%)]\tLoss: 2.007981\n",
            "{AlexNet} Train Epoch: 43 [33792/37814 (89%)]\tLoss: 1.937132\n",
            "{AlexNet} Train Epoch: 43 [34304/37814 (91%)]\tLoss: 1.978607\n",
            "{AlexNet} Train Epoch: 43 [34816/37814 (92%)]\tLoss: 1.990978\n",
            "{AlexNet} Train Epoch: 43 [35328/37814 (93%)]\tLoss: 1.991412\n",
            "{AlexNet} Train Epoch: 43 [35840/37814 (95%)]\tLoss: 1.973096\n",
            "{AlexNet} Train Epoch: 43 [36352/37814 (96%)]\tLoss: 1.995481\n",
            "{AlexNet} Train Epoch: 43 [36864/37814 (97%)]\tLoss: 1.996750\n",
            "{AlexNet} Train Epoch: 43 [31974/37814 (99%)]\tLoss: 1.942633\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9751, Accuracy: 1019/5000 (20%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.7386314868927 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 43 [0/37814 (0%)]\tLoss: 1.761260\n",
            "{SqueezeNet} Train Epoch: 43 [512/37814 (1%)]\tLoss: 1.818870\n",
            "{SqueezeNet} Train Epoch: 43 [1024/37814 (3%)]\tLoss: 1.882424\n",
            "{SqueezeNet} Train Epoch: 43 [1536/37814 (4%)]\tLoss: 1.831796\n",
            "{SqueezeNet} Train Epoch: 43 [2048/37814 (5%)]\tLoss: 1.749088\n",
            "{SqueezeNet} Train Epoch: 43 [2560/37814 (7%)]\tLoss: 1.833028\n",
            "{SqueezeNet} Train Epoch: 43 [3072/37814 (8%)]\tLoss: 1.801351\n",
            "{SqueezeNet} Train Epoch: 43 [3584/37814 (9%)]\tLoss: 1.818663\n",
            "{SqueezeNet} Train Epoch: 43 [4096/37814 (11%)]\tLoss: 1.796550\n",
            "{SqueezeNet} Train Epoch: 43 [4608/37814 (12%)]\tLoss: 1.822521\n",
            "{SqueezeNet} Train Epoch: 43 [5120/37814 (14%)]\tLoss: 1.848886\n",
            "{SqueezeNet} Train Epoch: 43 [5632/37814 (15%)]\tLoss: 1.849840\n",
            "{SqueezeNet} Train Epoch: 43 [6144/37814 (16%)]\tLoss: 1.785217\n",
            "{SqueezeNet} Train Epoch: 43 [6656/37814 (18%)]\tLoss: 1.844409\n",
            "{SqueezeNet} Train Epoch: 43 [7168/37814 (19%)]\tLoss: 1.811634\n",
            "{SqueezeNet} Train Epoch: 43 [7680/37814 (20%)]\tLoss: 1.825894\n",
            "{SqueezeNet} Train Epoch: 43 [8192/37814 (22%)]\tLoss: 1.777388\n",
            "{SqueezeNet} Train Epoch: 43 [8704/37814 (23%)]\tLoss: 1.801777\n",
            "{SqueezeNet} Train Epoch: 43 [9216/37814 (24%)]\tLoss: 1.843131\n",
            "{SqueezeNet} Train Epoch: 43 [9728/37814 (26%)]\tLoss: 1.770986\n",
            "{SqueezeNet} Train Epoch: 43 [10240/37814 (27%)]\tLoss: 1.778924\n",
            "{SqueezeNet} Train Epoch: 43 [10752/37814 (28%)]\tLoss: 1.861017\n",
            "{SqueezeNet} Train Epoch: 43 [11264/37814 (30%)]\tLoss: 1.790012\n",
            "{SqueezeNet} Train Epoch: 43 [11776/37814 (31%)]\tLoss: 1.834396\n",
            "{SqueezeNet} Train Epoch: 43 [12288/37814 (32%)]\tLoss: 1.816875\n",
            "{SqueezeNet} Train Epoch: 43 [12800/37814 (34%)]\tLoss: 1.835204\n",
            "{SqueezeNet} Train Epoch: 43 [13312/37814 (35%)]\tLoss: 1.831512\n",
            "{SqueezeNet} Train Epoch: 43 [13824/37814 (36%)]\tLoss: 1.794776\n",
            "{SqueezeNet} Train Epoch: 43 [14336/37814 (38%)]\tLoss: 1.800889\n",
            "{SqueezeNet} Train Epoch: 43 [14848/37814 (39%)]\tLoss: 1.828240\n",
            "{SqueezeNet} Train Epoch: 43 [15360/37814 (41%)]\tLoss: 1.882933\n",
            "{SqueezeNet} Train Epoch: 43 [15872/37814 (42%)]\tLoss: 1.808146\n",
            "{SqueezeNet} Train Epoch: 43 [16384/37814 (43%)]\tLoss: 1.839576\n",
            "{SqueezeNet} Train Epoch: 43 [16896/37814 (45%)]\tLoss: 1.815021\n",
            "{SqueezeNet} Train Epoch: 43 [17408/37814 (46%)]\tLoss: 1.820558\n",
            "{SqueezeNet} Train Epoch: 43 [17920/37814 (47%)]\tLoss: 1.816152\n",
            "{SqueezeNet} Train Epoch: 43 [18432/37814 (49%)]\tLoss: 1.858111\n",
            "{SqueezeNet} Train Epoch: 43 [18944/37814 (50%)]\tLoss: 1.806257\n",
            "{SqueezeNet} Train Epoch: 43 [19456/37814 (51%)]\tLoss: 1.847581\n",
            "{SqueezeNet} Train Epoch: 43 [19968/37814 (53%)]\tLoss: 1.769284\n",
            "{SqueezeNet} Train Epoch: 43 [20480/37814 (54%)]\tLoss: 1.781187\n",
            "{SqueezeNet} Train Epoch: 43 [20992/37814 (55%)]\tLoss: 1.861309\n",
            "{SqueezeNet} Train Epoch: 43 [21504/37814 (57%)]\tLoss: 1.750303\n",
            "{SqueezeNet} Train Epoch: 43 [22016/37814 (58%)]\tLoss: 1.831483\n",
            "{SqueezeNet} Train Epoch: 43 [22528/37814 (59%)]\tLoss: 1.906995\n",
            "{SqueezeNet} Train Epoch: 43 [23040/37814 (61%)]\tLoss: 1.807478\n",
            "{SqueezeNet} Train Epoch: 43 [23552/37814 (62%)]\tLoss: 1.814690\n",
            "{SqueezeNet} Train Epoch: 43 [24064/37814 (64%)]\tLoss: 1.788568\n",
            "{SqueezeNet} Train Epoch: 43 [24576/37814 (65%)]\tLoss: 1.820697\n",
            "{SqueezeNet} Train Epoch: 43 [25088/37814 (66%)]\tLoss: 1.818811\n",
            "{SqueezeNet} Train Epoch: 43 [25600/37814 (68%)]\tLoss: 1.810619\n",
            "{SqueezeNet} Train Epoch: 43 [26112/37814 (69%)]\tLoss: 1.824643\n",
            "{SqueezeNet} Train Epoch: 43 [26624/37814 (70%)]\tLoss: 1.813372\n",
            "{SqueezeNet} Train Epoch: 43 [27136/37814 (72%)]\tLoss: 1.856271\n",
            "{SqueezeNet} Train Epoch: 43 [27648/37814 (73%)]\tLoss: 1.781920\n",
            "{SqueezeNet} Train Epoch: 43 [28160/37814 (74%)]\tLoss: 1.833110\n",
            "{SqueezeNet} Train Epoch: 43 [28672/37814 (76%)]\tLoss: 1.803228\n",
            "{SqueezeNet} Train Epoch: 43 [29184/37814 (77%)]\tLoss: 1.840192\n",
            "{SqueezeNet} Train Epoch: 43 [29696/37814 (78%)]\tLoss: 1.811130\n",
            "{SqueezeNet} Train Epoch: 43 [30208/37814 (80%)]\tLoss: 1.803009\n",
            "{SqueezeNet} Train Epoch: 43 [30720/37814 (81%)]\tLoss: 1.834713\n",
            "{SqueezeNet} Train Epoch: 43 [31232/37814 (82%)]\tLoss: 1.889331\n",
            "{SqueezeNet} Train Epoch: 43 [31744/37814 (84%)]\tLoss: 1.833478\n",
            "{SqueezeNet} Train Epoch: 43 [32256/37814 (85%)]\tLoss: 1.816136\n",
            "{SqueezeNet} Train Epoch: 43 [32768/37814 (86%)]\tLoss: 1.821872\n",
            "{SqueezeNet} Train Epoch: 43 [33280/37814 (88%)]\tLoss: 1.823353\n",
            "{SqueezeNet} Train Epoch: 43 [33792/37814 (89%)]\tLoss: 1.824034\n",
            "{SqueezeNet} Train Epoch: 43 [34304/37814 (91%)]\tLoss: 1.845562\n",
            "{SqueezeNet} Train Epoch: 43 [34816/37814 (92%)]\tLoss: 1.908640\n",
            "{SqueezeNet} Train Epoch: 43 [35328/37814 (93%)]\tLoss: 1.856595\n",
            "{SqueezeNet} Train Epoch: 43 [35840/37814 (95%)]\tLoss: 1.890469\n",
            "{SqueezeNet} Train Epoch: 43 [36352/37814 (96%)]\tLoss: 1.783147\n",
            "{SqueezeNet} Train Epoch: 43 [36864/37814 (97%)]\tLoss: 1.838360\n",
            "{SqueezeNet} Train Epoch: 43 [31974/37814 (99%)]\tLoss: 1.794980\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8238, Accuracy: 1606/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.451411724090576 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f412a36-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"0dc1020a-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_524596f36e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f434de8-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_a9c22aeea1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f438ed4-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_8fb8024511"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f43d452-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f438ed4-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_dfdba8c006"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f6d6f74-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f434de8-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_27724f4dc8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f6f15c2-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_03b456bc66"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f6f5f0a-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_2a0dd071ed"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"2f6f922c-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f6f5f0a-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_875f289d7b"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 44 [0/37814 (0%)]\tLoss: 1.989599\n",
            "{AlexNet} Train Epoch: 44 [512/37814 (1%)]\tLoss: 1.996007\n",
            "{AlexNet} Train Epoch: 44 [1024/37814 (3%)]\tLoss: 2.021457\n",
            "{AlexNet} Train Epoch: 44 [1536/37814 (4%)]\tLoss: 1.982458\n",
            "{AlexNet} Train Epoch: 44 [2048/37814 (5%)]\tLoss: 1.978513\n",
            "{AlexNet} Train Epoch: 44 [2560/37814 (7%)]\tLoss: 1.969515\n",
            "{AlexNet} Train Epoch: 44 [3072/37814 (8%)]\tLoss: 1.972775\n",
            "{AlexNet} Train Epoch: 44 [3584/37814 (9%)]\tLoss: 1.989467\n",
            "{AlexNet} Train Epoch: 44 [4096/37814 (11%)]\tLoss: 2.002832\n",
            "{AlexNet} Train Epoch: 44 [4608/37814 (12%)]\tLoss: 1.930504\n",
            "{AlexNet} Train Epoch: 44 [5120/37814 (14%)]\tLoss: 2.045116\n",
            "{AlexNet} Train Epoch: 44 [5632/37814 (15%)]\tLoss: 1.979617\n",
            "{AlexNet} Train Epoch: 44 [6144/37814 (16%)]\tLoss: 1.966610\n",
            "{AlexNet} Train Epoch: 44 [6656/37814 (18%)]\tLoss: 1.924181\n",
            "{AlexNet} Train Epoch: 44 [7168/37814 (19%)]\tLoss: 1.952414\n",
            "{AlexNet} Train Epoch: 44 [7680/37814 (20%)]\tLoss: 1.994709\n",
            "{AlexNet} Train Epoch: 44 [8192/37814 (22%)]\tLoss: 1.974299\n",
            "{AlexNet} Train Epoch: 44 [8704/37814 (23%)]\tLoss: 1.949692\n",
            "{AlexNet} Train Epoch: 44 [9216/37814 (24%)]\tLoss: 2.012397\n",
            "{AlexNet} Train Epoch: 44 [9728/37814 (26%)]\tLoss: 1.985749\n",
            "{AlexNet} Train Epoch: 44 [10240/37814 (27%)]\tLoss: 2.004857\n",
            "{AlexNet} Train Epoch: 44 [10752/37814 (28%)]\tLoss: 1.988639\n",
            "{AlexNet} Train Epoch: 44 [11264/37814 (30%)]\tLoss: 1.947340\n",
            "{AlexNet} Train Epoch: 44 [11776/37814 (31%)]\tLoss: 1.949103\n",
            "{AlexNet} Train Epoch: 44 [12288/37814 (32%)]\tLoss: 1.925379\n",
            "{AlexNet} Train Epoch: 44 [12800/37814 (34%)]\tLoss: 1.986088\n",
            "{AlexNet} Train Epoch: 44 [13312/37814 (35%)]\tLoss: 2.007585\n",
            "{AlexNet} Train Epoch: 44 [13824/37814 (36%)]\tLoss: 2.001348\n",
            "{AlexNet} Train Epoch: 44 [14336/37814 (38%)]\tLoss: 1.997319\n",
            "{AlexNet} Train Epoch: 44 [14848/37814 (39%)]\tLoss: 1.982940\n",
            "{AlexNet} Train Epoch: 44 [15360/37814 (41%)]\tLoss: 1.977716\n",
            "{AlexNet} Train Epoch: 44 [15872/37814 (42%)]\tLoss: 1.981367\n",
            "{AlexNet} Train Epoch: 44 [16384/37814 (43%)]\tLoss: 1.954610\n",
            "{AlexNet} Train Epoch: 44 [16896/37814 (45%)]\tLoss: 1.962387\n",
            "{AlexNet} Train Epoch: 44 [17408/37814 (46%)]\tLoss: 1.950302\n",
            "{AlexNet} Train Epoch: 44 [17920/37814 (47%)]\tLoss: 1.956915\n",
            "{AlexNet} Train Epoch: 44 [18432/37814 (49%)]\tLoss: 1.975715\n",
            "{AlexNet} Train Epoch: 44 [18944/37814 (50%)]\tLoss: 1.982640\n",
            "{AlexNet} Train Epoch: 44 [19456/37814 (51%)]\tLoss: 1.991290\n",
            "{AlexNet} Train Epoch: 44 [19968/37814 (53%)]\tLoss: 1.955714\n",
            "{AlexNet} Train Epoch: 44 [20480/37814 (54%)]\tLoss: 1.966511\n",
            "{AlexNet} Train Epoch: 44 [20992/37814 (55%)]\tLoss: 1.953452\n",
            "{AlexNet} Train Epoch: 44 [21504/37814 (57%)]\tLoss: 1.976477\n",
            "{AlexNet} Train Epoch: 44 [22016/37814 (58%)]\tLoss: 1.992739\n",
            "{AlexNet} Train Epoch: 44 [22528/37814 (59%)]\tLoss: 2.018212\n",
            "{AlexNet} Train Epoch: 44 [23040/37814 (61%)]\tLoss: 1.946480\n",
            "{AlexNet} Train Epoch: 44 [23552/37814 (62%)]\tLoss: 1.964601\n",
            "{AlexNet} Train Epoch: 44 [24064/37814 (64%)]\tLoss: 1.955067\n",
            "{AlexNet} Train Epoch: 44 [24576/37814 (65%)]\tLoss: 2.020677\n",
            "{AlexNet} Train Epoch: 44 [25088/37814 (66%)]\tLoss: 1.968406\n",
            "{AlexNet} Train Epoch: 44 [25600/37814 (68%)]\tLoss: 2.017228\n",
            "{AlexNet} Train Epoch: 44 [26112/37814 (69%)]\tLoss: 1.967461\n",
            "{AlexNet} Train Epoch: 44 [26624/37814 (70%)]\tLoss: 1.958064\n",
            "{AlexNet} Train Epoch: 44 [27136/37814 (72%)]\tLoss: 1.947101\n",
            "{AlexNet} Train Epoch: 44 [27648/37814 (73%)]\tLoss: 1.949947\n",
            "{AlexNet} Train Epoch: 44 [28160/37814 (74%)]\tLoss: 1.966626\n",
            "{AlexNet} Train Epoch: 44 [28672/37814 (76%)]\tLoss: 1.982619\n",
            "{AlexNet} Train Epoch: 44 [29184/37814 (77%)]\tLoss: 2.018753\n",
            "{AlexNet} Train Epoch: 44 [29696/37814 (78%)]\tLoss: 1.964617\n",
            "{AlexNet} Train Epoch: 44 [30208/37814 (80%)]\tLoss: 1.936507\n",
            "{AlexNet} Train Epoch: 44 [30720/37814 (81%)]\tLoss: 2.007186\n",
            "{AlexNet} Train Epoch: 44 [31232/37814 (82%)]\tLoss: 1.959799\n",
            "{AlexNet} Train Epoch: 44 [31744/37814 (84%)]\tLoss: 1.972333\n",
            "{AlexNet} Train Epoch: 44 [32256/37814 (85%)]\tLoss: 1.998013\n",
            "{AlexNet} Train Epoch: 44 [32768/37814 (86%)]\tLoss: 1.977478\n",
            "{AlexNet} Train Epoch: 44 [33280/37814 (88%)]\tLoss: 1.998842\n",
            "{AlexNet} Train Epoch: 44 [33792/37814 (89%)]\tLoss: 1.981771\n",
            "{AlexNet} Train Epoch: 44 [34304/37814 (91%)]\tLoss: 1.960521\n",
            "{AlexNet} Train Epoch: 44 [34816/37814 (92%)]\tLoss: 1.963004\n",
            "{AlexNet} Train Epoch: 44 [35328/37814 (93%)]\tLoss: 1.996889\n",
            "{AlexNet} Train Epoch: 44 [35840/37814 (95%)]\tLoss: 1.954719\n",
            "{AlexNet} Train Epoch: 44 [36352/37814 (96%)]\tLoss: 2.022139\n",
            "{AlexNet} Train Epoch: 44 [36864/37814 (97%)]\tLoss: 1.923893\n",
            "{AlexNet} Train Epoch: 44 [31974/37814 (99%)]\tLoss: 1.956308\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9796, Accuracy: 1041/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.85923194885254 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 44 [0/37814 (0%)]\tLoss: 1.863112\n",
            "{SqueezeNet} Train Epoch: 44 [512/37814 (1%)]\tLoss: 1.793941\n",
            "{SqueezeNet} Train Epoch: 44 [1024/37814 (3%)]\tLoss: 1.786593\n",
            "{SqueezeNet} Train Epoch: 44 [1536/37814 (4%)]\tLoss: 1.849681\n",
            "{SqueezeNet} Train Epoch: 44 [2048/37814 (5%)]\tLoss: 1.790496\n",
            "{SqueezeNet} Train Epoch: 44 [2560/37814 (7%)]\tLoss: 1.804799\n",
            "{SqueezeNet} Train Epoch: 44 [3072/37814 (8%)]\tLoss: 1.832782\n",
            "{SqueezeNet} Train Epoch: 44 [3584/37814 (9%)]\tLoss: 1.881880\n",
            "{SqueezeNet} Train Epoch: 44 [4096/37814 (11%)]\tLoss: 1.818009\n",
            "{SqueezeNet} Train Epoch: 44 [4608/37814 (12%)]\tLoss: 1.846380\n",
            "{SqueezeNet} Train Epoch: 44 [5120/37814 (14%)]\tLoss: 1.889652\n",
            "{SqueezeNet} Train Epoch: 44 [5632/37814 (15%)]\tLoss: 1.804159\n",
            "{SqueezeNet} Train Epoch: 44 [6144/37814 (16%)]\tLoss: 1.819455\n",
            "{SqueezeNet} Train Epoch: 44 [6656/37814 (18%)]\tLoss: 1.905142\n",
            "{SqueezeNet} Train Epoch: 44 [7168/37814 (19%)]\tLoss: 1.840797\n",
            "{SqueezeNet} Train Epoch: 44 [7680/37814 (20%)]\tLoss: 1.836239\n",
            "{SqueezeNet} Train Epoch: 44 [8192/37814 (22%)]\tLoss: 1.803584\n",
            "{SqueezeNet} Train Epoch: 44 [8704/37814 (23%)]\tLoss: 1.792944\n",
            "{SqueezeNet} Train Epoch: 44 [9216/37814 (24%)]\tLoss: 1.792437\n",
            "{SqueezeNet} Train Epoch: 44 [9728/37814 (26%)]\tLoss: 1.810053\n",
            "{SqueezeNet} Train Epoch: 44 [10240/37814 (27%)]\tLoss: 1.777036\n",
            "{SqueezeNet} Train Epoch: 44 [10752/37814 (28%)]\tLoss: 1.823419\n",
            "{SqueezeNet} Train Epoch: 44 [11264/37814 (30%)]\tLoss: 1.865394\n",
            "{SqueezeNet} Train Epoch: 44 [11776/37814 (31%)]\tLoss: 1.875409\n",
            "{SqueezeNet} Train Epoch: 44 [12288/37814 (32%)]\tLoss: 1.782656\n",
            "{SqueezeNet} Train Epoch: 44 [12800/37814 (34%)]\tLoss: 1.833853\n",
            "{SqueezeNet} Train Epoch: 44 [13312/37814 (35%)]\tLoss: 1.894663\n",
            "{SqueezeNet} Train Epoch: 44 [13824/37814 (36%)]\tLoss: 1.812855\n",
            "{SqueezeNet} Train Epoch: 44 [14336/37814 (38%)]\tLoss: 1.840374\n",
            "{SqueezeNet} Train Epoch: 44 [14848/37814 (39%)]\tLoss: 1.823006\n",
            "{SqueezeNet} Train Epoch: 44 [15360/37814 (41%)]\tLoss: 1.832951\n",
            "{SqueezeNet} Train Epoch: 44 [15872/37814 (42%)]\tLoss: 1.842704\n",
            "{SqueezeNet} Train Epoch: 44 [16384/37814 (43%)]\tLoss: 1.808792\n",
            "{SqueezeNet} Train Epoch: 44 [16896/37814 (45%)]\tLoss: 1.854521\n",
            "{SqueezeNet} Train Epoch: 44 [17408/37814 (46%)]\tLoss: 1.814799\n",
            "{SqueezeNet} Train Epoch: 44 [17920/37814 (47%)]\tLoss: 1.804403\n",
            "{SqueezeNet} Train Epoch: 44 [18432/37814 (49%)]\tLoss: 1.838746\n",
            "{SqueezeNet} Train Epoch: 44 [18944/37814 (50%)]\tLoss: 1.863480\n",
            "{SqueezeNet} Train Epoch: 44 [19456/37814 (51%)]\tLoss: 1.857368\n",
            "{SqueezeNet} Train Epoch: 44 [19968/37814 (53%)]\tLoss: 1.824144\n",
            "{SqueezeNet} Train Epoch: 44 [20480/37814 (54%)]\tLoss: 1.902730\n",
            "{SqueezeNet} Train Epoch: 44 [20992/37814 (55%)]\tLoss: 1.871794\n",
            "{SqueezeNet} Train Epoch: 44 [21504/37814 (57%)]\tLoss: 1.760014\n",
            "{SqueezeNet} Train Epoch: 44 [22016/37814 (58%)]\tLoss: 1.766439\n",
            "{SqueezeNet} Train Epoch: 44 [22528/37814 (59%)]\tLoss: 1.838908\n",
            "{SqueezeNet} Train Epoch: 44 [23040/37814 (61%)]\tLoss: 1.771609\n",
            "{SqueezeNet} Train Epoch: 44 [23552/37814 (62%)]\tLoss: 1.827570\n",
            "{SqueezeNet} Train Epoch: 44 [24064/37814 (64%)]\tLoss: 1.827158\n",
            "{SqueezeNet} Train Epoch: 44 [24576/37814 (65%)]\tLoss: 1.728975\n",
            "{SqueezeNet} Train Epoch: 44 [25088/37814 (66%)]\tLoss: 1.817576\n",
            "{SqueezeNet} Train Epoch: 44 [25600/37814 (68%)]\tLoss: 1.789117\n",
            "{SqueezeNet} Train Epoch: 44 [26112/37814 (69%)]\tLoss: 1.793900\n",
            "{SqueezeNet} Train Epoch: 44 [26624/37814 (70%)]\tLoss: 1.862881\n",
            "{SqueezeNet} Train Epoch: 44 [27136/37814 (72%)]\tLoss: 1.790902\n",
            "{SqueezeNet} Train Epoch: 44 [27648/37814 (73%)]\tLoss: 1.864725\n",
            "{SqueezeNet} Train Epoch: 44 [28160/37814 (74%)]\tLoss: 1.834147\n",
            "{SqueezeNet} Train Epoch: 44 [28672/37814 (76%)]\tLoss: 1.801211\n",
            "{SqueezeNet} Train Epoch: 44 [29184/37814 (77%)]\tLoss: 1.826180\n",
            "{SqueezeNet} Train Epoch: 44 [29696/37814 (78%)]\tLoss: 1.806712\n",
            "{SqueezeNet} Train Epoch: 44 [30208/37814 (80%)]\tLoss: 1.825404\n",
            "{SqueezeNet} Train Epoch: 44 [30720/37814 (81%)]\tLoss: 1.808912\n",
            "{SqueezeNet} Train Epoch: 44 [31232/37814 (82%)]\tLoss: 1.796402\n",
            "{SqueezeNet} Train Epoch: 44 [31744/37814 (84%)]\tLoss: 1.766250\n",
            "{SqueezeNet} Train Epoch: 44 [32256/37814 (85%)]\tLoss: 1.818365\n",
            "{SqueezeNet} Train Epoch: 44 [32768/37814 (86%)]\tLoss: 1.773992\n",
            "{SqueezeNet} Train Epoch: 44 [33280/37814 (88%)]\tLoss: 1.816693\n",
            "{SqueezeNet} Train Epoch: 44 [33792/37814 (89%)]\tLoss: 1.863421\n",
            "{SqueezeNet} Train Epoch: 44 [34304/37814 (91%)]\tLoss: 1.794047\n",
            "{SqueezeNet} Train Epoch: 44 [34816/37814 (92%)]\tLoss: 1.813423\n",
            "{SqueezeNet} Train Epoch: 44 [35328/37814 (93%)]\tLoss: 1.868404\n",
            "{SqueezeNet} Train Epoch: 44 [35840/37814 (95%)]\tLoss: 1.785874\n",
            "{SqueezeNet} Train Epoch: 44 [36352/37814 (96%)]\tLoss: 1.816644\n",
            "{SqueezeNet} Train Epoch: 44 [36864/37814 (97%)]\tLoss: 1.838847\n",
            "{SqueezeNet} Train Epoch: 44 [31974/37814 (99%)]\tLoss: 1.844060\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8086, Accuracy: 1596/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.204667568206787 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"50dbbd5a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"2f6f15c2-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2e7e12c8e0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"50dd8536-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_3f5bbf8c7a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"50ddd27a-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_c095df244e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"50de1b9a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"50ddd27a-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_1f94f5cc7a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51061014-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"50dd8536-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_5820f9996e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51070cd0-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_7fe0f53936"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"51073e26-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_8a126ebe68"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"5107738c-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"51073e26-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6c11597b61"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 45 [0/37814 (0%)]\tLoss: 2.024362\n",
            "{AlexNet} Train Epoch: 45 [512/37814 (1%)]\tLoss: 1.972515\n",
            "{AlexNet} Train Epoch: 45 [1024/37814 (3%)]\tLoss: 1.953027\n",
            "{AlexNet} Train Epoch: 45 [1536/37814 (4%)]\tLoss: 1.968403\n",
            "{AlexNet} Train Epoch: 45 [2048/37814 (5%)]\tLoss: 1.967500\n",
            "{AlexNet} Train Epoch: 45 [2560/37814 (7%)]\tLoss: 1.975505\n",
            "{AlexNet} Train Epoch: 45 [3072/37814 (8%)]\tLoss: 1.964888\n",
            "{AlexNet} Train Epoch: 45 [3584/37814 (9%)]\tLoss: 2.009685\n",
            "{AlexNet} Train Epoch: 45 [4096/37814 (11%)]\tLoss: 1.982298\n",
            "{AlexNet} Train Epoch: 45 [4608/37814 (12%)]\tLoss: 1.976593\n",
            "{AlexNet} Train Epoch: 45 [5120/37814 (14%)]\tLoss: 1.923184\n",
            "{AlexNet} Train Epoch: 45 [5632/37814 (15%)]\tLoss: 1.971503\n",
            "{AlexNet} Train Epoch: 45 [6144/37814 (16%)]\tLoss: 1.963991\n",
            "{AlexNet} Train Epoch: 45 [6656/37814 (18%)]\tLoss: 1.978516\n",
            "{AlexNet} Train Epoch: 45 [7168/37814 (19%)]\tLoss: 1.912750\n",
            "{AlexNet} Train Epoch: 45 [7680/37814 (20%)]\tLoss: 1.978024\n",
            "{AlexNet} Train Epoch: 45 [8192/37814 (22%)]\tLoss: 1.998814\n",
            "{AlexNet} Train Epoch: 45 [8704/37814 (23%)]\tLoss: 1.996693\n",
            "{AlexNet} Train Epoch: 45 [9216/37814 (24%)]\tLoss: 1.978103\n",
            "{AlexNet} Train Epoch: 45 [9728/37814 (26%)]\tLoss: 2.000529\n",
            "{AlexNet} Train Epoch: 45 [10240/37814 (27%)]\tLoss: 1.977553\n",
            "{AlexNet} Train Epoch: 45 [10752/37814 (28%)]\tLoss: 2.036693\n",
            "{AlexNet} Train Epoch: 45 [11264/37814 (30%)]\tLoss: 1.967660\n",
            "{AlexNet} Train Epoch: 45 [11776/37814 (31%)]\tLoss: 2.018978\n",
            "{AlexNet} Train Epoch: 45 [12288/37814 (32%)]\tLoss: 1.990677\n",
            "{AlexNet} Train Epoch: 45 [12800/37814 (34%)]\tLoss: 1.990283\n",
            "{AlexNet} Train Epoch: 45 [13312/37814 (35%)]\tLoss: 1.955403\n",
            "{AlexNet} Train Epoch: 45 [13824/37814 (36%)]\tLoss: 1.921587\n",
            "{AlexNet} Train Epoch: 45 [14336/37814 (38%)]\tLoss: 1.996553\n",
            "{AlexNet} Train Epoch: 45 [14848/37814 (39%)]\tLoss: 1.995031\n",
            "{AlexNet} Train Epoch: 45 [15360/37814 (41%)]\tLoss: 2.012732\n",
            "{AlexNet} Train Epoch: 45 [15872/37814 (42%)]\tLoss: 2.012433\n",
            "{AlexNet} Train Epoch: 45 [16384/37814 (43%)]\tLoss: 1.991819\n",
            "{AlexNet} Train Epoch: 45 [16896/37814 (45%)]\tLoss: 1.954176\n",
            "{AlexNet} Train Epoch: 45 [17408/37814 (46%)]\tLoss: 2.015036\n",
            "{AlexNet} Train Epoch: 45 [17920/37814 (47%)]\tLoss: 1.936608\n",
            "{AlexNet} Train Epoch: 45 [18432/37814 (49%)]\tLoss: 1.933506\n",
            "{AlexNet} Train Epoch: 45 [18944/37814 (50%)]\tLoss: 1.996005\n",
            "{AlexNet} Train Epoch: 45 [19456/37814 (51%)]\tLoss: 1.981295\n",
            "{AlexNet} Train Epoch: 45 [19968/37814 (53%)]\tLoss: 2.009024\n",
            "{AlexNet} Train Epoch: 45 [20480/37814 (54%)]\tLoss: 2.014618\n",
            "{AlexNet} Train Epoch: 45 [20992/37814 (55%)]\tLoss: 1.965877\n",
            "{AlexNet} Train Epoch: 45 [21504/37814 (57%)]\tLoss: 1.958339\n",
            "{AlexNet} Train Epoch: 45 [22016/37814 (58%)]\tLoss: 1.985612\n",
            "{AlexNet} Train Epoch: 45 [22528/37814 (59%)]\tLoss: 2.009373\n",
            "{AlexNet} Train Epoch: 45 [23040/37814 (61%)]\tLoss: 2.006804\n",
            "{AlexNet} Train Epoch: 45 [23552/37814 (62%)]\tLoss: 1.961262\n",
            "{AlexNet} Train Epoch: 45 [24064/37814 (64%)]\tLoss: 2.013475\n",
            "{AlexNet} Train Epoch: 45 [24576/37814 (65%)]\tLoss: 1.959553\n",
            "{AlexNet} Train Epoch: 45 [25088/37814 (66%)]\tLoss: 2.009565\n",
            "{AlexNet} Train Epoch: 45 [25600/37814 (68%)]\tLoss: 1.930680\n",
            "{AlexNet} Train Epoch: 45 [26112/37814 (69%)]\tLoss: 1.936924\n",
            "{AlexNet} Train Epoch: 45 [26624/37814 (70%)]\tLoss: 2.004083\n",
            "{AlexNet} Train Epoch: 45 [27136/37814 (72%)]\tLoss: 1.987629\n",
            "{AlexNet} Train Epoch: 45 [27648/37814 (73%)]\tLoss: 1.998027\n",
            "{AlexNet} Train Epoch: 45 [28160/37814 (74%)]\tLoss: 1.952386\n",
            "{AlexNet} Train Epoch: 45 [28672/37814 (76%)]\tLoss: 1.965816\n",
            "{AlexNet} Train Epoch: 45 [29184/37814 (77%)]\tLoss: 1.939197\n",
            "{AlexNet} Train Epoch: 45 [29696/37814 (78%)]\tLoss: 1.984021\n",
            "{AlexNet} Train Epoch: 45 [30208/37814 (80%)]\tLoss: 1.987939\n",
            "{AlexNet} Train Epoch: 45 [30720/37814 (81%)]\tLoss: 1.957238\n",
            "{AlexNet} Train Epoch: 45 [31232/37814 (82%)]\tLoss: 1.977837\n",
            "{AlexNet} Train Epoch: 45 [31744/37814 (84%)]\tLoss: 1.958560\n",
            "{AlexNet} Train Epoch: 45 [32256/37814 (85%)]\tLoss: 1.909057\n",
            "{AlexNet} Train Epoch: 45 [32768/37814 (86%)]\tLoss: 1.977748\n",
            "{AlexNet} Train Epoch: 45 [33280/37814 (88%)]\tLoss: 1.985278\n",
            "{AlexNet} Train Epoch: 45 [33792/37814 (89%)]\tLoss: 1.976328\n",
            "{AlexNet} Train Epoch: 45 [34304/37814 (91%)]\tLoss: 1.964729\n",
            "{AlexNet} Train Epoch: 45 [34816/37814 (92%)]\tLoss: 1.976761\n",
            "{AlexNet} Train Epoch: 45 [35328/37814 (93%)]\tLoss: 1.989078\n",
            "{AlexNet} Train Epoch: 45 [35840/37814 (95%)]\tLoss: 1.973859\n",
            "{AlexNet} Train Epoch: 45 [36352/37814 (96%)]\tLoss: 2.003840\n",
            "{AlexNet} Train Epoch: 45 [36864/37814 (97%)]\tLoss: 1.955852\n",
            "{AlexNet} Train Epoch: 45 [31974/37814 (99%)]\tLoss: 1.918625\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9714, Accuracy: 1053/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.373656034469604 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 45 [0/37814 (0%)]\tLoss: 1.858245\n",
            "{SqueezeNet} Train Epoch: 45 [512/37814 (1%)]\tLoss: 1.821409\n",
            "{SqueezeNet} Train Epoch: 45 [1024/37814 (3%)]\tLoss: 1.808040\n",
            "{SqueezeNet} Train Epoch: 45 [1536/37814 (4%)]\tLoss: 1.846659\n",
            "{SqueezeNet} Train Epoch: 45 [2048/37814 (5%)]\tLoss: 1.821454\n",
            "{SqueezeNet} Train Epoch: 45 [2560/37814 (7%)]\tLoss: 1.905032\n",
            "{SqueezeNet} Train Epoch: 45 [3072/37814 (8%)]\tLoss: 1.804238\n",
            "{SqueezeNet} Train Epoch: 45 [3584/37814 (9%)]\tLoss: 1.804856\n",
            "{SqueezeNet} Train Epoch: 45 [4096/37814 (11%)]\tLoss: 1.813045\n",
            "{SqueezeNet} Train Epoch: 45 [4608/37814 (12%)]\tLoss: 1.772681\n",
            "{SqueezeNet} Train Epoch: 45 [5120/37814 (14%)]\tLoss: 1.821421\n",
            "{SqueezeNet} Train Epoch: 45 [5632/37814 (15%)]\tLoss: 1.788347\n",
            "{SqueezeNet} Train Epoch: 45 [6144/37814 (16%)]\tLoss: 1.863733\n",
            "{SqueezeNet} Train Epoch: 45 [6656/37814 (18%)]\tLoss: 1.807470\n",
            "{SqueezeNet} Train Epoch: 45 [7168/37814 (19%)]\tLoss: 1.794875\n",
            "{SqueezeNet} Train Epoch: 45 [7680/37814 (20%)]\tLoss: 1.813375\n",
            "{SqueezeNet} Train Epoch: 45 [8192/37814 (22%)]\tLoss: 1.853699\n",
            "{SqueezeNet} Train Epoch: 45 [8704/37814 (23%)]\tLoss: 1.810578\n",
            "{SqueezeNet} Train Epoch: 45 [9216/37814 (24%)]\tLoss: 1.820541\n",
            "{SqueezeNet} Train Epoch: 45 [9728/37814 (26%)]\tLoss: 1.793665\n",
            "{SqueezeNet} Train Epoch: 45 [10240/37814 (27%)]\tLoss: 1.798729\n",
            "{SqueezeNet} Train Epoch: 45 [10752/37814 (28%)]\tLoss: 1.827126\n",
            "{SqueezeNet} Train Epoch: 45 [11264/37814 (30%)]\tLoss: 1.885142\n",
            "{SqueezeNet} Train Epoch: 45 [11776/37814 (31%)]\tLoss: 1.851168\n",
            "{SqueezeNet} Train Epoch: 45 [12288/37814 (32%)]\tLoss: 1.791144\n",
            "{SqueezeNet} Train Epoch: 45 [12800/37814 (34%)]\tLoss: 1.803139\n",
            "{SqueezeNet} Train Epoch: 45 [13312/37814 (35%)]\tLoss: 1.816712\n",
            "{SqueezeNet} Train Epoch: 45 [13824/37814 (36%)]\tLoss: 1.852785\n",
            "{SqueezeNet} Train Epoch: 45 [14336/37814 (38%)]\tLoss: 1.805357\n",
            "{SqueezeNet} Train Epoch: 45 [14848/37814 (39%)]\tLoss: 1.821718\n",
            "{SqueezeNet} Train Epoch: 45 [15360/37814 (41%)]\tLoss: 1.786716\n",
            "{SqueezeNet} Train Epoch: 45 [15872/37814 (42%)]\tLoss: 1.806574\n",
            "{SqueezeNet} Train Epoch: 45 [16384/37814 (43%)]\tLoss: 1.792142\n",
            "{SqueezeNet} Train Epoch: 45 [16896/37814 (45%)]\tLoss: 1.882371\n",
            "{SqueezeNet} Train Epoch: 45 [17408/37814 (46%)]\tLoss: 1.852968\n",
            "{SqueezeNet} Train Epoch: 45 [17920/37814 (47%)]\tLoss: 1.904948\n",
            "{SqueezeNet} Train Epoch: 45 [18432/37814 (49%)]\tLoss: 1.780530\n",
            "{SqueezeNet} Train Epoch: 45 [18944/37814 (50%)]\tLoss: 1.803147\n",
            "{SqueezeNet} Train Epoch: 45 [19456/37814 (51%)]\tLoss: 1.835794\n",
            "{SqueezeNet} Train Epoch: 45 [19968/37814 (53%)]\tLoss: 1.843098\n",
            "{SqueezeNet} Train Epoch: 45 [20480/37814 (54%)]\tLoss: 1.825238\n",
            "{SqueezeNet} Train Epoch: 45 [20992/37814 (55%)]\tLoss: 1.797684\n",
            "{SqueezeNet} Train Epoch: 45 [21504/37814 (57%)]\tLoss: 1.823920\n",
            "{SqueezeNet} Train Epoch: 45 [22016/37814 (58%)]\tLoss: 1.862290\n",
            "{SqueezeNet} Train Epoch: 45 [22528/37814 (59%)]\tLoss: 1.779630\n",
            "{SqueezeNet} Train Epoch: 45 [23040/37814 (61%)]\tLoss: 1.814396\n",
            "{SqueezeNet} Train Epoch: 45 [23552/37814 (62%)]\tLoss: 1.795792\n",
            "{SqueezeNet} Train Epoch: 45 [24064/37814 (64%)]\tLoss: 1.852920\n",
            "{SqueezeNet} Train Epoch: 45 [24576/37814 (65%)]\tLoss: 1.797184\n",
            "{SqueezeNet} Train Epoch: 45 [25088/37814 (66%)]\tLoss: 1.836645\n",
            "{SqueezeNet} Train Epoch: 45 [25600/37814 (68%)]\tLoss: 1.822606\n",
            "{SqueezeNet} Train Epoch: 45 [26112/37814 (69%)]\tLoss: 1.798012\n",
            "{SqueezeNet} Train Epoch: 45 [26624/37814 (70%)]\tLoss: 1.848443\n",
            "{SqueezeNet} Train Epoch: 45 [27136/37814 (72%)]\tLoss: 1.816832\n",
            "{SqueezeNet} Train Epoch: 45 [27648/37814 (73%)]\tLoss: 1.840850\n",
            "{SqueezeNet} Train Epoch: 45 [28160/37814 (74%)]\tLoss: 1.861412\n",
            "{SqueezeNet} Train Epoch: 45 [28672/37814 (76%)]\tLoss: 1.834692\n",
            "{SqueezeNet} Train Epoch: 45 [29184/37814 (77%)]\tLoss: 1.800005\n",
            "{SqueezeNet} Train Epoch: 45 [29696/37814 (78%)]\tLoss: 1.781209\n",
            "{SqueezeNet} Train Epoch: 45 [30208/37814 (80%)]\tLoss: 1.790713\n",
            "{SqueezeNet} Train Epoch: 45 [30720/37814 (81%)]\tLoss: 1.841735\n",
            "{SqueezeNet} Train Epoch: 45 [31232/37814 (82%)]\tLoss: 1.762997\n",
            "{SqueezeNet} Train Epoch: 45 [31744/37814 (84%)]\tLoss: 1.822782\n",
            "{SqueezeNet} Train Epoch: 45 [32256/37814 (85%)]\tLoss: 1.848507\n",
            "{SqueezeNet} Train Epoch: 45 [32768/37814 (86%)]\tLoss: 1.823854\n",
            "{SqueezeNet} Train Epoch: 45 [33280/37814 (88%)]\tLoss: 1.794283\n",
            "{SqueezeNet} Train Epoch: 45 [33792/37814 (89%)]\tLoss: 1.814654\n",
            "{SqueezeNet} Train Epoch: 45 [34304/37814 (91%)]\tLoss: 1.866555\n",
            "{SqueezeNet} Train Epoch: 45 [34816/37814 (92%)]\tLoss: 1.800540\n",
            "{SqueezeNet} Train Epoch: 45 [35328/37814 (93%)]\tLoss: 1.822013\n",
            "{SqueezeNet} Train Epoch: 45 [35840/37814 (95%)]\tLoss: 1.813199\n",
            "{SqueezeNet} Train Epoch: 45 [36352/37814 (96%)]\tLoss: 1.934731\n",
            "{SqueezeNet} Train Epoch: 45 [36864/37814 (97%)]\tLoss: 1.788758\n",
            "{SqueezeNet} Train Epoch: 45 [31974/37814 (99%)]\tLoss: 1.818668\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8173, Accuracy: 1602/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.41344141960144 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"72493fda-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"51070cd0-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_2dbb18a882"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"724ac7f6-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_d30b53abba"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"724b0202-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_5b76b343d2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"724b3ccc-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"724b0202-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_05ef083ab1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"727682ba-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"724ac7f6-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_080d02fce7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"727897bc-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_4167800127"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"72791408-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_1f2c724d68"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"72798afa-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"72791408-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_12a625c61d"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 46 [0/37814 (0%)]\tLoss: 2.026967\n",
            "{AlexNet} Train Epoch: 46 [512/37814 (1%)]\tLoss: 1.917550\n",
            "{AlexNet} Train Epoch: 46 [1024/37814 (3%)]\tLoss: 1.975295\n",
            "{AlexNet} Train Epoch: 46 [1536/37814 (4%)]\tLoss: 1.996329\n",
            "{AlexNet} Train Epoch: 46 [2048/37814 (5%)]\tLoss: 1.983021\n",
            "{AlexNet} Train Epoch: 46 [2560/37814 (7%)]\tLoss: 1.963529\n",
            "{AlexNet} Train Epoch: 46 [3072/37814 (8%)]\tLoss: 2.038086\n",
            "{AlexNet} Train Epoch: 46 [3584/37814 (9%)]\tLoss: 2.003541\n",
            "{AlexNet} Train Epoch: 46 [4096/37814 (11%)]\tLoss: 1.990295\n",
            "{AlexNet} Train Epoch: 46 [4608/37814 (12%)]\tLoss: 2.000498\n",
            "{AlexNet} Train Epoch: 46 [5120/37814 (14%)]\tLoss: 1.966109\n",
            "{AlexNet} Train Epoch: 46 [5632/37814 (15%)]\tLoss: 1.959734\n",
            "{AlexNet} Train Epoch: 46 [6144/37814 (16%)]\tLoss: 1.953175\n",
            "{AlexNet} Train Epoch: 46 [6656/37814 (18%)]\tLoss: 1.940800\n",
            "{AlexNet} Train Epoch: 46 [7168/37814 (19%)]\tLoss: 1.959154\n",
            "{AlexNet} Train Epoch: 46 [7680/37814 (20%)]\tLoss: 1.980176\n",
            "{AlexNet} Train Epoch: 46 [8192/37814 (22%)]\tLoss: 1.960097\n",
            "{AlexNet} Train Epoch: 46 [8704/37814 (23%)]\tLoss: 1.985312\n",
            "{AlexNet} Train Epoch: 46 [9216/37814 (24%)]\tLoss: 1.934615\n",
            "{AlexNet} Train Epoch: 46 [9728/37814 (26%)]\tLoss: 1.966104\n",
            "{AlexNet} Train Epoch: 46 [10240/37814 (27%)]\tLoss: 1.999641\n",
            "{AlexNet} Train Epoch: 46 [10752/37814 (28%)]\tLoss: 1.923559\n",
            "{AlexNet} Train Epoch: 46 [11264/37814 (30%)]\tLoss: 1.958978\n",
            "{AlexNet} Train Epoch: 46 [11776/37814 (31%)]\tLoss: 1.966283\n",
            "{AlexNet} Train Epoch: 46 [12288/37814 (32%)]\tLoss: 1.961360\n",
            "{AlexNet} Train Epoch: 46 [12800/37814 (34%)]\tLoss: 1.975611\n",
            "{AlexNet} Train Epoch: 46 [13312/37814 (35%)]\tLoss: 1.958052\n",
            "{AlexNet} Train Epoch: 46 [13824/37814 (36%)]\tLoss: 1.950243\n",
            "{AlexNet} Train Epoch: 46 [14336/37814 (38%)]\tLoss: 1.965607\n",
            "{AlexNet} Train Epoch: 46 [14848/37814 (39%)]\tLoss: 1.936163\n",
            "{AlexNet} Train Epoch: 46 [15360/37814 (41%)]\tLoss: 1.958183\n",
            "{AlexNet} Train Epoch: 46 [15872/37814 (42%)]\tLoss: 1.957079\n",
            "{AlexNet} Train Epoch: 46 [16384/37814 (43%)]\tLoss: 1.997711\n",
            "{AlexNet} Train Epoch: 46 [16896/37814 (45%)]\tLoss: 1.994958\n",
            "{AlexNet} Train Epoch: 46 [17408/37814 (46%)]\tLoss: 1.942299\n",
            "{AlexNet} Train Epoch: 46 [17920/37814 (47%)]\tLoss: 1.933096\n",
            "{AlexNet} Train Epoch: 46 [18432/37814 (49%)]\tLoss: 1.999093\n",
            "{AlexNet} Train Epoch: 46 [18944/37814 (50%)]\tLoss: 1.937878\n",
            "{AlexNet} Train Epoch: 46 [19456/37814 (51%)]\tLoss: 2.003783\n",
            "{AlexNet} Train Epoch: 46 [19968/37814 (53%)]\tLoss: 1.983714\n",
            "{AlexNet} Train Epoch: 46 [20480/37814 (54%)]\tLoss: 1.980874\n",
            "{AlexNet} Train Epoch: 46 [20992/37814 (55%)]\tLoss: 1.977898\n",
            "{AlexNet} Train Epoch: 46 [21504/37814 (57%)]\tLoss: 2.001459\n",
            "{AlexNet} Train Epoch: 46 [22016/37814 (58%)]\tLoss: 1.934438\n",
            "{AlexNet} Train Epoch: 46 [22528/37814 (59%)]\tLoss: 1.973048\n",
            "{AlexNet} Train Epoch: 46 [23040/37814 (61%)]\tLoss: 1.959728\n",
            "{AlexNet} Train Epoch: 46 [23552/37814 (62%)]\tLoss: 2.014290\n",
            "{AlexNet} Train Epoch: 46 [24064/37814 (64%)]\tLoss: 1.977473\n",
            "{AlexNet} Train Epoch: 46 [24576/37814 (65%)]\tLoss: 1.980544\n",
            "{AlexNet} Train Epoch: 46 [25088/37814 (66%)]\tLoss: 1.973757\n",
            "{AlexNet} Train Epoch: 46 [25600/37814 (68%)]\tLoss: 1.949745\n",
            "{AlexNet} Train Epoch: 46 [26112/37814 (69%)]\tLoss: 1.949365\n",
            "{AlexNet} Train Epoch: 46 [26624/37814 (70%)]\tLoss: 1.970882\n",
            "{AlexNet} Train Epoch: 46 [27136/37814 (72%)]\tLoss: 1.946396\n",
            "{AlexNet} Train Epoch: 46 [27648/37814 (73%)]\tLoss: 2.026447\n",
            "{AlexNet} Train Epoch: 46 [28160/37814 (74%)]\tLoss: 1.990439\n",
            "{AlexNet} Train Epoch: 46 [28672/37814 (76%)]\tLoss: 1.967684\n",
            "{AlexNet} Train Epoch: 46 [29184/37814 (77%)]\tLoss: 2.017766\n",
            "{AlexNet} Train Epoch: 46 [29696/37814 (78%)]\tLoss: 1.981097\n",
            "{AlexNet} Train Epoch: 46 [30208/37814 (80%)]\tLoss: 1.938606\n",
            "{AlexNet} Train Epoch: 46 [30720/37814 (81%)]\tLoss: 1.962505\n",
            "{AlexNet} Train Epoch: 46 [31232/37814 (82%)]\tLoss: 1.971057\n",
            "{AlexNet} Train Epoch: 46 [31744/37814 (84%)]\tLoss: 1.954870\n",
            "{AlexNet} Train Epoch: 46 [32256/37814 (85%)]\tLoss: 1.986105\n",
            "{AlexNet} Train Epoch: 46 [32768/37814 (86%)]\tLoss: 1.994131\n",
            "{AlexNet} Train Epoch: 46 [33280/37814 (88%)]\tLoss: 1.996735\n",
            "{AlexNet} Train Epoch: 46 [33792/37814 (89%)]\tLoss: 1.983512\n",
            "{AlexNet} Train Epoch: 46 [34304/37814 (91%)]\tLoss: 1.975706\n",
            "{AlexNet} Train Epoch: 46 [34816/37814 (92%)]\tLoss: 1.961529\n",
            "{AlexNet} Train Epoch: 46 [35328/37814 (93%)]\tLoss: 1.984051\n",
            "{AlexNet} Train Epoch: 46 [35840/37814 (95%)]\tLoss: 2.003281\n",
            "{AlexNet} Train Epoch: 46 [36352/37814 (96%)]\tLoss: 1.971007\n",
            "{AlexNet} Train Epoch: 46 [36864/37814 (97%)]\tLoss: 2.016768\n",
            "{AlexNet} Train Epoch: 46 [31974/37814 (99%)]\tLoss: 1.971181\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9729, Accuracy: 1044/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.719099283218384 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 46 [0/37814 (0%)]\tLoss: 1.825657\n",
            "{SqueezeNet} Train Epoch: 46 [512/37814 (1%)]\tLoss: 1.801231\n",
            "{SqueezeNet} Train Epoch: 46 [1024/37814 (3%)]\tLoss: 1.795332\n",
            "{SqueezeNet} Train Epoch: 46 [1536/37814 (4%)]\tLoss: 1.815411\n",
            "{SqueezeNet} Train Epoch: 46 [2048/37814 (5%)]\tLoss: 1.818154\n",
            "{SqueezeNet} Train Epoch: 46 [2560/37814 (7%)]\tLoss: 1.794394\n",
            "{SqueezeNet} Train Epoch: 46 [3072/37814 (8%)]\tLoss: 1.838627\n",
            "{SqueezeNet} Train Epoch: 46 [3584/37814 (9%)]\tLoss: 1.884529\n",
            "{SqueezeNet} Train Epoch: 46 [4096/37814 (11%)]\tLoss: 1.795349\n",
            "{SqueezeNet} Train Epoch: 46 [4608/37814 (12%)]\tLoss: 1.870650\n",
            "{SqueezeNet} Train Epoch: 46 [5120/37814 (14%)]\tLoss: 1.809926\n",
            "{SqueezeNet} Train Epoch: 46 [5632/37814 (15%)]\tLoss: 1.762176\n",
            "{SqueezeNet} Train Epoch: 46 [6144/37814 (16%)]\tLoss: 1.752964\n",
            "{SqueezeNet} Train Epoch: 46 [6656/37814 (18%)]\tLoss: 1.804030\n",
            "{SqueezeNet} Train Epoch: 46 [7168/37814 (19%)]\tLoss: 1.814564\n",
            "{SqueezeNet} Train Epoch: 46 [7680/37814 (20%)]\tLoss: 1.776467\n",
            "{SqueezeNet} Train Epoch: 46 [8192/37814 (22%)]\tLoss: 1.812427\n",
            "{SqueezeNet} Train Epoch: 46 [8704/37814 (23%)]\tLoss: 1.918327\n",
            "{SqueezeNet} Train Epoch: 46 [9216/37814 (24%)]\tLoss: 1.785434\n",
            "{SqueezeNet} Train Epoch: 46 [9728/37814 (26%)]\tLoss: 1.866849\n",
            "{SqueezeNet} Train Epoch: 46 [10240/37814 (27%)]\tLoss: 1.849939\n",
            "{SqueezeNet} Train Epoch: 46 [10752/37814 (28%)]\tLoss: 1.810471\n",
            "{SqueezeNet} Train Epoch: 46 [11264/37814 (30%)]\tLoss: 1.873317\n",
            "{SqueezeNet} Train Epoch: 46 [11776/37814 (31%)]\tLoss: 1.811693\n",
            "{SqueezeNet} Train Epoch: 46 [12288/37814 (32%)]\tLoss: 1.794652\n",
            "{SqueezeNet} Train Epoch: 46 [12800/37814 (34%)]\tLoss: 1.851099\n",
            "{SqueezeNet} Train Epoch: 46 [13312/37814 (35%)]\tLoss: 1.822515\n",
            "{SqueezeNet} Train Epoch: 46 [13824/37814 (36%)]\tLoss: 1.868547\n",
            "{SqueezeNet} Train Epoch: 46 [14336/37814 (38%)]\tLoss: 1.717049\n",
            "{SqueezeNet} Train Epoch: 46 [14848/37814 (39%)]\tLoss: 1.803789\n",
            "{SqueezeNet} Train Epoch: 46 [15360/37814 (41%)]\tLoss: 1.862056\n",
            "{SqueezeNet} Train Epoch: 46 [15872/37814 (42%)]\tLoss: 1.740570\n",
            "{SqueezeNet} Train Epoch: 46 [16384/37814 (43%)]\tLoss: 1.824473\n",
            "{SqueezeNet} Train Epoch: 46 [16896/37814 (45%)]\tLoss: 1.828454\n",
            "{SqueezeNet} Train Epoch: 46 [17408/37814 (46%)]\tLoss: 1.831693\n",
            "{SqueezeNet} Train Epoch: 46 [17920/37814 (47%)]\tLoss: 1.849535\n",
            "{SqueezeNet} Train Epoch: 46 [18432/37814 (49%)]\tLoss: 1.761818\n",
            "{SqueezeNet} Train Epoch: 46 [18944/37814 (50%)]\tLoss: 1.866583\n",
            "{SqueezeNet} Train Epoch: 46 [19456/37814 (51%)]\tLoss: 1.797925\n",
            "{SqueezeNet} Train Epoch: 46 [19968/37814 (53%)]\tLoss: 1.848828\n",
            "{SqueezeNet} Train Epoch: 46 [20480/37814 (54%)]\tLoss: 1.880735\n",
            "{SqueezeNet} Train Epoch: 46 [20992/37814 (55%)]\tLoss: 1.858413\n",
            "{SqueezeNet} Train Epoch: 46 [21504/37814 (57%)]\tLoss: 1.817668\n",
            "{SqueezeNet} Train Epoch: 46 [22016/37814 (58%)]\tLoss: 1.756220\n",
            "{SqueezeNet} Train Epoch: 46 [22528/37814 (59%)]\tLoss: 1.846808\n",
            "{SqueezeNet} Train Epoch: 46 [23040/37814 (61%)]\tLoss: 1.814837\n",
            "{SqueezeNet} Train Epoch: 46 [23552/37814 (62%)]\tLoss: 1.885908\n",
            "{SqueezeNet} Train Epoch: 46 [24064/37814 (64%)]\tLoss: 1.792859\n",
            "{SqueezeNet} Train Epoch: 46 [24576/37814 (65%)]\tLoss: 1.798639\n",
            "{SqueezeNet} Train Epoch: 46 [25088/37814 (66%)]\tLoss: 1.847518\n",
            "{SqueezeNet} Train Epoch: 46 [25600/37814 (68%)]\tLoss: 1.811845\n",
            "{SqueezeNet} Train Epoch: 46 [26112/37814 (69%)]\tLoss: 1.870377\n",
            "{SqueezeNet} Train Epoch: 46 [26624/37814 (70%)]\tLoss: 1.858154\n",
            "{SqueezeNet} Train Epoch: 46 [27136/37814 (72%)]\tLoss: 1.752977\n",
            "{SqueezeNet} Train Epoch: 46 [27648/37814 (73%)]\tLoss: 1.903469\n",
            "{SqueezeNet} Train Epoch: 46 [28160/37814 (74%)]\tLoss: 1.822210\n",
            "{SqueezeNet} Train Epoch: 46 [28672/37814 (76%)]\tLoss: 1.827052\n",
            "{SqueezeNet} Train Epoch: 46 [29184/37814 (77%)]\tLoss: 1.844625\n",
            "{SqueezeNet} Train Epoch: 46 [29696/37814 (78%)]\tLoss: 1.811290\n",
            "{SqueezeNet} Train Epoch: 46 [30208/37814 (80%)]\tLoss: 1.823501\n",
            "{SqueezeNet} Train Epoch: 46 [30720/37814 (81%)]\tLoss: 1.819048\n",
            "{SqueezeNet} Train Epoch: 46 [31232/37814 (82%)]\tLoss: 1.816301\n",
            "{SqueezeNet} Train Epoch: 46 [31744/37814 (84%)]\tLoss: 1.847184\n",
            "{SqueezeNet} Train Epoch: 46 [32256/37814 (85%)]\tLoss: 1.837237\n",
            "{SqueezeNet} Train Epoch: 46 [32768/37814 (86%)]\tLoss: 1.846392\n",
            "{SqueezeNet} Train Epoch: 46 [33280/37814 (88%)]\tLoss: 1.794296\n",
            "{SqueezeNet} Train Epoch: 46 [33792/37814 (89%)]\tLoss: 1.792072\n",
            "{SqueezeNet} Train Epoch: 46 [34304/37814 (91%)]\tLoss: 1.795528\n",
            "{SqueezeNet} Train Epoch: 46 [34816/37814 (92%)]\tLoss: 1.836638\n",
            "{SqueezeNet} Train Epoch: 46 [35328/37814 (93%)]\tLoss: 1.788081\n",
            "{SqueezeNet} Train Epoch: 46 [35840/37814 (95%)]\tLoss: 1.857218\n",
            "{SqueezeNet} Train Epoch: 46 [36352/37814 (96%)]\tLoss: 1.782591\n",
            "{SqueezeNet} Train Epoch: 46 [36864/37814 (97%)]\tLoss: 1.892534\n",
            "{SqueezeNet} Train Epoch: 46 [31974/37814 (99%)]\tLoss: 1.763629\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.7854, Accuracy: 1661/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.450608491897583 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"93f5692e-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"727897bc-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6b128c9513"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"93f6a0a0-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_d3141759e0"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"93f720de-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_85691a8d90"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"93f77ee4-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"93f720de-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_159ee0c5a7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"942020b0-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"93f6a0a0-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_1f8eee8468"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"9421cd66-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_009b05cc1a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"94220dee-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_3a135901a1"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"94223d32-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"94220dee-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_db6be70cef"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 47 [0/37814 (0%)]\tLoss: 1.990197\n",
            "{AlexNet} Train Epoch: 47 [512/37814 (1%)]\tLoss: 1.956170\n",
            "{AlexNet} Train Epoch: 47 [1024/37814 (3%)]\tLoss: 1.925083\n",
            "{AlexNet} Train Epoch: 47 [1536/37814 (4%)]\tLoss: 1.962767\n",
            "{AlexNet} Train Epoch: 47 [2048/37814 (5%)]\tLoss: 1.980000\n",
            "{AlexNet} Train Epoch: 47 [2560/37814 (7%)]\tLoss: 1.985945\n",
            "{AlexNet} Train Epoch: 47 [3072/37814 (8%)]\tLoss: 1.962711\n",
            "{AlexNet} Train Epoch: 47 [3584/37814 (9%)]\tLoss: 1.983242\n",
            "{AlexNet} Train Epoch: 47 [4096/37814 (11%)]\tLoss: 2.004694\n",
            "{AlexNet} Train Epoch: 47 [4608/37814 (12%)]\tLoss: 1.981380\n",
            "{AlexNet} Train Epoch: 47 [5120/37814 (14%)]\tLoss: 1.891577\n",
            "{AlexNet} Train Epoch: 47 [5632/37814 (15%)]\tLoss: 1.971963\n",
            "{AlexNet} Train Epoch: 47 [6144/37814 (16%)]\tLoss: 1.964640\n",
            "{AlexNet} Train Epoch: 47 [6656/37814 (18%)]\tLoss: 1.982971\n",
            "{AlexNet} Train Epoch: 47 [7168/37814 (19%)]\tLoss: 1.968301\n",
            "{AlexNet} Train Epoch: 47 [7680/37814 (20%)]\tLoss: 1.925032\n",
            "{AlexNet} Train Epoch: 47 [8192/37814 (22%)]\tLoss: 1.950990\n",
            "{AlexNet} Train Epoch: 47 [8704/37814 (23%)]\tLoss: 1.974759\n",
            "{AlexNet} Train Epoch: 47 [9216/37814 (24%)]\tLoss: 1.967905\n",
            "{AlexNet} Train Epoch: 47 [9728/37814 (26%)]\tLoss: 1.931779\n",
            "{AlexNet} Train Epoch: 47 [10240/37814 (27%)]\tLoss: 1.953436\n",
            "{AlexNet} Train Epoch: 47 [10752/37814 (28%)]\tLoss: 1.964295\n",
            "{AlexNet} Train Epoch: 47 [11264/37814 (30%)]\tLoss: 1.996515\n",
            "{AlexNet} Train Epoch: 47 [11776/37814 (31%)]\tLoss: 1.992084\n",
            "{AlexNet} Train Epoch: 47 [12288/37814 (32%)]\tLoss: 2.005011\n",
            "{AlexNet} Train Epoch: 47 [12800/37814 (34%)]\tLoss: 1.932818\n",
            "{AlexNet} Train Epoch: 47 [13312/37814 (35%)]\tLoss: 1.933246\n",
            "{AlexNet} Train Epoch: 47 [13824/37814 (36%)]\tLoss: 1.938518\n",
            "{AlexNet} Train Epoch: 47 [14336/37814 (38%)]\tLoss: 1.930913\n",
            "{AlexNet} Train Epoch: 47 [14848/37814 (39%)]\tLoss: 1.941680\n",
            "{AlexNet} Train Epoch: 47 [15360/37814 (41%)]\tLoss: 1.973926\n",
            "{AlexNet} Train Epoch: 47 [15872/37814 (42%)]\tLoss: 1.996099\n",
            "{AlexNet} Train Epoch: 47 [16384/37814 (43%)]\tLoss: 1.996371\n",
            "{AlexNet} Train Epoch: 47 [16896/37814 (45%)]\tLoss: 1.976457\n",
            "{AlexNet} Train Epoch: 47 [17408/37814 (46%)]\tLoss: 1.957811\n",
            "{AlexNet} Train Epoch: 47 [17920/37814 (47%)]\tLoss: 1.979810\n",
            "{AlexNet} Train Epoch: 47 [18432/37814 (49%)]\tLoss: 1.999460\n",
            "{AlexNet} Train Epoch: 47 [18944/37814 (50%)]\tLoss: 1.950601\n",
            "{AlexNet} Train Epoch: 47 [19456/37814 (51%)]\tLoss: 1.973144\n",
            "{AlexNet} Train Epoch: 47 [19968/37814 (53%)]\tLoss: 1.990803\n",
            "{AlexNet} Train Epoch: 47 [20480/37814 (54%)]\tLoss: 1.998956\n",
            "{AlexNet} Train Epoch: 47 [20992/37814 (55%)]\tLoss: 1.955412\n",
            "{AlexNet} Train Epoch: 47 [21504/37814 (57%)]\tLoss: 1.980638\n",
            "{AlexNet} Train Epoch: 47 [22016/37814 (58%)]\tLoss: 1.993860\n",
            "{AlexNet} Train Epoch: 47 [22528/37814 (59%)]\tLoss: 1.979448\n",
            "{AlexNet} Train Epoch: 47 [23040/37814 (61%)]\tLoss: 1.927985\n",
            "{AlexNet} Train Epoch: 47 [23552/37814 (62%)]\tLoss: 1.954317\n",
            "{AlexNet} Train Epoch: 47 [24064/37814 (64%)]\tLoss: 2.011295\n",
            "{AlexNet} Train Epoch: 47 [24576/37814 (65%)]\tLoss: 1.964803\n",
            "{AlexNet} Train Epoch: 47 [25088/37814 (66%)]\tLoss: 1.993860\n",
            "{AlexNet} Train Epoch: 47 [25600/37814 (68%)]\tLoss: 1.993513\n",
            "{AlexNet} Train Epoch: 47 [26112/37814 (69%)]\tLoss: 1.982399\n",
            "{AlexNet} Train Epoch: 47 [26624/37814 (70%)]\tLoss: 1.966972\n",
            "{AlexNet} Train Epoch: 47 [27136/37814 (72%)]\tLoss: 1.979941\n",
            "{AlexNet} Train Epoch: 47 [27648/37814 (73%)]\tLoss: 1.988072\n",
            "{AlexNet} Train Epoch: 47 [28160/37814 (74%)]\tLoss: 1.956877\n",
            "{AlexNet} Train Epoch: 47 [28672/37814 (76%)]\tLoss: 2.023122\n",
            "{AlexNet} Train Epoch: 47 [29184/37814 (77%)]\tLoss: 1.987127\n",
            "{AlexNet} Train Epoch: 47 [29696/37814 (78%)]\tLoss: 1.987760\n",
            "{AlexNet} Train Epoch: 47 [30208/37814 (80%)]\tLoss: 1.989026\n",
            "{AlexNet} Train Epoch: 47 [30720/37814 (81%)]\tLoss: 1.973883\n",
            "{AlexNet} Train Epoch: 47 [31232/37814 (82%)]\tLoss: 1.928096\n",
            "{AlexNet} Train Epoch: 47 [31744/37814 (84%)]\tLoss: 1.986285\n",
            "{AlexNet} Train Epoch: 47 [32256/37814 (85%)]\tLoss: 1.970778\n",
            "{AlexNet} Train Epoch: 47 [32768/37814 (86%)]\tLoss: 2.016310\n",
            "{AlexNet} Train Epoch: 47 [33280/37814 (88%)]\tLoss: 1.986886\n",
            "{AlexNet} Train Epoch: 47 [33792/37814 (89%)]\tLoss: 1.986370\n",
            "{AlexNet} Train Epoch: 47 [34304/37814 (91%)]\tLoss: 1.962566\n",
            "{AlexNet} Train Epoch: 47 [34816/37814 (92%)]\tLoss: 1.991086\n",
            "{AlexNet} Train Epoch: 47 [35328/37814 (93%)]\tLoss: 2.003984\n",
            "{AlexNet} Train Epoch: 47 [35840/37814 (95%)]\tLoss: 1.985838\n",
            "{AlexNet} Train Epoch: 47 [36352/37814 (96%)]\tLoss: 1.969159\n",
            "{AlexNet} Train Epoch: 47 [36864/37814 (97%)]\tLoss: 1.970228\n",
            "{AlexNet} Train Epoch: 47 [31974/37814 (99%)]\tLoss: 1.983508\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9761, Accuracy: 1050/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.903584718704224 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 47 [0/37814 (0%)]\tLoss: 1.833877\n",
            "{SqueezeNet} Train Epoch: 47 [512/37814 (1%)]\tLoss: 1.787911\n",
            "{SqueezeNet} Train Epoch: 47 [1024/37814 (3%)]\tLoss: 1.853917\n",
            "{SqueezeNet} Train Epoch: 47 [1536/37814 (4%)]\tLoss: 1.851380\n",
            "{SqueezeNet} Train Epoch: 47 [2048/37814 (5%)]\tLoss: 1.795942\n",
            "{SqueezeNet} Train Epoch: 47 [2560/37814 (7%)]\tLoss: 1.880666\n",
            "{SqueezeNet} Train Epoch: 47 [3072/37814 (8%)]\tLoss: 1.814544\n",
            "{SqueezeNet} Train Epoch: 47 [3584/37814 (9%)]\tLoss: 1.784625\n",
            "{SqueezeNet} Train Epoch: 47 [4096/37814 (11%)]\tLoss: 1.874192\n",
            "{SqueezeNet} Train Epoch: 47 [4608/37814 (12%)]\tLoss: 1.829752\n",
            "{SqueezeNet} Train Epoch: 47 [5120/37814 (14%)]\tLoss: 1.857834\n",
            "{SqueezeNet} Train Epoch: 47 [5632/37814 (15%)]\tLoss: 1.825769\n",
            "{SqueezeNet} Train Epoch: 47 [6144/37814 (16%)]\tLoss: 1.816704\n",
            "{SqueezeNet} Train Epoch: 47 [6656/37814 (18%)]\tLoss: 1.815532\n",
            "{SqueezeNet} Train Epoch: 47 [7168/37814 (19%)]\tLoss: 1.811633\n",
            "{SqueezeNet} Train Epoch: 47 [7680/37814 (20%)]\tLoss: 1.778098\n",
            "{SqueezeNet} Train Epoch: 47 [8192/37814 (22%)]\tLoss: 1.850995\n",
            "{SqueezeNet} Train Epoch: 47 [8704/37814 (23%)]\tLoss: 1.845360\n",
            "{SqueezeNet} Train Epoch: 47 [9216/37814 (24%)]\tLoss: 1.846308\n",
            "{SqueezeNet} Train Epoch: 47 [9728/37814 (26%)]\tLoss: 1.824403\n",
            "{SqueezeNet} Train Epoch: 47 [10240/37814 (27%)]\tLoss: 1.813692\n",
            "{SqueezeNet} Train Epoch: 47 [10752/37814 (28%)]\tLoss: 1.793864\n",
            "{SqueezeNet} Train Epoch: 47 [11264/37814 (30%)]\tLoss: 1.787091\n",
            "{SqueezeNet} Train Epoch: 47 [11776/37814 (31%)]\tLoss: 1.827681\n",
            "{SqueezeNet} Train Epoch: 47 [12288/37814 (32%)]\tLoss: 1.818169\n",
            "{SqueezeNet} Train Epoch: 47 [12800/37814 (34%)]\tLoss: 1.837756\n",
            "{SqueezeNet} Train Epoch: 47 [13312/37814 (35%)]\tLoss: 1.812941\n",
            "{SqueezeNet} Train Epoch: 47 [13824/37814 (36%)]\tLoss: 1.795153\n",
            "{SqueezeNet} Train Epoch: 47 [14336/37814 (38%)]\tLoss: 1.871006\n",
            "{SqueezeNet} Train Epoch: 47 [14848/37814 (39%)]\tLoss: 1.805269\n",
            "{SqueezeNet} Train Epoch: 47 [15360/37814 (41%)]\tLoss: 1.864629\n",
            "{SqueezeNet} Train Epoch: 47 [15872/37814 (42%)]\tLoss: 1.874746\n",
            "{SqueezeNet} Train Epoch: 47 [16384/37814 (43%)]\tLoss: 1.809376\n",
            "{SqueezeNet} Train Epoch: 47 [16896/37814 (45%)]\tLoss: 1.854802\n",
            "{SqueezeNet} Train Epoch: 47 [17408/37814 (46%)]\tLoss: 1.811699\n",
            "{SqueezeNet} Train Epoch: 47 [17920/37814 (47%)]\tLoss: 1.819276\n",
            "{SqueezeNet} Train Epoch: 47 [18432/37814 (49%)]\tLoss: 1.823975\n",
            "{SqueezeNet} Train Epoch: 47 [18944/37814 (50%)]\tLoss: 1.831591\n",
            "{SqueezeNet} Train Epoch: 47 [19456/37814 (51%)]\tLoss: 1.825501\n",
            "{SqueezeNet} Train Epoch: 47 [19968/37814 (53%)]\tLoss: 1.861507\n",
            "{SqueezeNet} Train Epoch: 47 [20480/37814 (54%)]\tLoss: 1.827945\n",
            "{SqueezeNet} Train Epoch: 47 [20992/37814 (55%)]\tLoss: 1.780900\n",
            "{SqueezeNet} Train Epoch: 47 [21504/37814 (57%)]\tLoss: 1.812000\n",
            "{SqueezeNet} Train Epoch: 47 [22016/37814 (58%)]\tLoss: 1.805468\n",
            "{SqueezeNet} Train Epoch: 47 [22528/37814 (59%)]\tLoss: 1.800047\n",
            "{SqueezeNet} Train Epoch: 47 [23040/37814 (61%)]\tLoss: 1.815118\n",
            "{SqueezeNet} Train Epoch: 47 [23552/37814 (62%)]\tLoss: 1.842879\n",
            "{SqueezeNet} Train Epoch: 47 [24064/37814 (64%)]\tLoss: 1.782873\n",
            "{SqueezeNet} Train Epoch: 47 [24576/37814 (65%)]\tLoss: 1.801801\n",
            "{SqueezeNet} Train Epoch: 47 [25088/37814 (66%)]\tLoss: 1.867574\n",
            "{SqueezeNet} Train Epoch: 47 [25600/37814 (68%)]\tLoss: 1.822198\n",
            "{SqueezeNet} Train Epoch: 47 [26112/37814 (69%)]\tLoss: 1.828943\n",
            "{SqueezeNet} Train Epoch: 47 [26624/37814 (70%)]\tLoss: 1.855182\n",
            "{SqueezeNet} Train Epoch: 47 [27136/37814 (72%)]\tLoss: 1.821875\n",
            "{SqueezeNet} Train Epoch: 47 [27648/37814 (73%)]\tLoss: 1.841572\n",
            "{SqueezeNet} Train Epoch: 47 [28160/37814 (74%)]\tLoss: 1.872477\n",
            "{SqueezeNet} Train Epoch: 47 [28672/37814 (76%)]\tLoss: 1.836357\n",
            "{SqueezeNet} Train Epoch: 47 [29184/37814 (77%)]\tLoss: 1.830501\n",
            "{SqueezeNet} Train Epoch: 47 [29696/37814 (78%)]\tLoss: 1.831438\n",
            "{SqueezeNet} Train Epoch: 47 [30208/37814 (80%)]\tLoss: 1.852301\n",
            "{SqueezeNet} Train Epoch: 47 [30720/37814 (81%)]\tLoss: 1.779847\n",
            "{SqueezeNet} Train Epoch: 47 [31232/37814 (82%)]\tLoss: 1.791089\n",
            "{SqueezeNet} Train Epoch: 47 [31744/37814 (84%)]\tLoss: 1.776865\n",
            "{SqueezeNet} Train Epoch: 47 [32256/37814 (85%)]\tLoss: 1.787914\n",
            "{SqueezeNet} Train Epoch: 47 [32768/37814 (86%)]\tLoss: 1.828130\n",
            "{SqueezeNet} Train Epoch: 47 [33280/37814 (88%)]\tLoss: 1.776769\n",
            "{SqueezeNet} Train Epoch: 47 [33792/37814 (89%)]\tLoss: 1.831566\n",
            "{SqueezeNet} Train Epoch: 47 [34304/37814 (91%)]\tLoss: 1.828106\n",
            "{SqueezeNet} Train Epoch: 47 [34816/37814 (92%)]\tLoss: 1.779676\n",
            "{SqueezeNet} Train Epoch: 47 [35328/37814 (93%)]\tLoss: 1.850527\n",
            "{SqueezeNet} Train Epoch: 47 [35840/37814 (95%)]\tLoss: 1.886119\n",
            "{SqueezeNet} Train Epoch: 47 [36352/37814 (96%)]\tLoss: 1.840236\n",
            "{SqueezeNet} Train Epoch: 47 [36864/37814 (97%)]\tLoss: 1.825073\n",
            "{SqueezeNet} Train Epoch: 47 [31974/37814 (99%)]\tLoss: 1.882511\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.7982, Accuracy: 1663/5000 (33%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.32764172554016 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6401e66-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"9421cd66-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_1ce7bc2374"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b641624e-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_74efbb60af"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b6419b24-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_23f578f64a"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b641d3be-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b6419b24-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c89b2c2fa3"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b66c0058-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b641624e-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_6d1df78640"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b66d070a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_993c865bed"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b66d3e28-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_3166cb03e7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"b66d723a-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b66d3e28-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_0cb2c6bd09"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 48 [0/37814 (0%)]\tLoss: 1.914978\n",
            "{AlexNet} Train Epoch: 48 [512/37814 (1%)]\tLoss: 1.999185\n",
            "{AlexNet} Train Epoch: 48 [1024/37814 (3%)]\tLoss: 1.983663\n",
            "{AlexNet} Train Epoch: 48 [1536/37814 (4%)]\tLoss: 1.969196\n",
            "{AlexNet} Train Epoch: 48 [2048/37814 (5%)]\tLoss: 1.953591\n",
            "{AlexNet} Train Epoch: 48 [2560/37814 (7%)]\tLoss: 1.973475\n",
            "{AlexNet} Train Epoch: 48 [3072/37814 (8%)]\tLoss: 1.971552\n",
            "{AlexNet} Train Epoch: 48 [3584/37814 (9%)]\tLoss: 1.998140\n",
            "{AlexNet} Train Epoch: 48 [4096/37814 (11%)]\tLoss: 1.977440\n",
            "{AlexNet} Train Epoch: 48 [4608/37814 (12%)]\tLoss: 2.016729\n",
            "{AlexNet} Train Epoch: 48 [5120/37814 (14%)]\tLoss: 1.994868\n",
            "{AlexNet} Train Epoch: 48 [5632/37814 (15%)]\tLoss: 1.979821\n",
            "{AlexNet} Train Epoch: 48 [6144/37814 (16%)]\tLoss: 1.988595\n",
            "{AlexNet} Train Epoch: 48 [6656/37814 (18%)]\tLoss: 1.967761\n",
            "{AlexNet} Train Epoch: 48 [7168/37814 (19%)]\tLoss: 1.999835\n",
            "{AlexNet} Train Epoch: 48 [7680/37814 (20%)]\tLoss: 2.003975\n",
            "{AlexNet} Train Epoch: 48 [8192/37814 (22%)]\tLoss: 1.963943\n",
            "{AlexNet} Train Epoch: 48 [8704/37814 (23%)]\tLoss: 1.984313\n",
            "{AlexNet} Train Epoch: 48 [9216/37814 (24%)]\tLoss: 1.942291\n",
            "{AlexNet} Train Epoch: 48 [9728/37814 (26%)]\tLoss: 1.983721\n",
            "{AlexNet} Train Epoch: 48 [10240/37814 (27%)]\tLoss: 1.971025\n",
            "{AlexNet} Train Epoch: 48 [10752/37814 (28%)]\tLoss: 1.955547\n",
            "{AlexNet} Train Epoch: 48 [11264/37814 (30%)]\tLoss: 1.955964\n",
            "{AlexNet} Train Epoch: 48 [11776/37814 (31%)]\tLoss: 1.992788\n",
            "{AlexNet} Train Epoch: 48 [12288/37814 (32%)]\tLoss: 1.937211\n",
            "{AlexNet} Train Epoch: 48 [12800/37814 (34%)]\tLoss: 1.986246\n",
            "{AlexNet} Train Epoch: 48 [13312/37814 (35%)]\tLoss: 1.945010\n",
            "{AlexNet} Train Epoch: 48 [13824/37814 (36%)]\tLoss: 1.980255\n",
            "{AlexNet} Train Epoch: 48 [14336/37814 (38%)]\tLoss: 2.025733\n",
            "{AlexNet} Train Epoch: 48 [14848/37814 (39%)]\tLoss: 1.964070\n",
            "{AlexNet} Train Epoch: 48 [15360/37814 (41%)]\tLoss: 1.979041\n",
            "{AlexNet} Train Epoch: 48 [15872/37814 (42%)]\tLoss: 1.963914\n",
            "{AlexNet} Train Epoch: 48 [16384/37814 (43%)]\tLoss: 1.994472\n",
            "{AlexNet} Train Epoch: 48 [16896/37814 (45%)]\tLoss: 1.940414\n",
            "{AlexNet} Train Epoch: 48 [17408/37814 (46%)]\tLoss: 1.991060\n",
            "{AlexNet} Train Epoch: 48 [17920/37814 (47%)]\tLoss: 1.946792\n",
            "{AlexNet} Train Epoch: 48 [18432/37814 (49%)]\tLoss: 1.993329\n",
            "{AlexNet} Train Epoch: 48 [18944/37814 (50%)]\tLoss: 1.932908\n",
            "{AlexNet} Train Epoch: 48 [19456/37814 (51%)]\tLoss: 2.000069\n",
            "{AlexNet} Train Epoch: 48 [19968/37814 (53%)]\tLoss: 1.954235\n",
            "{AlexNet} Train Epoch: 48 [20480/37814 (54%)]\tLoss: 1.940794\n",
            "{AlexNet} Train Epoch: 48 [20992/37814 (55%)]\tLoss: 1.967480\n",
            "{AlexNet} Train Epoch: 48 [21504/37814 (57%)]\tLoss: 2.004146\n",
            "{AlexNet} Train Epoch: 48 [22016/37814 (58%)]\tLoss: 1.919214\n",
            "{AlexNet} Train Epoch: 48 [22528/37814 (59%)]\tLoss: 2.007425\n",
            "{AlexNet} Train Epoch: 48 [23040/37814 (61%)]\tLoss: 1.973689\n",
            "{AlexNet} Train Epoch: 48 [23552/37814 (62%)]\tLoss: 1.954159\n",
            "{AlexNet} Train Epoch: 48 [24064/37814 (64%)]\tLoss: 1.951524\n",
            "{AlexNet} Train Epoch: 48 [24576/37814 (65%)]\tLoss: 1.935471\n",
            "{AlexNet} Train Epoch: 48 [25088/37814 (66%)]\tLoss: 2.024983\n",
            "{AlexNet} Train Epoch: 48 [25600/37814 (68%)]\tLoss: 1.947297\n",
            "{AlexNet} Train Epoch: 48 [26112/37814 (69%)]\tLoss: 1.982526\n",
            "{AlexNet} Train Epoch: 48 [26624/37814 (70%)]\tLoss: 1.962859\n",
            "{AlexNet} Train Epoch: 48 [27136/37814 (72%)]\tLoss: 1.997940\n",
            "{AlexNet} Train Epoch: 48 [27648/37814 (73%)]\tLoss: 1.990922\n",
            "{AlexNet} Train Epoch: 48 [28160/37814 (74%)]\tLoss: 1.972075\n",
            "{AlexNet} Train Epoch: 48 [28672/37814 (76%)]\tLoss: 1.970597\n",
            "{AlexNet} Train Epoch: 48 [29184/37814 (77%)]\tLoss: 1.953569\n",
            "{AlexNet} Train Epoch: 48 [29696/37814 (78%)]\tLoss: 1.917060\n",
            "{AlexNet} Train Epoch: 48 [30208/37814 (80%)]\tLoss: 1.938312\n",
            "{AlexNet} Train Epoch: 48 [30720/37814 (81%)]\tLoss: 1.998249\n",
            "{AlexNet} Train Epoch: 48 [31232/37814 (82%)]\tLoss: 1.982803\n",
            "{AlexNet} Train Epoch: 48 [31744/37814 (84%)]\tLoss: 1.976506\n",
            "{AlexNet} Train Epoch: 48 [32256/37814 (85%)]\tLoss: 1.918254\n",
            "{AlexNet} Train Epoch: 48 [32768/37814 (86%)]\tLoss: 2.017869\n",
            "{AlexNet} Train Epoch: 48 [33280/37814 (88%)]\tLoss: 2.021106\n",
            "{AlexNet} Train Epoch: 48 [33792/37814 (89%)]\tLoss: 1.946489\n",
            "{AlexNet} Train Epoch: 48 [34304/37814 (91%)]\tLoss: 1.983810\n",
            "{AlexNet} Train Epoch: 48 [34816/37814 (92%)]\tLoss: 1.952026\n",
            "{AlexNet} Train Epoch: 48 [35328/37814 (93%)]\tLoss: 1.973011\n",
            "{AlexNet} Train Epoch: 48 [35840/37814 (95%)]\tLoss: 1.952849\n",
            "{AlexNet} Train Epoch: 48 [36352/37814 (96%)]\tLoss: 2.023704\n",
            "{AlexNet} Train Epoch: 48 [36864/37814 (97%)]\tLoss: 1.966504\n",
            "{AlexNet} Train Epoch: 48 [31974/37814 (99%)]\tLoss: 1.953018\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9708, Accuracy: 1045/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 26.874592542648315 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 48 [0/37814 (0%)]\tLoss: 1.840752\n",
            "{SqueezeNet} Train Epoch: 48 [512/37814 (1%)]\tLoss: 1.878284\n",
            "{SqueezeNet} Train Epoch: 48 [1024/37814 (3%)]\tLoss: 1.844481\n",
            "{SqueezeNet} Train Epoch: 48 [1536/37814 (4%)]\tLoss: 1.757640\n",
            "{SqueezeNet} Train Epoch: 48 [2048/37814 (5%)]\tLoss: 1.817297\n",
            "{SqueezeNet} Train Epoch: 48 [2560/37814 (7%)]\tLoss: 1.807227\n",
            "{SqueezeNet} Train Epoch: 48 [3072/37814 (8%)]\tLoss: 1.861197\n",
            "{SqueezeNet} Train Epoch: 48 [3584/37814 (9%)]\tLoss: 1.867991\n",
            "{SqueezeNet} Train Epoch: 48 [4096/37814 (11%)]\tLoss: 1.789959\n",
            "{SqueezeNet} Train Epoch: 48 [4608/37814 (12%)]\tLoss: 1.878423\n",
            "{SqueezeNet} Train Epoch: 48 [5120/37814 (14%)]\tLoss: 1.794911\n",
            "{SqueezeNet} Train Epoch: 48 [5632/37814 (15%)]\tLoss: 1.869936\n",
            "{SqueezeNet} Train Epoch: 48 [6144/37814 (16%)]\tLoss: 1.911068\n",
            "{SqueezeNet} Train Epoch: 48 [6656/37814 (18%)]\tLoss: 1.828180\n",
            "{SqueezeNet} Train Epoch: 48 [7168/37814 (19%)]\tLoss: 1.783803\n",
            "{SqueezeNet} Train Epoch: 48 [7680/37814 (20%)]\tLoss: 1.835693\n",
            "{SqueezeNet} Train Epoch: 48 [8192/37814 (22%)]\tLoss: 1.839082\n",
            "{SqueezeNet} Train Epoch: 48 [8704/37814 (23%)]\tLoss: 1.816834\n",
            "{SqueezeNet} Train Epoch: 48 [9216/37814 (24%)]\tLoss: 1.787221\n",
            "{SqueezeNet} Train Epoch: 48 [9728/37814 (26%)]\tLoss: 1.823908\n",
            "{SqueezeNet} Train Epoch: 48 [10240/37814 (27%)]\tLoss: 1.867533\n",
            "{SqueezeNet} Train Epoch: 48 [10752/37814 (28%)]\tLoss: 1.775226\n",
            "{SqueezeNet} Train Epoch: 48 [11264/37814 (30%)]\tLoss: 1.795125\n",
            "{SqueezeNet} Train Epoch: 48 [11776/37814 (31%)]\tLoss: 1.846949\n",
            "{SqueezeNet} Train Epoch: 48 [12288/37814 (32%)]\tLoss: 1.860510\n",
            "{SqueezeNet} Train Epoch: 48 [12800/37814 (34%)]\tLoss: 1.802679\n",
            "{SqueezeNet} Train Epoch: 48 [13312/37814 (35%)]\tLoss: 1.758481\n",
            "{SqueezeNet} Train Epoch: 48 [13824/37814 (36%)]\tLoss: 1.803303\n",
            "{SqueezeNet} Train Epoch: 48 [14336/37814 (38%)]\tLoss: 1.839944\n",
            "{SqueezeNet} Train Epoch: 48 [14848/37814 (39%)]\tLoss: 1.798650\n",
            "{SqueezeNet} Train Epoch: 48 [15360/37814 (41%)]\tLoss: 1.830971\n",
            "{SqueezeNet} Train Epoch: 48 [15872/37814 (42%)]\tLoss: 1.845051\n",
            "{SqueezeNet} Train Epoch: 48 [16384/37814 (43%)]\tLoss: 1.792070\n",
            "{SqueezeNet} Train Epoch: 48 [16896/37814 (45%)]\tLoss: 1.774886\n",
            "{SqueezeNet} Train Epoch: 48 [17408/37814 (46%)]\tLoss: 1.851298\n",
            "{SqueezeNet} Train Epoch: 48 [17920/37814 (47%)]\tLoss: 1.782637\n",
            "{SqueezeNet} Train Epoch: 48 [18432/37814 (49%)]\tLoss: 1.822774\n",
            "{SqueezeNet} Train Epoch: 48 [18944/37814 (50%)]\tLoss: 1.795313\n",
            "{SqueezeNet} Train Epoch: 48 [19456/37814 (51%)]\tLoss: 1.813338\n",
            "{SqueezeNet} Train Epoch: 48 [19968/37814 (53%)]\tLoss: 1.829579\n",
            "{SqueezeNet} Train Epoch: 48 [20480/37814 (54%)]\tLoss: 1.823138\n",
            "{SqueezeNet} Train Epoch: 48 [20992/37814 (55%)]\tLoss: 1.791407\n",
            "{SqueezeNet} Train Epoch: 48 [21504/37814 (57%)]\tLoss: 1.883905\n",
            "{SqueezeNet} Train Epoch: 48 [22016/37814 (58%)]\tLoss: 1.845813\n",
            "{SqueezeNet} Train Epoch: 48 [22528/37814 (59%)]\tLoss: 1.796928\n",
            "{SqueezeNet} Train Epoch: 48 [23040/37814 (61%)]\tLoss: 1.878756\n",
            "{SqueezeNet} Train Epoch: 48 [23552/37814 (62%)]\tLoss: 1.869742\n",
            "{SqueezeNet} Train Epoch: 48 [24064/37814 (64%)]\tLoss: 1.830175\n",
            "{SqueezeNet} Train Epoch: 48 [24576/37814 (65%)]\tLoss: 1.853298\n",
            "{SqueezeNet} Train Epoch: 48 [25088/37814 (66%)]\tLoss: 1.808348\n",
            "{SqueezeNet} Train Epoch: 48 [25600/37814 (68%)]\tLoss: 1.808166\n",
            "{SqueezeNet} Train Epoch: 48 [26112/37814 (69%)]\tLoss: 1.819489\n",
            "{SqueezeNet} Train Epoch: 48 [26624/37814 (70%)]\tLoss: 1.792780\n",
            "{SqueezeNet} Train Epoch: 48 [27136/37814 (72%)]\tLoss: 1.791591\n",
            "{SqueezeNet} Train Epoch: 48 [27648/37814 (73%)]\tLoss: 1.848011\n",
            "{SqueezeNet} Train Epoch: 48 [28160/37814 (74%)]\tLoss: 1.782226\n",
            "{SqueezeNet} Train Epoch: 48 [28672/37814 (76%)]\tLoss: 1.801975\n",
            "{SqueezeNet} Train Epoch: 48 [29184/37814 (77%)]\tLoss: 1.776139\n",
            "{SqueezeNet} Train Epoch: 48 [29696/37814 (78%)]\tLoss: 1.820226\n",
            "{SqueezeNet} Train Epoch: 48 [30208/37814 (80%)]\tLoss: 1.842134\n",
            "{SqueezeNet} Train Epoch: 48 [30720/37814 (81%)]\tLoss: 1.833418\n",
            "{SqueezeNet} Train Epoch: 48 [31232/37814 (82%)]\tLoss: 1.821027\n",
            "{SqueezeNet} Train Epoch: 48 [31744/37814 (84%)]\tLoss: 1.815424\n",
            "{SqueezeNet} Train Epoch: 48 [32256/37814 (85%)]\tLoss: 1.852446\n",
            "{SqueezeNet} Train Epoch: 48 [32768/37814 (86%)]\tLoss: 1.795488\n",
            "{SqueezeNet} Train Epoch: 48 [33280/37814 (88%)]\tLoss: 1.784460\n",
            "{SqueezeNet} Train Epoch: 48 [33792/37814 (89%)]\tLoss: 1.857577\n",
            "{SqueezeNet} Train Epoch: 48 [34304/37814 (91%)]\tLoss: 1.794672\n",
            "{SqueezeNet} Train Epoch: 48 [34816/37814 (92%)]\tLoss: 1.768951\n",
            "{SqueezeNet} Train Epoch: 48 [35328/37814 (93%)]\tLoss: 1.810636\n",
            "{SqueezeNet} Train Epoch: 48 [35840/37814 (95%)]\tLoss: 1.828822\n",
            "{SqueezeNet} Train Epoch: 48 [36352/37814 (96%)]\tLoss: 1.821437\n",
            "{SqueezeNet} Train Epoch: 48 [36864/37814 (97%)]\tLoss: 1.823977\n",
            "{SqueezeNet} Train Epoch: 48 [31974/37814 (99%)]\tLoss: 1.836522\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8093, Accuracy: 1602/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.37367057800293 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d75d0320-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"b66d070a-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_e67fef58c9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d75e8592-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_4121ae5ecd"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d75ecbce-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_8f23c322f7"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d75f1142-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d75ecbce-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_4713aa70d8"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d7895ee8-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d75e8592-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_b9060d1a1e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d78b29da-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_bf8dd00642"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d78b6846-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_d2dc0848fa"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"d78ba8f6-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d78b6846-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_5bbea18138"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 49 [0/37814 (0%)]\tLoss: 1.981722\n",
            "{AlexNet} Train Epoch: 49 [512/37814 (1%)]\tLoss: 1.950558\n",
            "{AlexNet} Train Epoch: 49 [1024/37814 (3%)]\tLoss: 1.959630\n",
            "{AlexNet} Train Epoch: 49 [1536/37814 (4%)]\tLoss: 1.959918\n",
            "{AlexNet} Train Epoch: 49 [2048/37814 (5%)]\tLoss: 1.945564\n",
            "{AlexNet} Train Epoch: 49 [2560/37814 (7%)]\tLoss: 1.949343\n",
            "{AlexNet} Train Epoch: 49 [3072/37814 (8%)]\tLoss: 1.984698\n",
            "{AlexNet} Train Epoch: 49 [3584/37814 (9%)]\tLoss: 1.958004\n",
            "{AlexNet} Train Epoch: 49 [4096/37814 (11%)]\tLoss: 2.010419\n",
            "{AlexNet} Train Epoch: 49 [4608/37814 (12%)]\tLoss: 2.011965\n",
            "{AlexNet} Train Epoch: 49 [5120/37814 (14%)]\tLoss: 1.971272\n",
            "{AlexNet} Train Epoch: 49 [5632/37814 (15%)]\tLoss: 1.981306\n",
            "{AlexNet} Train Epoch: 49 [6144/37814 (16%)]\tLoss: 1.932950\n",
            "{AlexNet} Train Epoch: 49 [6656/37814 (18%)]\tLoss: 1.985486\n",
            "{AlexNet} Train Epoch: 49 [7168/37814 (19%)]\tLoss: 1.980597\n",
            "{AlexNet} Train Epoch: 49 [7680/37814 (20%)]\tLoss: 1.972655\n",
            "{AlexNet} Train Epoch: 49 [8192/37814 (22%)]\tLoss: 1.926288\n",
            "{AlexNet} Train Epoch: 49 [8704/37814 (23%)]\tLoss: 1.946199\n",
            "{AlexNet} Train Epoch: 49 [9216/37814 (24%)]\tLoss: 1.936219\n",
            "{AlexNet} Train Epoch: 49 [9728/37814 (26%)]\tLoss: 1.910616\n",
            "{AlexNet} Train Epoch: 49 [10240/37814 (27%)]\tLoss: 1.979297\n",
            "{AlexNet} Train Epoch: 49 [10752/37814 (28%)]\tLoss: 1.971783\n",
            "{AlexNet} Train Epoch: 49 [11264/37814 (30%)]\tLoss: 1.932738\n",
            "{AlexNet} Train Epoch: 49 [11776/37814 (31%)]\tLoss: 1.978427\n",
            "{AlexNet} Train Epoch: 49 [12288/37814 (32%)]\tLoss: 2.011216\n",
            "{AlexNet} Train Epoch: 49 [12800/37814 (34%)]\tLoss: 1.987365\n",
            "{AlexNet} Train Epoch: 49 [13312/37814 (35%)]\tLoss: 1.935660\n",
            "{AlexNet} Train Epoch: 49 [13824/37814 (36%)]\tLoss: 1.980257\n",
            "{AlexNet} Train Epoch: 49 [14336/37814 (38%)]\tLoss: 1.991496\n",
            "{AlexNet} Train Epoch: 49 [14848/37814 (39%)]\tLoss: 1.999547\n",
            "{AlexNet} Train Epoch: 49 [15360/37814 (41%)]\tLoss: 1.999043\n",
            "{AlexNet} Train Epoch: 49 [15872/37814 (42%)]\tLoss: 1.934769\n",
            "{AlexNet} Train Epoch: 49 [16384/37814 (43%)]\tLoss: 1.988349\n",
            "{AlexNet} Train Epoch: 49 [16896/37814 (45%)]\tLoss: 1.976416\n",
            "{AlexNet} Train Epoch: 49 [17408/37814 (46%)]\tLoss: 1.993284\n",
            "{AlexNet} Train Epoch: 49 [17920/37814 (47%)]\tLoss: 1.977261\n",
            "{AlexNet} Train Epoch: 49 [18432/37814 (49%)]\tLoss: 1.970551\n",
            "{AlexNet} Train Epoch: 49 [18944/37814 (50%)]\tLoss: 1.962364\n",
            "{AlexNet} Train Epoch: 49 [19456/37814 (51%)]\tLoss: 1.994480\n",
            "{AlexNet} Train Epoch: 49 [19968/37814 (53%)]\tLoss: 1.990123\n",
            "{AlexNet} Train Epoch: 49 [20480/37814 (54%)]\tLoss: 1.953702\n",
            "{AlexNet} Train Epoch: 49 [20992/37814 (55%)]\tLoss: 1.944379\n",
            "{AlexNet} Train Epoch: 49 [21504/37814 (57%)]\tLoss: 1.910941\n",
            "{AlexNet} Train Epoch: 49 [22016/37814 (58%)]\tLoss: 1.960129\n",
            "{AlexNet} Train Epoch: 49 [22528/37814 (59%)]\tLoss: 1.993436\n",
            "{AlexNet} Train Epoch: 49 [23040/37814 (61%)]\tLoss: 1.971938\n",
            "{AlexNet} Train Epoch: 49 [23552/37814 (62%)]\tLoss: 1.955771\n",
            "{AlexNet} Train Epoch: 49 [24064/37814 (64%)]\tLoss: 2.005156\n",
            "{AlexNet} Train Epoch: 49 [24576/37814 (65%)]\tLoss: 1.952864\n",
            "{AlexNet} Train Epoch: 49 [25088/37814 (66%)]\tLoss: 1.984999\n",
            "{AlexNet} Train Epoch: 49 [25600/37814 (68%)]\tLoss: 2.009904\n",
            "{AlexNet} Train Epoch: 49 [26112/37814 (69%)]\tLoss: 1.957537\n",
            "{AlexNet} Train Epoch: 49 [26624/37814 (70%)]\tLoss: 1.989813\n",
            "{AlexNet} Train Epoch: 49 [27136/37814 (72%)]\tLoss: 1.948270\n",
            "{AlexNet} Train Epoch: 49 [27648/37814 (73%)]\tLoss: 1.986828\n",
            "{AlexNet} Train Epoch: 49 [28160/37814 (74%)]\tLoss: 1.951816\n",
            "{AlexNet} Train Epoch: 49 [28672/37814 (76%)]\tLoss: 1.971246\n",
            "{AlexNet} Train Epoch: 49 [29184/37814 (77%)]\tLoss: 2.005947\n",
            "{AlexNet} Train Epoch: 49 [29696/37814 (78%)]\tLoss: 1.975953\n",
            "{AlexNet} Train Epoch: 49 [30208/37814 (80%)]\tLoss: 2.004244\n",
            "{AlexNet} Train Epoch: 49 [30720/37814 (81%)]\tLoss: 1.967593\n",
            "{AlexNet} Train Epoch: 49 [31232/37814 (82%)]\tLoss: 1.987971\n",
            "{AlexNet} Train Epoch: 49 [31744/37814 (84%)]\tLoss: 1.928833\n",
            "{AlexNet} Train Epoch: 49 [32256/37814 (85%)]\tLoss: 1.997981\n",
            "{AlexNet} Train Epoch: 49 [32768/37814 (86%)]\tLoss: 2.001990\n",
            "{AlexNet} Train Epoch: 49 [33280/37814 (88%)]\tLoss: 2.023701\n",
            "{AlexNet} Train Epoch: 49 [33792/37814 (89%)]\tLoss: 1.937864\n",
            "{AlexNet} Train Epoch: 49 [34304/37814 (91%)]\tLoss: 1.964314\n",
            "{AlexNet} Train Epoch: 49 [34816/37814 (92%)]\tLoss: 1.999589\n",
            "{AlexNet} Train Epoch: 49 [35328/37814 (93%)]\tLoss: 1.952985\n",
            "{AlexNet} Train Epoch: 49 [35840/37814 (95%)]\tLoss: 1.984141\n",
            "{AlexNet} Train Epoch: 49 [36352/37814 (96%)]\tLoss: 1.980893\n",
            "{AlexNet} Train Epoch: 49 [36864/37814 (97%)]\tLoss: 1.998829\n",
            "{AlexNet} Train Epoch: 49 [31974/37814 (99%)]\tLoss: 1.999491\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9730, Accuracy: 1041/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.89903712272644 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 49 [0/37814 (0%)]\tLoss: 1.860169\n",
            "{SqueezeNet} Train Epoch: 49 [512/37814 (1%)]\tLoss: 1.840395\n",
            "{SqueezeNet} Train Epoch: 49 [1024/37814 (3%)]\tLoss: 1.785089\n",
            "{SqueezeNet} Train Epoch: 49 [1536/37814 (4%)]\tLoss: 1.848860\n",
            "{SqueezeNet} Train Epoch: 49 [2048/37814 (5%)]\tLoss: 1.840219\n",
            "{SqueezeNet} Train Epoch: 49 [2560/37814 (7%)]\tLoss: 1.792017\n",
            "{SqueezeNet} Train Epoch: 49 [3072/37814 (8%)]\tLoss: 1.838524\n",
            "{SqueezeNet} Train Epoch: 49 [3584/37814 (9%)]\tLoss: 1.894385\n",
            "{SqueezeNet} Train Epoch: 49 [4096/37814 (11%)]\tLoss: 1.827397\n",
            "{SqueezeNet} Train Epoch: 49 [4608/37814 (12%)]\tLoss: 1.853675\n",
            "{SqueezeNet} Train Epoch: 49 [5120/37814 (14%)]\tLoss: 1.836996\n",
            "{SqueezeNet} Train Epoch: 49 [5632/37814 (15%)]\tLoss: 1.842103\n",
            "{SqueezeNet} Train Epoch: 49 [6144/37814 (16%)]\tLoss: 1.862868\n",
            "{SqueezeNet} Train Epoch: 49 [6656/37814 (18%)]\tLoss: 1.811357\n",
            "{SqueezeNet} Train Epoch: 49 [7168/37814 (19%)]\tLoss: 1.801897\n",
            "{SqueezeNet} Train Epoch: 49 [7680/37814 (20%)]\tLoss: 1.800903\n",
            "{SqueezeNet} Train Epoch: 49 [8192/37814 (22%)]\tLoss: 1.832045\n",
            "{SqueezeNet} Train Epoch: 49 [8704/37814 (23%)]\tLoss: 1.808056\n",
            "{SqueezeNet} Train Epoch: 49 [9216/37814 (24%)]\tLoss: 1.796338\n",
            "{SqueezeNet} Train Epoch: 49 [9728/37814 (26%)]\tLoss: 1.865016\n",
            "{SqueezeNet} Train Epoch: 49 [10240/37814 (27%)]\tLoss: 1.841779\n",
            "{SqueezeNet} Train Epoch: 49 [10752/37814 (28%)]\tLoss: 1.870630\n",
            "{SqueezeNet} Train Epoch: 49 [11264/37814 (30%)]\tLoss: 1.843252\n",
            "{SqueezeNet} Train Epoch: 49 [11776/37814 (31%)]\tLoss: 1.773874\n",
            "{SqueezeNet} Train Epoch: 49 [12288/37814 (32%)]\tLoss: 1.766147\n",
            "{SqueezeNet} Train Epoch: 49 [12800/37814 (34%)]\tLoss: 1.821895\n",
            "{SqueezeNet} Train Epoch: 49 [13312/37814 (35%)]\tLoss: 1.836952\n",
            "{SqueezeNet} Train Epoch: 49 [13824/37814 (36%)]\tLoss: 1.797297\n",
            "{SqueezeNet} Train Epoch: 49 [14336/37814 (38%)]\tLoss: 1.826688\n",
            "{SqueezeNet} Train Epoch: 49 [14848/37814 (39%)]\tLoss: 1.794023\n",
            "{SqueezeNet} Train Epoch: 49 [15360/37814 (41%)]\tLoss: 1.861359\n",
            "{SqueezeNet} Train Epoch: 49 [15872/37814 (42%)]\tLoss: 1.794798\n",
            "{SqueezeNet} Train Epoch: 49 [16384/37814 (43%)]\tLoss: 1.794476\n",
            "{SqueezeNet} Train Epoch: 49 [16896/37814 (45%)]\tLoss: 1.800308\n",
            "{SqueezeNet} Train Epoch: 49 [17408/37814 (46%)]\tLoss: 1.849055\n",
            "{SqueezeNet} Train Epoch: 49 [17920/37814 (47%)]\tLoss: 1.873834\n",
            "{SqueezeNet} Train Epoch: 49 [18432/37814 (49%)]\tLoss: 1.806051\n",
            "{SqueezeNet} Train Epoch: 49 [18944/37814 (50%)]\tLoss: 1.870515\n",
            "{SqueezeNet} Train Epoch: 49 [19456/37814 (51%)]\tLoss: 1.882300\n",
            "{SqueezeNet} Train Epoch: 49 [19968/37814 (53%)]\tLoss: 1.880403\n",
            "{SqueezeNet} Train Epoch: 49 [20480/37814 (54%)]\tLoss: 1.825075\n",
            "{SqueezeNet} Train Epoch: 49 [20992/37814 (55%)]\tLoss: 1.840512\n",
            "{SqueezeNet} Train Epoch: 49 [21504/37814 (57%)]\tLoss: 1.820145\n",
            "{SqueezeNet} Train Epoch: 49 [22016/37814 (58%)]\tLoss: 1.797218\n",
            "{SqueezeNet} Train Epoch: 49 [22528/37814 (59%)]\tLoss: 1.804180\n",
            "{SqueezeNet} Train Epoch: 49 [23040/37814 (61%)]\tLoss: 1.824591\n",
            "{SqueezeNet} Train Epoch: 49 [23552/37814 (62%)]\tLoss: 1.815962\n",
            "{SqueezeNet} Train Epoch: 49 [24064/37814 (64%)]\tLoss: 1.773889\n",
            "{SqueezeNet} Train Epoch: 49 [24576/37814 (65%)]\tLoss: 1.782284\n",
            "{SqueezeNet} Train Epoch: 49 [25088/37814 (66%)]\tLoss: 1.782622\n",
            "{SqueezeNet} Train Epoch: 49 [25600/37814 (68%)]\tLoss: 1.857322\n",
            "{SqueezeNet} Train Epoch: 49 [26112/37814 (69%)]\tLoss: 1.771113\n",
            "{SqueezeNet} Train Epoch: 49 [26624/37814 (70%)]\tLoss: 1.832912\n",
            "{SqueezeNet} Train Epoch: 49 [27136/37814 (72%)]\tLoss: 1.905479\n",
            "{SqueezeNet} Train Epoch: 49 [27648/37814 (73%)]\tLoss: 1.848593\n",
            "{SqueezeNet} Train Epoch: 49 [28160/37814 (74%)]\tLoss: 1.843782\n",
            "{SqueezeNet} Train Epoch: 49 [28672/37814 (76%)]\tLoss: 1.760100\n",
            "{SqueezeNet} Train Epoch: 49 [29184/37814 (77%)]\tLoss: 1.789601\n",
            "{SqueezeNet} Train Epoch: 49 [29696/37814 (78%)]\tLoss: 1.789175\n",
            "{SqueezeNet} Train Epoch: 49 [30208/37814 (80%)]\tLoss: 1.829713\n",
            "{SqueezeNet} Train Epoch: 49 [30720/37814 (81%)]\tLoss: 1.771139\n",
            "{SqueezeNet} Train Epoch: 49 [31232/37814 (82%)]\tLoss: 1.860652\n",
            "{SqueezeNet} Train Epoch: 49 [31744/37814 (84%)]\tLoss: 1.841269\n",
            "{SqueezeNet} Train Epoch: 49 [32256/37814 (85%)]\tLoss: 1.821043\n",
            "{SqueezeNet} Train Epoch: 49 [32768/37814 (86%)]\tLoss: 1.814655\n",
            "{SqueezeNet} Train Epoch: 49 [33280/37814 (88%)]\tLoss: 1.796333\n",
            "{SqueezeNet} Train Epoch: 49 [33792/37814 (89%)]\tLoss: 1.841642\n",
            "{SqueezeNet} Train Epoch: 49 [34304/37814 (91%)]\tLoss: 1.825544\n",
            "{SqueezeNet} Train Epoch: 49 [34816/37814 (92%)]\tLoss: 1.838830\n",
            "{SqueezeNet} Train Epoch: 49 [35328/37814 (93%)]\tLoss: 1.838206\n",
            "{SqueezeNet} Train Epoch: 49 [35840/37814 (95%)]\tLoss: 1.856791\n",
            "{SqueezeNet} Train Epoch: 49 [36352/37814 (96%)]\tLoss: 1.833788\n",
            "{SqueezeNet} Train Epoch: 49 [36864/37814 (97%)]\tLoss: 1.841235\n",
            "{SqueezeNet} Train Epoch: 49 [31974/37814 (99%)]\tLoss: 1.882621\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8195, Accuracy: 1597/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 28.376516342163086 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f9177310-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"d78b29da-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_44786c7d02"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f9191bc0-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_1fa3909d31"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f919636e-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_ff08ebf527"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f919a5e0-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f919636e-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_d1b5d4fbb9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f944b870-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f9191bc0-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_8dce1b3b41"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f947130e-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_913a5cafd2"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f9479cac-60e8-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-1-0\");\n",
              "//# sourceURL=js_ea4d000661"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"f9481448-60e8-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f9479cac-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_9e86da8282"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{AlexNet} Train Epoch: 50 [0/37814 (0%)]\tLoss: 1.956772\n",
            "{AlexNet} Train Epoch: 50 [512/37814 (1%)]\tLoss: 1.946265\n",
            "{AlexNet} Train Epoch: 50 [1024/37814 (3%)]\tLoss: 2.038295\n",
            "{AlexNet} Train Epoch: 50 [1536/37814 (4%)]\tLoss: 1.986378\n",
            "{AlexNet} Train Epoch: 50 [2048/37814 (5%)]\tLoss: 2.025028\n",
            "{AlexNet} Train Epoch: 50 [2560/37814 (7%)]\tLoss: 2.028066\n",
            "{AlexNet} Train Epoch: 50 [3072/37814 (8%)]\tLoss: 1.946960\n",
            "{AlexNet} Train Epoch: 50 [3584/37814 (9%)]\tLoss: 1.954227\n",
            "{AlexNet} Train Epoch: 50 [4096/37814 (11%)]\tLoss: 1.974874\n",
            "{AlexNet} Train Epoch: 50 [4608/37814 (12%)]\tLoss: 1.933396\n",
            "{AlexNet} Train Epoch: 50 [5120/37814 (14%)]\tLoss: 1.973376\n",
            "{AlexNet} Train Epoch: 50 [5632/37814 (15%)]\tLoss: 1.978571\n",
            "{AlexNet} Train Epoch: 50 [6144/37814 (16%)]\tLoss: 1.941976\n",
            "{AlexNet} Train Epoch: 50 [6656/37814 (18%)]\tLoss: 1.923374\n",
            "{AlexNet} Train Epoch: 50 [7168/37814 (19%)]\tLoss: 1.989848\n",
            "{AlexNet} Train Epoch: 50 [7680/37814 (20%)]\tLoss: 1.978399\n",
            "{AlexNet} Train Epoch: 50 [8192/37814 (22%)]\tLoss: 1.991490\n",
            "{AlexNet} Train Epoch: 50 [8704/37814 (23%)]\tLoss: 1.986485\n",
            "{AlexNet} Train Epoch: 50 [9216/37814 (24%)]\tLoss: 1.964472\n",
            "{AlexNet} Train Epoch: 50 [9728/37814 (26%)]\tLoss: 1.957270\n",
            "{AlexNet} Train Epoch: 50 [10240/37814 (27%)]\tLoss: 1.962832\n",
            "{AlexNet} Train Epoch: 50 [10752/37814 (28%)]\tLoss: 1.972952\n",
            "{AlexNet} Train Epoch: 50 [11264/37814 (30%)]\tLoss: 2.001464\n",
            "{AlexNet} Train Epoch: 50 [11776/37814 (31%)]\tLoss: 1.990706\n",
            "{AlexNet} Train Epoch: 50 [12288/37814 (32%)]\tLoss: 2.015527\n",
            "{AlexNet} Train Epoch: 50 [12800/37814 (34%)]\tLoss: 1.935092\n",
            "{AlexNet} Train Epoch: 50 [13312/37814 (35%)]\tLoss: 1.981652\n",
            "{AlexNet} Train Epoch: 50 [13824/37814 (36%)]\tLoss: 1.982242\n",
            "{AlexNet} Train Epoch: 50 [14336/37814 (38%)]\tLoss: 1.990137\n",
            "{AlexNet} Train Epoch: 50 [14848/37814 (39%)]\tLoss: 2.017046\n",
            "{AlexNet} Train Epoch: 50 [15360/37814 (41%)]\tLoss: 1.966915\n",
            "{AlexNet} Train Epoch: 50 [15872/37814 (42%)]\tLoss: 1.977704\n",
            "{AlexNet} Train Epoch: 50 [16384/37814 (43%)]\tLoss: 1.960643\n",
            "{AlexNet} Train Epoch: 50 [16896/37814 (45%)]\tLoss: 1.973891\n",
            "{AlexNet} Train Epoch: 50 [17408/37814 (46%)]\tLoss: 1.909355\n",
            "{AlexNet} Train Epoch: 50 [17920/37814 (47%)]\tLoss: 1.964177\n",
            "{AlexNet} Train Epoch: 50 [18432/37814 (49%)]\tLoss: 1.924564\n",
            "{AlexNet} Train Epoch: 50 [18944/37814 (50%)]\tLoss: 1.916294\n",
            "{AlexNet} Train Epoch: 50 [19456/37814 (51%)]\tLoss: 2.037492\n",
            "{AlexNet} Train Epoch: 50 [19968/37814 (53%)]\tLoss: 1.960191\n",
            "{AlexNet} Train Epoch: 50 [20480/37814 (54%)]\tLoss: 1.993911\n",
            "{AlexNet} Train Epoch: 50 [20992/37814 (55%)]\tLoss: 1.963540\n",
            "{AlexNet} Train Epoch: 50 [21504/37814 (57%)]\tLoss: 1.972448\n",
            "{AlexNet} Train Epoch: 50 [22016/37814 (58%)]\tLoss: 1.955210\n",
            "{AlexNet} Train Epoch: 50 [22528/37814 (59%)]\tLoss: 2.010537\n",
            "{AlexNet} Train Epoch: 50 [23040/37814 (61%)]\tLoss: 1.987082\n",
            "{AlexNet} Train Epoch: 50 [23552/37814 (62%)]\tLoss: 1.990380\n",
            "{AlexNet} Train Epoch: 50 [24064/37814 (64%)]\tLoss: 1.987572\n",
            "{AlexNet} Train Epoch: 50 [24576/37814 (65%)]\tLoss: 1.970884\n",
            "{AlexNet} Train Epoch: 50 [25088/37814 (66%)]\tLoss: 1.969121\n",
            "{AlexNet} Train Epoch: 50 [25600/37814 (68%)]\tLoss: 1.976203\n",
            "{AlexNet} Train Epoch: 50 [26112/37814 (69%)]\tLoss: 1.978109\n",
            "{AlexNet} Train Epoch: 50 [26624/37814 (70%)]\tLoss: 1.992570\n",
            "{AlexNet} Train Epoch: 50 [27136/37814 (72%)]\tLoss: 1.944239\n",
            "{AlexNet} Train Epoch: 50 [27648/37814 (73%)]\tLoss: 1.976631\n",
            "{AlexNet} Train Epoch: 50 [28160/37814 (74%)]\tLoss: 2.001639\n",
            "{AlexNet} Train Epoch: 50 [28672/37814 (76%)]\tLoss: 1.989302\n",
            "{AlexNet} Train Epoch: 50 [29184/37814 (77%)]\tLoss: 1.960958\n",
            "{AlexNet} Train Epoch: 50 [29696/37814 (78%)]\tLoss: 1.937853\n",
            "{AlexNet} Train Epoch: 50 [30208/37814 (80%)]\tLoss: 1.979860\n",
            "{AlexNet} Train Epoch: 50 [30720/37814 (81%)]\tLoss: 1.991242\n",
            "{AlexNet} Train Epoch: 50 [31232/37814 (82%)]\tLoss: 2.020543\n",
            "{AlexNet} Train Epoch: 50 [31744/37814 (84%)]\tLoss: 1.980280\n",
            "{AlexNet} Train Epoch: 50 [32256/37814 (85%)]\tLoss: 1.949277\n",
            "{AlexNet} Train Epoch: 50 [32768/37814 (86%)]\tLoss: 1.912589\n",
            "{AlexNet} Train Epoch: 50 [33280/37814 (88%)]\tLoss: 1.993895\n",
            "{AlexNet} Train Epoch: 50 [33792/37814 (89%)]\tLoss: 1.974253\n",
            "{AlexNet} Train Epoch: 50 [34304/37814 (91%)]\tLoss: 2.027558\n",
            "{AlexNet} Train Epoch: 50 [34816/37814 (92%)]\tLoss: 2.026500\n",
            "{AlexNet} Train Epoch: 50 [35328/37814 (93%)]\tLoss: 1.931204\n",
            "{AlexNet} Train Epoch: 50 [35840/37814 (95%)]\tLoss: 1.982792\n",
            "{AlexNet} Train Epoch: 50 [36352/37814 (96%)]\tLoss: 1.974080\n",
            "{AlexNet} Train Epoch: 50 [36864/37814 (97%)]\tLoss: 1.969671\n",
            "{AlexNet} Train Epoch: 50 [31974/37814 (99%)]\tLoss: 1.980811\n",
            "\n",
            "{AlexNet} Validation set: Average loss: 1.9716, Accuracy: 1035/5000 (21%)\n",
            "\n",
            "{AlexNet} The last training epoch took 27.1761736869812 seconds.\n",
            "\n",
            "\n",
            "{SqueezeNet} Train Epoch: 50 [0/37814 (0%)]\tLoss: 1.855631\n",
            "{SqueezeNet} Train Epoch: 50 [512/37814 (1%)]\tLoss: 1.820992\n",
            "{SqueezeNet} Train Epoch: 50 [1024/37814 (3%)]\tLoss: 1.837909\n",
            "{SqueezeNet} Train Epoch: 50 [1536/37814 (4%)]\tLoss: 1.838749\n",
            "{SqueezeNet} Train Epoch: 50 [2048/37814 (5%)]\tLoss: 1.772302\n",
            "{SqueezeNet} Train Epoch: 50 [2560/37814 (7%)]\tLoss: 1.744278\n",
            "{SqueezeNet} Train Epoch: 50 [3072/37814 (8%)]\tLoss: 1.872845\n",
            "{SqueezeNet} Train Epoch: 50 [3584/37814 (9%)]\tLoss: 1.753018\n",
            "{SqueezeNet} Train Epoch: 50 [4096/37814 (11%)]\tLoss: 1.827324\n",
            "{SqueezeNet} Train Epoch: 50 [4608/37814 (12%)]\tLoss: 1.867424\n",
            "{SqueezeNet} Train Epoch: 50 [5120/37814 (14%)]\tLoss: 1.790601\n",
            "{SqueezeNet} Train Epoch: 50 [5632/37814 (15%)]\tLoss: 1.789943\n",
            "{SqueezeNet} Train Epoch: 50 [6144/37814 (16%)]\tLoss: 1.858216\n",
            "{SqueezeNet} Train Epoch: 50 [6656/37814 (18%)]\tLoss: 1.865039\n",
            "{SqueezeNet} Train Epoch: 50 [7168/37814 (19%)]\tLoss: 1.783097\n",
            "{SqueezeNet} Train Epoch: 50 [7680/37814 (20%)]\tLoss: 1.874734\n",
            "{SqueezeNet} Train Epoch: 50 [8192/37814 (22%)]\tLoss: 1.810989\n",
            "{SqueezeNet} Train Epoch: 50 [8704/37814 (23%)]\tLoss: 1.854111\n",
            "{SqueezeNet} Train Epoch: 50 [9216/37814 (24%)]\tLoss: 1.822550\n",
            "{SqueezeNet} Train Epoch: 50 [9728/37814 (26%)]\tLoss: 1.882029\n",
            "{SqueezeNet} Train Epoch: 50 [10240/37814 (27%)]\tLoss: 1.876041\n",
            "{SqueezeNet} Train Epoch: 50 [10752/37814 (28%)]\tLoss: 1.761812\n",
            "{SqueezeNet} Train Epoch: 50 [11264/37814 (30%)]\tLoss: 1.813397\n",
            "{SqueezeNet} Train Epoch: 50 [11776/37814 (31%)]\tLoss: 1.832423\n",
            "{SqueezeNet} Train Epoch: 50 [12288/37814 (32%)]\tLoss: 1.898410\n",
            "{SqueezeNet} Train Epoch: 50 [12800/37814 (34%)]\tLoss: 1.871821\n",
            "{SqueezeNet} Train Epoch: 50 [13312/37814 (35%)]\tLoss: 1.830864\n",
            "{SqueezeNet} Train Epoch: 50 [13824/37814 (36%)]\tLoss: 1.836726\n",
            "{SqueezeNet} Train Epoch: 50 [14336/37814 (38%)]\tLoss: 1.844055\n",
            "{SqueezeNet} Train Epoch: 50 [14848/37814 (39%)]\tLoss: 1.835802\n",
            "{SqueezeNet} Train Epoch: 50 [15360/37814 (41%)]\tLoss: 1.757638\n",
            "{SqueezeNet} Train Epoch: 50 [15872/37814 (42%)]\tLoss: 1.794131\n",
            "{SqueezeNet} Train Epoch: 50 [16384/37814 (43%)]\tLoss: 1.827289\n",
            "{SqueezeNet} Train Epoch: 50 [16896/37814 (45%)]\tLoss: 1.829178\n",
            "{SqueezeNet} Train Epoch: 50 [17408/37814 (46%)]\tLoss: 1.803565\n",
            "{SqueezeNet} Train Epoch: 50 [17920/37814 (47%)]\tLoss: 1.867321\n",
            "{SqueezeNet} Train Epoch: 50 [18432/37814 (49%)]\tLoss: 1.800224\n",
            "{SqueezeNet} Train Epoch: 50 [18944/37814 (50%)]\tLoss: 1.837739\n",
            "{SqueezeNet} Train Epoch: 50 [19456/37814 (51%)]\tLoss: 1.837407\n",
            "{SqueezeNet} Train Epoch: 50 [19968/37814 (53%)]\tLoss: 1.862338\n",
            "{SqueezeNet} Train Epoch: 50 [20480/37814 (54%)]\tLoss: 1.840231\n",
            "{SqueezeNet} Train Epoch: 50 [20992/37814 (55%)]\tLoss: 1.717680\n",
            "{SqueezeNet} Train Epoch: 50 [21504/37814 (57%)]\tLoss: 1.849205\n",
            "{SqueezeNet} Train Epoch: 50 [22016/37814 (58%)]\tLoss: 1.768023\n",
            "{SqueezeNet} Train Epoch: 50 [22528/37814 (59%)]\tLoss: 1.796300\n",
            "{SqueezeNet} Train Epoch: 50 [23040/37814 (61%)]\tLoss: 1.811402\n",
            "{SqueezeNet} Train Epoch: 50 [23552/37814 (62%)]\tLoss: 1.814128\n",
            "{SqueezeNet} Train Epoch: 50 [24064/37814 (64%)]\tLoss: 1.858972\n",
            "{SqueezeNet} Train Epoch: 50 [24576/37814 (65%)]\tLoss: 1.774132\n",
            "{SqueezeNet} Train Epoch: 50 [25088/37814 (66%)]\tLoss: 1.821687\n",
            "{SqueezeNet} Train Epoch: 50 [25600/37814 (68%)]\tLoss: 1.797971\n",
            "{SqueezeNet} Train Epoch: 50 [26112/37814 (69%)]\tLoss: 1.848221\n",
            "{SqueezeNet} Train Epoch: 50 [26624/37814 (70%)]\tLoss: 1.829486\n",
            "{SqueezeNet} Train Epoch: 50 [27136/37814 (72%)]\tLoss: 1.820368\n",
            "{SqueezeNet} Train Epoch: 50 [27648/37814 (73%)]\tLoss: 1.878345\n",
            "{SqueezeNet} Train Epoch: 50 [28160/37814 (74%)]\tLoss: 1.837121\n",
            "{SqueezeNet} Train Epoch: 50 [28672/37814 (76%)]\tLoss: 1.795970\n",
            "{SqueezeNet} Train Epoch: 50 [29184/37814 (77%)]\tLoss: 1.797961\n",
            "{SqueezeNet} Train Epoch: 50 [29696/37814 (78%)]\tLoss: 1.848700\n",
            "{SqueezeNet} Train Epoch: 50 [30208/37814 (80%)]\tLoss: 1.725536\n",
            "{SqueezeNet} Train Epoch: 50 [30720/37814 (81%)]\tLoss: 1.770197\n",
            "{SqueezeNet} Train Epoch: 50 [31232/37814 (82%)]\tLoss: 1.801302\n",
            "{SqueezeNet} Train Epoch: 50 [31744/37814 (84%)]\tLoss: 1.862494\n",
            "{SqueezeNet} Train Epoch: 50 [32256/37814 (85%)]\tLoss: 1.794556\n",
            "{SqueezeNet} Train Epoch: 50 [32768/37814 (86%)]\tLoss: 1.824426\n",
            "{SqueezeNet} Train Epoch: 50 [33280/37814 (88%)]\tLoss: 1.882354\n",
            "{SqueezeNet} Train Epoch: 50 [33792/37814 (89%)]\tLoss: 1.824682\n",
            "{SqueezeNet} Train Epoch: 50 [34304/37814 (91%)]\tLoss: 1.810727\n",
            "{SqueezeNet} Train Epoch: 50 [34816/37814 (92%)]\tLoss: 1.793748\n",
            "{SqueezeNet} Train Epoch: 50 [35328/37814 (93%)]\tLoss: 1.817208\n",
            "{SqueezeNet} Train Epoch: 50 [35840/37814 (95%)]\tLoss: 1.794623\n",
            "{SqueezeNet} Train Epoch: 50 [36352/37814 (96%)]\tLoss: 1.828340\n",
            "{SqueezeNet} Train Epoch: 50 [36864/37814 (97%)]\tLoss: 1.826735\n",
            "{SqueezeNet} Train Epoch: 50 [31974/37814 (99%)]\tLoss: 1.825692\n",
            "\n",
            "{SqueezeNet} Validation set: Average loss: 1.8103, Accuracy: 1600/5000 (32%)\n",
            "\n",
            "{SqueezeNet} The last training epoch took 29.80705690383911 seconds.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b412bde-60e9-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"f947130e-60e8-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_f7eef8ee4e"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-1-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b42da74-60e9-11e9-bc79-0242ac1c0002\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_04c4a91d23"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b434540-60e9-11e9-bc79-0242ac1c0002\"] = document.querySelector(\"#id12-0-0\");\n",
              "//# sourceURL=js_63b9afd184"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b43b110-60e9-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b434540-60e9-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_c05a9cf890"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12",
              "user_output"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window[\"1b6d75cc-60e9-11e9-bc79-0242ac1c0002\"] = google.colab.output.setActiveOutputArea(window[\"1b42da74-60e9-11e9-bc79-0242ac1c0002\"]);\n",
              "//# sourceURL=js_1d89077ac9"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": [
              "id12-0-0",
              "outputarea_id12"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "{AlexNet} Test set: Average loss: 1.9653, Accuracy: 2104/10000 (21%)\n",
            "\n",
            "\n",
            "{SqueezeNet} Test set: Average loss: 1.8005, Accuracy: 3340/10000 (33%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExccUHXqIwED",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "03004885-24b9-4f04-e050-08ba73e418c6"
      },
      "source": [
        "#@title Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_results(alex_results, \"AlexNet\", \"r\", squeeze_results, \"SqueezeNet\", \"b\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzCiYg7CMAzDiMr/BwUOh3Rg/qDrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYUe0CkaFFZ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc8658c1-dfe9-4023-b5a2-29f5396eee24"
      },
      "source": [
        "#@title Download\n",
        "from google.colab import files\n",
        "import datetime\n",
        "import tarfile\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "if os.path.exists('out'):\n",
        "    for f in os.listdir('out'):\n",
        "        os.remove(os.path.join('out', f))\n",
        "else:\n",
        "    os.mkdir('out')\n",
        "\n",
        "params = \\\n",
        "{\n",
        "    '_nb_epochs': _nb_epochs,\n",
        "    '_dataset': _dataset,\n",
        "    '_interpolation_method': _interpolation_method,\n",
        "    '_alex_enabled': _alex_enabled,\n",
        "    '_alex_learning_rate': _alex_learning_rate,\n",
        "    '_alex_momentum': _alex_momentum,\n",
        "    '_alex_weight_decay': _alex_weight_decay,\n",
        "    '_alex_scheduler': _alex_scheduler,\n",
        "    '_alex_step_size': _alex_step_size,\n",
        "    '_alex_gamma': _alex_gamma,\n",
        "    '_alex_pretrained': _alex_pretrained,\n",
        "    '_squeeze_learning_rate': _squeeze_learning_rate,\n",
        "    '_squeeze_momentum': _squeeze_momentum,\n",
        "    '_squeeze_weight_decay': _squeeze_weight_decay,\n",
        "    '_squeeze_scheduler': _squeeze_scheduler,\n",
        "    '_squeeze_step_size': _squeeze_step_size,\n",
        "    '_squeeze_gamma': _squeeze_gamma,\n",
        "    '_squeeze_pretrained': _squeeze_pretrained\n",
        "}\n",
        "\n",
        "joblib.dump(params, 'out/params.joblib')\n",
        "torch.save(squeeze.state_dict(), 'out/SqueezeNet.pt')\n",
        "joblib.dump([alex_results, squeeze_results], 'out/results.joblib', compress=3)\n",
        "\n",
        "if _alex_enabled:\n",
        "    torch.save(alex.state_dict(), 'out/AlexNet.pt')\n",
        "\n",
        "output_filename = \"alex_vs_squeeze_{}.tar.gz\".format(\n",
        "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        ")\n",
        "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "    outfiles = os.listdir('out')\n",
        "    for f in outfiles:\n",
        "        tar.add(os.path.join('out', f))\n",
        "\n",
        "# output_filename = \"/gdrive/My Drive/COMP551_Assignment4/alex_vs_squeeze_{}.tar.gz\".format(\n",
        "#     datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "# )\n",
        "# with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "#     outfiles = os.listdir('out')\n",
        "#     for f in outfiles:\n",
        "#         tar.add(os.path.join('out', f))\n",
        "\n",
        "drive_savefolder = \"/gdrive/My Drive/COMP551_Assignment4/\" #@param {type:\"string\"}\n",
        "\n",
        "output_filename = os.path.join(drive_savefolder, \"alex_vs_squeeze_{}.joblib\".format(\n",
        "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "))\n",
        "joblib.dump([params, alex_results, squeeze_results], output_filename, compress=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/COMP551_Assignment4/alex_vs_squeeze_2019-04-17_081656.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKOhHBILzlla",
        "cellView": "form"
      },
      "source": [
        "#@title Learning parameter grid search\n",
        "# from google.colab import widgets\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import datetime\n",
        "# import joblib\n",
        "# import re\n",
        "\n",
        "# drive_savefolder = \"/gdrive/My Drive/COMP551_Assignment4/\" #@param {type:\"string\"}\n",
        "\n",
        "# def train_nonan(model, model_name, device, train_loader, optimizer, epoch):\n",
        "#     model.train()\n",
        "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
        "#         data, target = data.to(device), target.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(data)\n",
        "#         loss = F.nll_loss(output, target)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if batch_idx % _log_interval == 0:\n",
        "#             print('{{{}}} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#                     model_name, epoch, batch_idx * len(data),\n",
        "#                     len(train_loader.dataset),\n",
        "#                     100. * batch_idx / len(train_loader), loss.item()\n",
        "#                 )\n",
        "#             )\n",
        "    \n",
        "#     # Need to finish batch so that loader will reset for next model/batch\n",
        "#     return not np.isnan(loss.cpu().detach().numpy())\n",
        "\n",
        "# def test_nonan(model, model_name, device, loader, notnan):\n",
        "\n",
        "#     if notnan:\n",
        "#         test(squeeze, \"SqueezeNet\", \"cuda\", test_loader)\n",
        "#     else:\n",
        "#         print(\n",
        "#             '\\n{{{}}} Test set: Average loss: {:.4f}, Accuracy: {} ({})\\n'\n",
        "#             .format(\n",
        "#                 model_name, np.nan, np.nan, np.nan\n",
        "#             )\n",
        "#         )\n",
        "#         return np.nan, np.nan\n",
        "  \n",
        "    \n",
        "# def do(_alex_learning_rate, _alex_momentum, _alex_weight_decay, _squeeze_learning_rate, _squeeze_momentum, _squeeze_weight_decay):\n",
        "    \n",
        "#     alex_notnan = True\n",
        "#     squeeze_notnan = True\n",
        "    \n",
        "#     # Initialize AlexNet\n",
        "#     if _alex_enabled:\n",
        "#         alex = AlexNet().to(\"cuda\")\n",
        "#         alex_opt = optim.SGD(\n",
        "#             alex.parameters(), lr=_alex_learning_rate, momentum=_alex_momentum, weight_decay=_alex_weight_decay)\n",
        "#         if \"Step\" == _alex_scheduler:\n",
        "#             alex_sch = optim.lr_scheduler.StepLR(alex_opt, _alex_step_size, _alex_gamma)\n",
        "#         elif \"Adaptive\" == _alex_scheduler:\n",
        "#             alex_sch = optim.lr_scheduler.ReduceLROnPlateau(alex_opt, factor=0.5, patience=2, verbose=True)\n",
        "#         else:\n",
        "#             assert False, \"Scheduler type not recognized.\"\n",
        "\n",
        "#     # Initialize SqueezeNet\n",
        "#     squeeze = SqueezeNet().to(\"cuda\")\n",
        "#     squeeze_opt = optim.SGD(\n",
        "#         squeeze.parameters(), lr=_squeeze_learning_rate, momentum=_squeeze_momentum, weight_decay=_squeeze_weight_decay\n",
        "#     )\n",
        "#     if \"Step\" == _squeeze_scheduler:\n",
        "#         squeeze_sch = optim.lr_scheduler.StepLR(squeeze_opt, _squeeze_step_size, _squeeze_gamma)\n",
        "#     elif \"Adaptive\" == _squeeze_scheduler:\n",
        "#         squeeze_sch = optim.lr_scheduler.ReduceLROnPlateau(alex_opt, factor=0.5, patience=2, verbose=True)\n",
        "#     else:\n",
        "#         assert False, \"Scheduler type not recognized.\"\n",
        "\n",
        "#     # Done!\n",
        "#     print('Dataloaders and models initialized.')\n",
        "    \n",
        "#     # Train and validate\n",
        "#     find_lr = re.compile(\"[0-9.e-]*.$\")\n",
        "\n",
        "#     if _alex_enabled or 'alex_results' not in locals():\n",
        "#         alex_results = np.full((1+_nb_epochs, 3), np.nan)\n",
        "#         alex_results[0, 2] = locals().get('_alex_learning_rate', np.nan)\n",
        "\n",
        "#     squeeze_results = np.full((1+_nb_epochs, 3), np.nan)\n",
        "#     squeeze_results[0, 2] = _squeeze_learning_rate\n",
        "\n",
        "#     # Prepare figure\n",
        "#     plot_results_to_grid(grid, (0, 0), alex_results, \"AlexNet\", \"r\", squeeze_results, \"SqueezeNet\", \"b\")\n",
        "\n",
        "#     for epoch in range(1, _nb_epochs + 1):\n",
        "#         # Display training output\n",
        "#         with grid.output_to(1, 0):\n",
        "#             if _alex_enabled and alex_notnan:\n",
        "#                 elapsed_time(\"AlexNet\", 0)\n",
        "#                 if \"Step\" == _alex_scheduler:\n",
        "#                     alex_sch.step()\n",
        "\n",
        "#                 alex_results[epoch-1, 2] = alex_opt.param_groups[0]['lr']\n",
        "                \n",
        "#                 alex_notnan = train_nonan(alex, \"AlexNet\", \"cuda\", train_loader, alex_opt, epoch)\n",
        "#                 alex_results[epoch-1, :2] = validate(alex, \"AlexNet\", \"cuda\", valid_loader)\n",
        "#                 if \"Adaptive\" == _alex_scheduler:\n",
        "#                     alex_sch.step(alex_results[epoch-1, 0])\n",
        "#                 elapsed_time(\"AlexNet\", 1)\n",
        "#             else:\n",
        "#                 print(\"Skipping AlexNet (NaN)\")\n",
        "            \n",
        "#             if squeeze_notnan:\n",
        "#                 elapsed_time(\"SqueezeNet\", 0)\n",
        "#                 squeeze_sch.step()\n",
        "#                 if \"Step\" == _squeeze_scheduler:\n",
        "#                     squeeze_results[epoch-1, 2] = squeeze_opt.param_groups[0]['lr']\n",
        "#                 squeeze_notnan = train_nonan(squeeze, \"SqueezeNet\", \"cuda\", train_loader, squeeze_opt, epoch)\n",
        "#                 squeeze_results[epoch-1, :2] = validate(squeeze, \"SqueezeNet\", \"cuda\", valid_loader)\n",
        "#                 if \"Adaptive\" == _squeeze_scheduler:\n",
        "#                     squeeze_sch.step(squeeze_results[epoch-1, 0])\n",
        "#                 elapsed_time(\"SqueezeNet\", 1)\n",
        "#             else:\n",
        "#                 print(\"Skipping SqueezeNet (NaN)\")\n",
        "\n",
        "#         plot_results_to_grid(grid, (0, 0), alex_results, \"AlexNet\", \"r\", squeeze_results, \"SqueezeNet\", \"b\")\n",
        "\n",
        "#     # Test results\n",
        "#     if _alex_enabled:\n",
        "#         alex_results[-1, :2] = test_nonan(alex, \"AlexNet\", \"cuda\", test_loader, alex_notnan)\n",
        "    \n",
        "#     squeeze_results[-1, :2] = test_nonan(squeeze, \"SqueezeNet\", \"cuda\", test_loader, squeeze_notnan)\n",
        "    \n",
        "#     # Save results\n",
        "#     params = \\\n",
        "#     {\n",
        "#         '_nb_epochs': _nb_epochs,\n",
        "#         '_dataset': _dataset,\n",
        "#         '_interpolation_method': _interpolation_method,\n",
        "#         '_alex_enabled': _alex_enabled,\n",
        "#         '_alex_learning_rate': _alex_learning_rate,\n",
        "#         '_alex_momentum': _alex_momentum,\n",
        "#         '_alex_weight_decay': _alex_weight_decay,\n",
        "#         '_alex_scheduler': _alex_scheduler,\n",
        "#         '_alex_step_size': _alex_step_size,\n",
        "#         '_alex_gamma': _alex_gamma,\n",
        "#         '_alex_pretrained': _alex_pretrained,\n",
        "#         '_squeeze_learning_rate': _squeeze_learning_rate,\n",
        "#         '_squeeze_momentum': _squeeze_momentum,\n",
        "#         '_squeeze_weight_decay': _squeeze_weight_decay,\n",
        "#         '_squeeze_scheduler': _squeeze_scheduler,\n",
        "#         '_squeeze_step_size': _squeeze_step_size,\n",
        "#         '_squeeze_gamma': _squeeze_gamma,\n",
        "#         '_squeeze_pretrained': _squeeze_pretrained\n",
        "#     }\n",
        "#     output_filename = os.path.join(drive_savefolder, \"alex_vs_squeeze_{}.joblib\".format(\n",
        "#         datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "#     ))\n",
        "#     joblib.dump([params, alex_results, squeeze_results], output_filename, compress=3)\n",
        "    \n",
        "#     return [alex_results[-1, 0], squeeze_results[-1, 0]]\n",
        "\n",
        "# LearningRate = [0.09, 0.05, 0.01, 0.005, 0.001]\n",
        "# Momentum = [0.9, 0.5, 0.1]\n",
        "# WeightDecay = [0, 0.0002]\n",
        "# search = {}\n",
        "# grid = widgets.Grid(2, 1)\n",
        "\n",
        "# for lr in LearningRate:\n",
        "#     for m in Momentum:\n",
        "#         for wd in WeightDecay:\n",
        "#             search[(lr,m,wd)] = do(lr, m, wd, lr, m, wd)\n",
        "\n",
        "# joblib.dump(search, os.path.join(drive_savefolder, 'alex_squeeze_grid_search_{}.joblib'.format(\n",
        "#     datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "# )))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}